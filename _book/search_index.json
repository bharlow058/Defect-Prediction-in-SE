[
["index.html", "Defect Prediction in Software Engineering Welcome", " Defect Prediction in Software Engineering Daniel Rodriguez and Javier Dolado 2017-07-07 Welcome These notes will try to describe the state of the art in software defect prediction. Acknowledgments TIN2016-76956-C3-1-R TIN2015-71841-REDT Work in progress. This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License. "],
["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Defect prediction in software engineering addresses the problem of predicting which software modules will be error prone. In this way, testing effort can be focused on those modules that are more error prone. Techniques are typically classified into: Defect Classification - using data mining tecniques to find error prone modules Defect Ranking - sort the modules according to their probability of being error-prone. For example, finding the 20 per cent of the modules with most bugs. There are other works that make use of textual information to classify defects. For example in order to find what to fix first or what requirement should be implemented (functinal or non-functional requierements). Categorise reviews - for example from Bug Tracking tools (e.g. Bugzilla) defect descriptions or Mobile aplications (Appstore or Google play) reviews Defect categories - Standards to deal with defects, for example Orthogonal Defect Classification (ODC) or with classifying requirements so that we can answer “What to fix first or implement (non-functional requierements)”. For example, (Huang et al. 2011) There are other tools to deal with defects such as Findbugs or PMD that work with a set of rules describing possible problems but these tools do not use predictive models but here we refer mainly to machine learning approaches to defect prediction. Some systematic reviews of software fault prediction studies include: (Hall et al. 2012) - A Systematic Literature Review on Fault Prediction Performance in Software Engineering (Catal and Diri 2009) - A systematic review of software fault prediction studies (Catal 2011) - Software fault prediction: A literature review and current trends º# (PART) Data Sources and Metrics and Standards in Software Engineering Defect Prediction {-} References "],
["data-sources-in-software-engineering.html", "Chapter 2 Data Sources in Software Engineering", " Chapter 2 Data Sources in Software Engineering We classify this trail in the following categories: Source code can be studied to measure its properties, such as size or complexity. Source Code Management Systems (SCM) make it possible to store all the changes that the different source code files undergo during the project. Also, SCM systems allow for work to be done in parallel by different developers over the same source code tree. Every change recorded in the system is accompanied with meta-information (author, date, reason for the change, etc) that can be used for research purposes. Issue or Bug tracking systems (ITS). Bugs, defects and user requests are managed in ISTs, where users and developers can fill tickets with a description of a defect found, or a desired new functionality. All the changes to the ticket are recorded in the system, and most of the systems also record the comments and communications among all the users and developers implied in the task. Messages between developers and users. In the case of free/open source software, the projects are open to the world, and the messages are archived in the form of mailing lists and social networks which can also be mined for research purposes. There are also some other open message systems, such as IRC or forums. Meta-data about the projects. As well as the low level information of the software processes, we can also find meta-data about the software projects which can be useful for research. This meta-data may include intended-audience, programming language, domain of application, license (in the case of open source), etc. Metadata (Source: I. Herraiz) Usage data. There are statistics about software downloads, logs from servers, software reviews, etc. Types of information stored in the repositories: Meta-information about the project itself and the people that participated. Low-level information Mailing Lists (ML) Bugs Tracking Systems (BTS) or Project Tracker System (PTS) Software Configuration Management Systems (SCM) Processed information. For example project management information about the effort estimation and cost of the project. Whether the repository is public or not Single project vs. multiprojects. Whether the repository contains information of a single project with multiples versions or multiples projects and/or versions. Type of content, open source or industrial projects Format in which the information is stored and formats or technologies for accessing the information: Text. It can be just plain text, CSV (Comma Separated Values) files, Attribute-Relation File Format (ARFF) or its variants Through databases. Downloading dumps of the database. Remote access such as APIs of Web services or REST "],
["repositories.html", "Chapter 3 Repositories", " Chapter 3 Repositories There is a number of open research repositories in Software Engineering. Among them: PROMISE (PRedictOr Models In Software Engineering): http://openscience.us/repo/ Promise Repository Finding Faults using Ensemble Learners (ELFF) (Shippey et al. 2016) http://www.elff.org.uk/ FLOSSMole (Howison, Conklin, and Crowston 2006) http://flossmole.org/ FLOSSMetrics (Herraiz et al. 2009): http://flossmetrics.org/ Qualitas Corpus (QC) (Tempero et al. 2010): http://qualitascorpus.com/ Sourcerer Project (Linstead et al. 2009): http://sourcerer.ics.uci.edu/ Ultimate Debian Database (UDD) (Nussbaum and Zacchiroli 2010) http://udd.debian.org/ SourceForge Research Data Archive (SRDA) (Van Antwerp and Madey 2008) http://zerlot.cse.nd.edu/ SECOLD (Source code ECOsystem Linked Data): http://www.secold.org/ Software-artifact Infrastructure Repository (SIR) [http://sir.unl.edu] OpenHub: https://www.openhub.net/ Not openly available (and mainly for effort estimation): The International Software Benchmarking Standards Group (ISBSG) http://www.isbsg.org/ TukuTuku http://www.metriq.biz/tukutuku/ Some papers and publications/theses that have been used in the literature: Helix Data Set (Vasa 2010): http://www.ict.swin.edu.au/research/projects/helix/ Bug Prediction Dataset (BPD) (D’Ambros, Lanza, and Robbes 2010,D’Ambros, Lanza, and Robbes (2011)): http://bug.inf.usi.ch/ Eclipse Bug Data (EBD) (Zimmermann, Premraj, and Zeller 2007,Nagappan et al. (2012)): http://www.st.cs.uni-saarland.de/softevo/bug-data/eclipse/ References "],
["open-toolsdashboards-to-extract-data.html", "Chapter 4 Open Tools/Dashboards to extract data 4.1 Issues", " Chapter 4 Open Tools/Dashboards to extract data Process to extract data: Process Within the open source community, several toolkits allow us to extract data that can be used to explore projects: Metrics Grimoire http://metricsgrimoire.github.io/ Grimoire SonarQube http://www.sonarqube.org/ SonarQube CKJMS http://gromit.iiar.pwr.wroc.pl/p_inf/ckjm/ Collects a large number of object-oriented metrics from code. 4.1 Issues There are problems such as different tools report different values for the same metric (Lincke, Lundberg, and Löwe 2008) It is well-know that the NASA datasets have some problems: (Gray et al. 2011b) The misuse of the NASA metrics data program data sets for automated software defect prediction (Shepperd et al. 2013) Data Quality: Some Comments on the NASA Software Defect Datasets Linking information: (Śliwerski, Zimmermann, and Zeller 2005)(Kim et al. 2006) SZZ algorithm References "],
["metrics-in-software-enginering-prediction.html", "Chapter 5 Metrics in Software Enginering Prediction 5.1 Complexity Metrics 5.2 Object-Oriented Metrics 5.3 Churn and Process Metrics 5.4 Defect Standards", " Chapter 5 Metrics in Software Enginering Prediction There are many types of metrics that can be easily collected from source code or software configuration management systems. Most work have focused on static metrics from code but recently other metrics (Source: Moser et al) Change Metrics describe how a file changed in the past, e.g., REVISIONS, REFACTORINGS, BUGFIXES, LOC_ADDED, etc. 5.1 Complexity Metrics These metrics have been used for quality assurance during: Development to obtain quality measures, code reviews etc. Testing to focus and prioritize testing effort, improve efficiency etc. Maintenance as indicators of comprehensibility of the modules etc. Generally, the developers or maintainers use rules of thumb or threshold values to keep modules, methods etc. within certain range. For example, if the cyclomatic complexity (\\(v(g)\\)) of a module is between 1 and 10, it is considered to have a very low risk of being defective; however, any value greater than 50 is considered to have an unmanageable complexity and risk. For the essential complexity (\\(ev(g)\\)), the threshold suggested is 4 etc. Although these metrics have been used for long time, there are no clear thresholds, for example, although McCabe suggests a threshold of 10 for \\(v(g)\\), NASA’s in–house studies for this metric concluded that a threshold of 20 can be a better predictor of a module being defective. Several authors have studied the relation between lines of code and defects, for example, (Zhang 2009) 5.1.1 McCabe’s Complexity Measures McCabe defined a set of complexity measures based on the graph of a program (McCabe 1976): Cyclomatic complexity \\(v(g)\\) is number of the control graph of the function \\(v(g)=e-n+2\\), where \\(e\\) represents the number of edges and \\(n\\) the number of nodes. Module Design Complexity \\(iv(G)\\) Essential Complexity \\(ev(G)\\) is a measure of the degree to which a module contains unstructured constructs 5.1.2 Halstead-s Software Science Late 70s, Halstead developed a set of metrics based on (Halstead 1977): Operators, keywords such as if, while, for, operators +, =, AND, etc. Operands, program variables and constants. All metrics are calculated using the following: \\(n_1\\), number of distinct operators \\(N_1\\), total number of operators \\(n_2\\), number of distinct operands \\(N_2\\), the total number of operands For example, given the following excerpt: if (MAX &lt; 2) { a = b * MAX; System.out.print(a); } We obtain the following values: \\(n1 = 6\\) (if, { }, System.out.print(), =, *, $&lt;$) \\(N1 = 6\\) (if, { }, System.out.print(), =, *, $&lt;$) \\(n2 = 4\\) (MAX, a, b, 2) \\(N2 = 6\\) (MAX, 2, a, b, MAX, a) Using this four variable, several metrics are calculated. Program vocabulary (\\(n = n_{1} + n_{2}\\)), as a measure of complexity, the less number of elements, the simpler it should be the program. Program length, \\(N = N_{1} + N_{2}\\), as a proxy for size, the larger it is, the harder it is to understand it. Program length can be estimated with \\(\\hat{N} = N_{1} log_2 n_1 + N_{2} log_2 n_2\\) (calculated program length) Volumen, \\(V = N \\cdot log_{2}(n)\\), number of bits needed to codify a program. While \\(length\\) is just the count of operators and operands, the \\(volumen\\) considers the number of distint operators. Given two same size programs, the one of with more distint operators will be harder to understand, i.e, greater volumen. Level, \\(L=V&#39;/V\\), where \\(V&#39;\\) is the potential volumen (minimun number of operands and operators needed and \\(V\\). As \\(V&#39;\\) is difficult to calculate, it can be approximated with \\(\\hat{L}= \\frac{n_1&#39;}{n_1} \\cdot \\frac{n_2}{N_2}\\), where \\(n_1&#39;\\) is the minimun number of distint operators. Mental effort, \\(E = V / L\\), where \\(L\\) depends on the programming language. It can also be calculated as \\(E = \\frac{n_1 N_2 N log_2 n}{2 n_2}\\) Time, \\(T=E/S\\), time to develop a program in seconds, being \\(S\\) the number of metal discrimitaions with a value between \\(5 \\leq S \\leq 20\\). Inteligence, \\(i = (2n_{2} / n_{1}N_{2} ) \\cdot (N_{1} + N_{2})log_{2}(n_{1} + n_{2})\\) Although these metrics are still highly used, they are also criticised, e.g., the mental discriminations is very hard to define and calculate. 5.2 Object-Oriented Metrics chidamber and Kemerer introduced a set of OO complexity Metrics that is being highly used (Chidamber and Kemerer 1994). Coupling Between Objects (CBO) for a class is a count of the number of other classes to which it is coupled. Coupling between two classes is said to occur when one class uses methods or variables of another class. COB is measured by counting the number of distinct non-inheritance related class hierarchies on which a class depends. Depth of Inheritance Tree (DIT) measures the maximum level of the inheritance hierarchy of a class; the root of the inheritance tree inherits from no class and is at level zero of the inheritance tree. Direct count of the levels of the levels in an inheritance hierarchy. Number of Children (NOC) is the number of immediate subclasses subordinate to a class in the hierarchy. NOC counts the number of subclasses belonging to a class. According to C&amp;K, NOC indicate the level of reuse, the likelihood of improper abstraction and as possible indication of the level of testing required. Response For a Class (RFC) is the count of the set of all methods that can be invoked in response to a message to an object of the class or by some method in the class. According to C&amp;K, RFC is a measure of the complexity of a class through the number of methods and the amount of communication with other classes. Weighted Methods per Class (WMC) measures the complexity of an individual class. If all methods are considered equally complex, then WMC is simply the number of methods defined in each class. C&amp;K suggests that WMC is intended to measure the complexity of a class. Therefore, it is an indicator of the effort needed to develop and maintain a class. Lack of Cohesion in Methods (LCOM) measures the extent to which methods reference the classes instance data. Etc. (Jureczko and Spinellis 2010) Jurescko and Spinellis (Bansiya and Davis 2002) A hierarchical model for object-oriented design quality assessment 5.3 Churn and Process Metrics Churned LOC is usually the number of source code lines added, deleted or changes to a component between a baseline version and a new version. Authors can also count the number of files created/deleted and the time span between those changes. Madeyski and Jurescko present a taxonomy of process metrics (Madeyski and Jureczko 2015): Metric Source code Developers Defects Number of Revisions (NR) X Number of Modified Lines (NML) X Is New (IN) X Number of Distinct Commiters (NDC) X X Number of Defects in Previous Version (NDPV) X X A Comparative Analysis of the Efficiency of Change Metrics and Static Code Attributes for Defect Prediction (Moser, Pedrycz, and Succi 2008) Metrics Definition Commits No. of commits a file has gone through Refactorings No. of times a file has been refactored Bugfixes No. of times a file has been involved in bug fixing Authors No. of distinct authors that made commits to the file LoC-Added No. of lines of code added to the file Max-LoC-Added Max no. of lines of code added for all commits Avg-LoC-Added Avg lines of code added per commit LoC-DELETED No. of lines of code deleted from the file Max-LoC-Added Max no. of lines of code deleted for all commits Avg-LoC-Added Avg lines of code deleted per commit CodeChurn Sum of all commits (added lines of code deleted lines of code) Max-CodeChurn Maximum CODECHURN for all commits Avg-CodeChurn Average CODECHURN per commit Max-Changeset Maximum number of files committed together with the repository Avg-Changeset Average number of files committed together with the repository Weighted-Age Age of a file normalized by added lines of code A further description of typical metrics used (Krishnan et al. 2011): 5.4 Defect Standards 5.4.1 IBM’s Orthogonal Defect Classificatoin (ODC) ODC Activity ODC Trigger ODC Impact Design review Design conformance Installability Code inspection Logic, flow Standards Unit test Backward compatibility Serviceability Build Lateral compatibility Integrity BVT Concurrency Security CVT Internal document Migration SVT Language dependency Reliability IVT Side effect Performance GVT Rare situations Documentation TVT Simple path Requirements DBCS IVT Complex path Maintenance Performance Coverage Usability Scalability Variation Accessibility ID review Sequencing Capability GUI review Interaction Acceptance Workload, stress Beta Recovery, exception Start, restart Hardware configuration Software configuration Screen text, characters Navigation Widget GUI behavior The impact is classified in the following categories: ODC Impact Description Installability The ability of the customer to prepare and place the software in position for use (not include Usability). Integrity/Security The protection of systems, programs, and data from inadvertent or malicious destruction, alteration, or disclosure. Performance The speed of the software as perceived by the customer and the customer’s end users, in terms of their ability to perform their tasks. Maintenance The ease of applying preventive or corrective fixes to the software. An example would be that the fixes can not be applied due to a bad medium. Another example might be that the application of maintenance requires a great deal of manual effort, or is calling many pre- or co-requisite requisite maintenance. Serviceability The ability to diagnose failures easily and quickly, with minimal impact to the customer. Migration The ease of upgrading to a current release, particularly in terms of the impact on existing customer data and operations. This would include planning for migration, where a lack of adequate documentation makes this task difficult. It would also apply in those situations where a new release of an existing product introduces changes effecting the external interfaces between the product and the customer’s applications. Documentation The degree to which the publication aids provided for understanding the structure and intended uses of the software are correct and complete. Usability The degree to which the software and publication aids enable the product to be easily understood and conveniently employed by its end user. Standards The degree to which the software complies with established pertinent standards. Reliability The ability of the software to consistently perform its intended function without unplanned interruption. Severe interruptions, such as ABEND and WAIT would always be considered reliability. Requirements A customer expectation, with regard to capability, which was not known, understood, or prioritized as a requirement for the current product or release. This value should be chosen during development for additions to the plan of record. It should also be selected, after a product is made generally available, when customers report that the capability of the function or product does not meet their expectation. Accessibility Ensuring that successful access to information and use of information technology is provided to people who have disabilities. Capability The ability of the software to perform its intended functions, and satisfy known requirements, where the customer is not impacted in any of the previous categories. These impact categories have been used to classify defects (Huang et al. 2011) References "],
["supervised-classification.html", "Chapter 6 Supervised Classification 6.1 Classification Trees 6.2 Rules 6.3 Distanced-based Methods 6.4 Neural Networks 6.5 Support Vector Machine 6.6 Probabilistic Methods 6.7 Linear Discriminant Analysis (LDA) 6.8 Binary Logistic Regression (BLR) 6.9 The caret package 6.10 Ensembles", " Chapter 6 Supervised Classification A classification problem can be defined as the induction, from a dataset \\(\\cal D\\), of a classification function \\(\\psi\\) that, given the attribute vector of an instance/example, returns a class \\({c}\\). A regression problem, on the other hand, returns an numeric value. Dataset, \\(\\cal D\\), is typically composed of \\(n\\) attributes and a class attribute \\(C\\). \\(Att_1\\) … \\(Att_n\\) \\(Class\\) \\(a_{11}\\) … \\(a_{1n}\\) \\(c_1\\) \\(a_{21}\\) … \\(a_{2n}\\) \\(c_2\\) … … … … \\(a_{m1}\\) … \\(a_{mn}\\) \\(c_m\\) Columns are usually called attributes or features. Typically, there is a class attribute, which can be numeric or discrete. When the class is numeric, it is a regression problem. With discrete values, we can talk about binary classification or multiclass (multinomial classification) when we have more than three values. Supervised Classification We have multiple types of models such as classification trees, rules, neural networks, and probabilistic classifiers that can be used to classify instances. Fernandez et al provide an extensive comparison of 176 classifiers using the UCI dataset (Fernández-Delgado et al. 2014). We will show the use of different classification techniques in the problem of defect prediction as running example. In this example,the different datasets are composed of classical metrics (Halstead or McCabe metrics) based on counts of operators/operands and like or object-oriented metrics (e.g. Chidamber and Kemerer) and the class attribute indicating whether the module or class was defective. Most works in defect predicition have compared and analysed different classifiers with different datasets. Some relevant works include: 6.1 Classification Trees There are several packages for inducing classification trees, for example with the party package (recursive partitioning): library(foreign) # To load arff file library(party) # Build a decision tree library(caret) jm1 &lt;- read.arff(&quot;./datasets/defectPred/D1/JM1.arff&quot;) str(jm1) ## &#39;data.frame&#39;: 9593 obs. of 22 variables: ## $ LOC_BLANK : num 447 37 11 106 101 67 105 18 39 143 ... ## $ BRANCH_COUNT : num 826 29 405 240 464 187 344 47 163 67 ... ## $ LOC_CODE_AND_COMMENT : num 12 8 0 7 11 4 9 0 1 7 ... ## $ LOC_COMMENTS : num 157 42 17 344 75 1 40 10 6 49 ... ## $ CYCLOMATIC_COMPLEXITY: num 470 19 404 127 263 94 207 24 94 34 ... ## $ DESIGN_COMPLEXITY : num 385 19 2 105 256 63 171 13 67 25 ... ## $ ESSENTIAL_COMPLEXITY : num 113 6 1 33 140 27 58 1 3 1 ... ## $ LOC_EXECUTABLE : num 2824 133 814 952 1339 ... ## $ HALSTEAD_CONTENT : num 210 108 101 218 106 ... ## $ HALSTEAD_DIFFICULTY : num 384.4 46.3 206 215.2 337.4 ... ## $ HALSTEAD_EFFORT : num 31079782 232044 4294926 10100867 12120796 ... ## $ HALSTEAD_ERROR_EST : num 26.95 1.67 6.95 15.65 11.98 ... ## $ HALSTEAD_LENGTH : num 8441 685 2033 5669 4308 ... ## $ HALSTEAD_LEVEL : num 0 0.02 0 0 0 0.02 0 0.03 0.01 0.02 ... ## $ HALSTEAD_PROG_TIME : num 1726655 12891 238607 561159 673378 ... ## $ HALSTEAD_VOLUME : num 80843 5009 20848 46944 35928 ... ## $ NUM_OPERANDS : num 3021 295 813 2301 1556 ... ## $ NUM_OPERATORS : num 5420 390 1220 3368 2752 ... ## $ NUM_UNIQUE_OPERANDS : num 609 121 811 262 226 167 279 47 117 355 ... ## $ NUM_UNIQUE_OPERATORS : num 155 38 411 49 98 27 105 18 52 23 ... ## $ LOC_TOTAL : num 3442 222 844 1411 1532 ... ## $ Defective : Factor w/ 2 levels &quot;N&quot;,&quot;Y&quot;: 2 2 2 2 2 2 2 2 1 2 ... # Stratified partition (training and test sets) set.seed(1234) inTrain &lt;- createDataPartition(y=jm1$Defective,p=.60,list=FALSE) jm1.train &lt;- jm1[inTrain,] jm1.test &lt;- jm1[-inTrain,] jm1.formula &lt;- jm1$Defective ~ . # formula approach: defect as dependent variable and the rest as independent variables jm1.ctree &lt;- ctree(jm1.formula, data = jm1.train) # predict on test data pred &lt;- predict(jm1.ctree, newdata = jm1.test) # check prediction result table(pred, jm1.test$Defective) ## ## pred N Y ## N 82 3 ## Y 3051 700 plot(jm1.ctree) Using the C50 package, there are two ways, specifying train and testing library(C50) require(utils) # c50t &lt;- C5.0(jm1.train[,-ncol(jm1.train)], jm1.train[,ncol(jm1.train)]) c50t2 &lt;- C5.0(Defective ~ ., jm1.train) summary(c50t) plot(c50t) c50tPred &lt;- predict(c50t, jm1.train) # table(c50tPred, jm1.train$Defective) Using the ‘rpart’ package # Using the &#39;rpart&#39; package library(rpart) jm1.rpart &lt;- rpart(Defective ~ ., data=jm1.train, parms = list(prior = c(.65,.35), split = &quot;information&quot;)) # par(mfrow = c(1,2), xpd = NA) plot(jm1.rpart) text(jm1.rpart, use.n = TRUE) jm1.rpart ## n= 5757 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 5757 2014.95000 N (0.6500000 0.3500000) ## 2) LOC_TOTAL&lt; 42.5 4309 1032.28000 N (0.7439560 0.2560440) * ## 3) LOC_TOTAL&gt;=42.5 1448 742.67870 Y (0.4304514 0.5695486) ## 6) NUM_UNIQUE_OPERANDS&lt; 59.5 1206 655.11750 Y (0.4726955 0.5273045) ## 12) LOC_BLANK&lt; 7.5 449 209.89060 N (0.5624895 0.4375105) ## 24) LOC_CODE_AND_COMMENT&lt; 3.5 412 171.72870 N (0.5988063 0.4011937) * ## 25) LOC_CODE_AND_COMMENT&gt;=3.5 37 13.53220 Y (0.2617743 0.7382257) * ## 13) LOC_BLANK&gt;=7.5 757 385.26960 Y (0.4251579 0.5748421) ## 26) HALSTEAD_DIFFICULTY&gt;=64.45 55 13.35668 N (0.7409751 0.2590249) * ## 27) HALSTEAD_DIFFICULTY&lt; 64.45 702 347.06100 Y (0.4061023 0.5938977) * ## 7) NUM_UNIQUE_OPERANDS&gt;=59.5 242 87.56126 Y (0.2579656 0.7420344) * library(rpart.plot) # asRules(jm1.rpart) # fancyRpartPlot(jm1.rpart) 6.2 Rules C5 Rules library(C50) c50r &lt;- C5.0(jm1.train[,-ncol(jm1.train)], jm1.train[,ncol(jm1.train)], rules = TRUE) summary(c50r) ## ## Call: ## C5.0.default(x = jm1.train[, -ncol(jm1.train)], y = ## jm1.train[, ncol(jm1.train)], rules = TRUE) ## ## ## C5.0 [Release 2.07 GPL Edition] Fri Jul 7 15:48:20 2017 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 5757 cases (22 attributes) from undefined.data ## ## Rules: ## ## Rule 1: (5512/923, lift 1.0) ## CYCLOMATIC_COMPLEXITY &lt;= 67 ## NUM_UNIQUE_OPERANDS &lt;= 59 ## -&gt; class N [0.832] ## ## Rule 2: (11/1, lift 4.6) ## LOC_BLANK &gt; 3 ## DESIGN_COMPLEXITY &gt; 3 ## LOC_EXECUTABLE &gt; 31 ## LOC_EXECUTABLE &lt;= 33 ## LOC_TOTAL &lt;= 42 ## -&gt; class Y [0.846] ## ## Rule 3: (25/5, lift 4.2) ## CYCLOMATIC_COMPLEXITY &gt; 67 ## -&gt; class Y [0.778] ## ## Rule 4: (242/110, lift 3.0) ## NUM_UNIQUE_OPERANDS &gt; 59 ## LOC_TOTAL &gt; 42 ## -&gt; class Y [0.545] ## ## Default class: N ## ## ## Evaluation on training data (5757 cases): ## ## Rules ## ---------------- ## No Errors ## ## 4 1025(17.8%) &lt;&lt; ## ## ## (a) (b) &lt;-classified as ## ---- ---- ## 4589 112 (a): class N ## 913 143 (b): class Y ## ## ## Attribute usage: ## ## 99.95% NUM_UNIQUE_OPERANDS ## 96.18% CYCLOMATIC_COMPLEXITY ## 4.39% LOC_TOTAL ## 0.19% LOC_BLANK ## 0.19% DESIGN_COMPLEXITY ## 0.19% LOC_EXECUTABLE ## ## ## Time: 0.2 secs # c50rPred &lt;- predict(c50r, jm1.train) # table(c50rPred, jm1.train$Defective) 6.3 Distanced-based Methods In this case, there is no model as such. Given a new instance to classify, this approach finds the closest \\(k\\)-neighbours to the given instance. (Source: Wikipedia - https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) library(class) m1 &lt;- knn(train=jm1.train[,-22], test=jm1.test[,-22], cl=jm1.train[,22], k=3) table(jm1.test[,22],m1) ## m1 ## N Y ## N 2827 306 ## Y 553 150 6.4 Neural Networks Neural Networks Neural Networks 6.5 Support Vector Machine (Source: wikipedia https://en.wikipedia.org/wiki/Support_vector_machine) 6.6 Probabilistic Methods 6.6.1 Naive Bayes Probabilistic graphical model assigning a probability to each possible outcome \\(p(C_k, x_1,\\ldots,x_n)\\) Naive Bayes Using the klaR package with caret: library(caret) library(klaR) ## Loading required package: MASS model &lt;- NaiveBayes(Defective ~ ., data = jm1.train) predictions &lt;- predict(model, jm1.test[,-22]) confusionMatrix(predictions$class, jm1.test$Defective) ## Confusion Matrix and Statistics ## ## Reference ## Prediction N Y ## N 2990 548 ## Y 143 155 ## ## Accuracy : 0.8199 ## 95% CI : (0.8073, 0.8319) ## No Information Rate : 0.8167 ## P-Value [Acc &gt; NIR] : 0.3168 ## ## Kappa : 0.2251 ## Mcnemar&#39;s Test P-Value : &lt;2e-16 ## ## Sensitivity : 0.9544 ## Specificity : 0.2205 ## Pos Pred Value : 0.8451 ## Neg Pred Value : 0.5201 ## Prevalence : 0.8167 ## Detection Rate : 0.7795 ## Detection Prevalence : 0.9223 ## Balanced Accuracy : 0.5874 ## ## &#39;Positive&#39; Class : N ## Using the e1071 package: library (e1071) n1 &lt;-naiveBayes(jm1.train$Defective ~ ., data=jm1.train) # Show first 3 results using &#39;class&#39; head(predict(n1,jm1.test, type = c(&quot;class&quot;)),3) # class by default ## [1] N Y Y ## Levels: N Y # Show first 3 results using &#39;raw&#39; head(predict(n1,jm1.test, type = c(&quot;raw&quot;)),3) ## N Y ## [1,] 1.0000000000 0.0000000 ## [2,] 0.0000000000 1.0000000 ## [3,] 0.0007540951 0.9992459 There are other variants such as TAN and KDB that do not assume the independece condition allowin us more complex structures. Naive Bayes Naive Bayes A comprehensice comparison of 6.7 Linear Discriminant Analysis (LDA) One classical approach to classification is Linear Discriminant Analysis (LDA), a generalization of Fisher’s linear discriminant, as a method used to find a linear combination of features to separate two or more classes. ldaModel &lt;- train (Defective ~ ., data=jm1.train, method=&quot;lda&quot;, preProc=c(&quot;center&quot;,&quot;scale&quot;)) ldaModel ## Linear Discriminant Analysis ## ## 5757 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 5757, 5757, 5757, 5757, 5757, 5757, ... ## Resampling results: ## ## Accuracy Kappa ## 0.8184746 0.1371754 We can observe that we are training our model using Defective ~ . as a formula were Defective is the class variable separed by ~ and the ´.´ means the rest of the variables. Also, we are using a filter for the training data to (preProc) to center and scale. Also, as stated in the documentation about the train method : &gt; http://topepo.github.io/caret/training.html ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,repeats=3) ldaModel &lt;- train (Defective ~ ., data=jm1.train, method=&quot;lda&quot;, trControl=ctrl, preProc=c(&quot;center&quot;,&quot;scale&quot;)) ldaModel ## Linear Discriminant Analysis ## ## 5757 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 5182, 5181, 5181, 5181, 5181, 5181, ... ## Resampling results: ## ## Accuracy Kappa ## 0.8167443 0.1308431 Instead of accuracy we can activate other metrics using summaryFunction=twoClassSummary such as ROC, sensitivity and specificity. To do so, we also need to speficy classProbs=TRUE. ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,repeats=3, classProbs=TRUE, summaryFunction=twoClassSummary) ldaModel3xcv10 &lt;- train (Defective ~ ., data=jm1.train, method=&quot;lda&quot;, trControl=ctrl, preProc=c(&quot;center&quot;,&quot;scale&quot;)) ldaModel3xcv10 ## Linear Discriminant Analysis ## ## 5757 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 5181, 5182, 5180, 5181, 5182, 5181, ... ## Resampling results: ## ## ROC Sens Spec ## 0.7073554 0.9731275 0.1189847 Most methods have parameters that need to be optimised and that is one of the plsFit3x10cv &lt;- train (Defective ~ ., data=jm1.train, method=&quot;pls&quot;, trControl=trainControl(classProbs=TRUE), metric=&quot;ROC&quot;, preProc=c(&quot;center&quot;,&quot;scale&quot;)) plsFit3x10cv ## Partial Least Squares ## ## 5757 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 5757, 5757, 5757, 5757, 5757, 5757, ... ## Resampling results across tuning parameters: ## ## ncomp Accuracy Kappa ## 1 0.8204925 0.06414865 ## 2 0.8196635 0.08261024 ## 3 0.8200588 0.08777031 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was ncomp = 1. plot(plsFit3x10cv) The parameter tuneLength allow us to specify the number values per parameter to consider. plsFit3x10cv &lt;- train (Defective ~ ., data=jm1.train, method=&quot;pls&quot;, trControl=ctrl, metric=&quot;ROC&quot;, tuneLength=5, preProc=c(&quot;center&quot;,&quot;scale&quot;)) plsFit3x10cv ## Partial Least Squares ## ## 5757 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 5182, 5182, 5181, 5181, 5181, 5181, ... ## Resampling results across tuning parameters: ## ## ncomp ROC Sens Spec ## 1 0.6930018 0.9951784 0.04482480 ## 2 0.6989680 0.9904273 0.06441450 ## 3 0.7019246 0.9892927 0.06441450 ## 4 0.7044831 0.9887963 0.06693621 ## 5 0.7052068 0.9884417 0.06756813 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was ncomp = 5. plot(plsFit3x10cv) Finally to predict new cases, caret will use the best classfier obtained for prediction. plsProbs &lt;- predict(plsFit3x10cv, newdata = jm1.test, type = &quot;prob&quot;) plsClasses &lt;- predict(plsFit3x10cv, newdata = jm1.test, type = &quot;raw&quot;) confusionMatrix(data=plsClasses,jm1.test$Defective) ## Confusion Matrix and Statistics ## ## Reference ## Prediction N Y ## N 3108 643 ## Y 25 60 ## ## Accuracy : 0.8259 ## 95% CI : (0.8135, 0.8377) ## No Information Rate : 0.8167 ## P-Value [Acc &gt; NIR] : 0.07427 ## ## Kappa : 0.1174 ## Mcnemar&#39;s Test P-Value : &lt; 2e-16 ## ## Sensitivity : 0.99202 ## Specificity : 0.08535 ## Pos Pred Value : 0.82858 ## Neg Pred Value : 0.70588 ## Prevalence : 0.81674 ## Detection Rate : 0.81022 ## Detection Prevalence : 0.97784 ## Balanced Accuracy : 0.53868 ## ## &#39;Positive&#39; Class : N ## 6.7.1 Predicting the number of defects (numerical class) From the Bug Prediction Repository (BPR) http://bug.inf.usi.ch/download.php Some datasets contain CK and other 11 object-oriented metrics for the last version of the system plus categorized (with severity and priority) post-release defects. Using such dataset: jdt &lt;- read.csv(&quot;./datasets/defectPred/BPD/single-version-ck-oo-EclipseJDTCore.csv&quot;, sep=&quot;;&quot;) # We just use the number of bugs, so we removed others jdt$classname &lt;- NULL jdt$nonTrivialBugs &lt;- NULL jdt$majorBugs &lt;- NULL jdt$minorBugs &lt;- NULL jdt$criticalBugs &lt;- NULL jdt$highPriorityBugs &lt;- NULL jdt$X &lt;- NULL # Caret library(caret) # Split data into training and test datasets set.seed(1) inTrain &lt;- createDataPartition(y=jdt$bugs,p=.8,list=FALSE) jdt.train &lt;- jdt[inTrain,] jdt.test &lt;- jdt[-inTrain,] ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,repeats=3) glmModel &lt;- train (bugs ~ ., data=jdt.train, method=&quot;glm&quot;, trControl=ctrl, preProc=c(&quot;center&quot;,&quot;scale&quot;)) glmModel ## Generalized Linear Model ## ## 798 samples ## 17 predictors ## ## Pre-processing: centered (17), scaled (17) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 718, 718, 718, 718, 719, 718, ... ## Resampling results: ## ## RMSE Rsquared ## 0.8411011 0.3855316 Others such as Elasticnet: glmnetModel &lt;- train (bugs ~ ., data=jdt.train, method=&quot;glmnet&quot;, trControl=ctrl, preProc=c(&quot;center&quot;,&quot;scale&quot;)) ## Loading required package: glmnet ## Loading required package: Matrix ## Loading required package: foreach ## Loaded glmnet 2.0-10 glmnetModel ## glmnet ## ## 798 samples ## 17 predictors ## ## Pre-processing: centered (17), scaled (17) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 718, 718, 718, 718, 718, 718, ... ## Resampling results across tuning parameters: ## ## alpha lambda RMSE Rsquared ## 0.10 0.001202348 0.8127568 0.3411090 ## 0.10 0.012023480 0.8183111 0.3344713 ## 0.10 0.120234797 0.8077544 0.3396316 ## 0.55 0.001202348 0.8119513 0.3412175 ## 0.55 0.012023480 0.8227484 0.3268770 ## 0.55 0.120234797 0.8117291 0.3473930 ## 1.00 0.001202348 0.8116078 0.3407650 ## 1.00 0.012023480 0.8189354 0.3309040 ## 1.00 0.120234797 0.8167381 0.3445179 ## ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were alpha = 0.1 and lambda ## = 0.1202348. 6.8 Binary Logistic Regression (BLR) Binary Logistic Regression (BLR) can models fault-proneness as follows \\[fp(X) = \\frac{e^{logit()}}{1 + e^{logit(X)}}\\] where the simplest form for logit is: \\(logit(X) = c_{0} + c_{1}X\\) jdt &lt;- read.csv(&quot;./datasets/defectPred/BPD/single-version-ck-oo-EclipseJDTCore.csv&quot;, sep=&quot;;&quot;) # Caret library(caret) # Convert the response variable into a boolean variable (0/1) jdt$bugs[jdt$bugs&gt;=1]&lt;-1 cbo &lt;- jdt$cbo bugs &lt;- jdt$bugs # Split data into training and test datasets jdt2 = data.frame(cbo, bugs) inTrain &lt;- createDataPartition(y=jdt2$bugs,p=.8,list=FALSE) jdtTrain &lt;- jdt2[inTrain,] jdtTest &lt;- jdt2[-inTrain,] BLR models fault-proneness are as follows \\(fp(X) = \\frac{e^{logit()}}{1 + e^{logit(X)}}\\) where the simplest form for logit is \\(logit(X) = c_{0} + c_{1}X\\) # logit regression # glmLogit &lt;- train (bugs ~ ., data=jdt.train, method=&quot;glm&quot;, family=binomial(link = logit)) glmLogit &lt;- glm (bugs ~ ., data=jdtTrain, family=binomial(link = logit)) summary(glmLogit) ## ## Call: ## glm(formula = bugs ~ ., family = binomial(link = logit), data = jdtTrain) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.5734 -0.6125 -0.5378 -0.4968 2.0992 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.086378 0.134620 -15.498 &lt; 2e-16 *** ## cbo 0.056462 0.007045 8.014 1.11e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 831.84 on 797 degrees of freedom ## Residual deviance: 725.93 on 796 degrees of freedom ## AIC: 729.93 ## ## Number of Fisher Scoring iterations: 5 Predict a single point: newData = data.frame(cbo = 3) predict(glmLogit, newData, type = &quot;response&quot;) ## 1 ## 0.1281974 Draw the results, modified from: http://www.shizukalab.com/toolkits/plotting-logistic-regression-in-r results &lt;- predict(glmLogit, jdtTest, type = &quot;response&quot;) range(jdtTrain$cbo) ## [1] 0 156 range(results) ## [1] 0.1104278 0.9840854 plot(jdt2$cbo,jdt2$bugs) curve(predict(glmLogit, data.frame(cbo=x), type = &quot;response&quot;),add=TRUE) # points(jdtTrain$cbo,fitted(glmLogit)) Another type of graph: library(popbio) ## ## Attaching package: &#39;popbio&#39; ## The following object is masked from &#39;package:caret&#39;: ## ## sensitivity logi.hist.plot(jdt2$cbo,jdt2$bugs,boxp=FALSE,type=&quot;hist&quot;,col=&quot;gray&quot;) 6.9 The caret package There are hundreds of packages to perform classification task in R, but many of those can be used throught the ‘caret’ package which helps with many of the data mining process task as described next. The caret packagehttp://topepo.github.io/caret/ provides a unified interface for modeling and prediction with around 150 different models with tools for: + data splitting + pre-processing + feature selection + model tuning using resampling + variable importance estimation, etc. Website: http://caret.r-forge.r-project.org JSS Paper: www.jstatsoft.org/v28/i05/paper Book: Applied Predictive Modeling 6.10 Ensembles Ensembles or meta-learners combine multiple models to obtain better predictions. They are typically classified as Bagging, Boosting and Stacking (Stacked generalization). Bagging (Breiman 1996) (also known as Bootstrap aggregating) is an ensemble technique in which a base learner is applied to multiple equal size datasets created from the original data using bootstraping. Predictions are based on voting of the individual predictions. An advantage of bagging is that it does not require any modification to the learning algorithm and takes advantage of the instability of the base classifier to create diversity among individual ensembles so that individual members of the ensemble perform well in different regions of the data. Bagging does not perform well with classifiers if their output is robust to perturbation of the data such as nearest-neighbour (NN) classifiers. Boosting techniques generate multiple models that complement each other inducing models that improve regions of the data where previous induced models preformed poorly. This is achieved by increasing the weights of instances wrongly classified, so new learners focus on those instances. Finally, classification is based on a weighted voted among all members of the ensemble. In particular, AdaBoost (Adaptive Boosting) is a popular boosting algorithm for classification (Freund, Schapire, and Abe 1999). The set of training examples is assigned an equal weight at the beginning and the weight of instances is either increased or decreased depending on whether the learner classified that instance incorrectly or not. The following iterations focus on those instances with higher weights. AdaBoost can be applied to any base learner. Stacking (Stacked generalisation) which combines different types of models An very propular ensemble is Rotation Forests [40] combine randomly chosen subsets of attributes (random subspaces) and bagging approaches with principal components feature generation to construct an ensemble of decision trees. Principal Component Analysis is used as a feature selection technique combining subsets of attributes which are used with a bootstrapped subset of the training data by the base classifier. # Load library library(randomForest) library(foreign) # To load arff file #library(party) # Build a decision tree #library(caret) kc1 &lt;- read.arff(&quot;./datasets/defectPred/D1/KC1.arff&quot;) kc1.rf &lt;- randomForest(kc1$Defective ~ . ,data = kc1, na.action=na.omit) print(kc1.rf) ## ## Call: ## randomForest(formula = kc1$Defective ~ ., data = kc1, na.action = na.omit) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 4 ## ## OOB estimate of error rate: 13.6% ## Confusion matrix: ## N Y class.error ## N 1703 68 0.03839639 ## Y 217 108 0.66769231 plot(kc1.rf) A problem with ensembles is that their models are difficult to interpret (they behave as blackboxes) in comparison to decision trees or rules which provide an explanation of their decision making process. References "],
["regression-tecniques-in-software-defect-prediction.html", "Chapter 7 Regression tecniques in Software defect prediction 7.1 Zero Poison 7.2 Poisson Regression 7.3 Compare to Null Model (intercept) 7.4 Negative Binomial 7.5 Compare Models Fit. AIC and BIC 7.6 Compare prediction of Defects 7.7 Plot predictions 7.8 Regression diagnostics 7.9 References and R code used", " Chapter 7 Regression tecniques in Software defect prediction The purpose of this Section is to show the basic ways of estimating software defects by means of several regression models. We apply the techniques on the Equinox dataset. In what follows we use the same data points for training and testing, for the sake of clarity. 7.1 Zero Poison When estimating the number of defects but we have many zeroes, we can use different types of zero Poison regression. 7.1.1 Load packages ## Loading required package: pscl ## Classes and Methods for R developed in the ## Political Science Computational Laboratory ## Department of Political Science ## Stanford University ## Simon Jackman ## hurdle and zeroinfl functions by Achim Zeileis ## Loading required package: boot ## ## Attaching package: &#39;boot&#39; ## The following object is masked from &#39;package:lattice&#39;: ## ## melanoma ## ## Attaching package: &#39;corrplot&#39; ## The following object is masked from &#39;package:pls&#39;: ## ## corrplot 7.1.2 The number of Software Defects. Count Data The number of software defects found in a software product can be assimilated to the “count data” concept that is used in many disciplines, because the outcome,number of defects, of whatever software process is a count. There are several ways of analyzing count data. The classical Poisson, negative binomial regression model can be augmented with zero-inflated poisson and zero-inflated negative binomial models to cope with the excess of zeros in the count data. Zero-inflated means that the response variable -software defects- contains more zeros than expected, based on the Poisson or negative binomial distribution. A simple histogram may show the trend. Count variables appear in different areas and have common properties: their range is a non-negative integer(0, 1, ….); and their values are skewed towards zero, because few values are high. 7.1.3 Normal Regression The normal regression provided by the general linear model (glm in R) is not appropriate for count data because of the non-normality of the residuals. Usually the variance of the residuals increase with their value. 7.1.4 Poisson Regression This model can be used if there is no excess in the number of zeros. The poisson model is the most common and it is a probability distribution that is controlled by a single parameter, \\(\\lambda\\) 7.1.5 Negative binomial The negative binomial takes into account overdispersion in the count outcome. When there are more high values or zeros than expected the mean and the variance of the sample data are different. 7.1.6 Zero-Inflated Poisson Regression ZIP This is a model that can deal with the excess of zeros by explicitly modeling the part that generates the false zeros. There may be different sources for the excess of zeros. The count process is modeled by a Poisson model. 7.1.7 Zero-Inflated Negative Binomial ZINB In this case, besides taking into account the excess of zeros the count process is modeled by a Negative Binomial model that allows for overdispersion from the non-zero counts. 7.1.8 Read Data # setwd(&quot;~/DocProjects/zeropoisson&quot;) auxfile &lt;- read.table(&quot;./datasets/defectPred/BPD/single-version-ck-oo.csv&quot;, header = TRUE, sep = &quot;;&quot;) myvars &lt;- c(&#39;bugs&#39;, &#39;wmc&#39;, &#39;rfc&#39;, &#39;cbo&#39;, &#39;lcom&#39;, &#39;numberOfLinesOfCode&#39;) # wmd weighted methods per class Despite its long name, WMC is simply the method count for a class. # rfc response for a class set of methods that can potentially be executed in response to a message received by an object of that class # cbo coupling between objects number of classes to which a class is coupled # lcom lack of cohesion equinox &lt;- auxfile[myvars] names(equinox)[names(equinox) == &#39;bugs&#39;] &lt;- &#39;count&#39; 7.1.9 Plot histogram The first histogram shows the high number of modules with no defects. The second histogram shows the distribution of the non-zero values. hist(equinox$count,breaks=&quot;FD&quot;) ## histogram with x axis in log10 scale ggplot(equinox, aes(count)) + geom_histogram() + scale_x_log10() ## Warning: Transformation introduced infinite values in continuous x-axis ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 195 rows containing non-finite values (stat_bin). 7.1.10 Correlation among variables The correlation plots and the tables show a strong correlation among some variables. ## count wmc rfc cbo lcom ## count 1.0000000 0.6151012 0.5786541 0.7065440 0.6327475 ## wmc 0.6151012 1.0000000 0.9656904 0.7647778 0.8208613 ## rfc 0.5786541 0.9656904 1.0000000 0.7804111 0.8283116 ## cbo 0.7065440 0.7647778 0.7804111 1.0000000 0.7208646 ## lcom 0.6327475 0.8208613 0.8283116 0.7208646 1.0000000 ## numberOfLinesOfCode 0.6404813 0.9725794 0.9683453 0.7932072 0.8263641 ## numberOfLinesOfCode ## count 0.6404813 ## wmc 0.9725794 ## rfc 0.9683453 ## cbo 0.7932072 ## lcom 0.8263641 ## numberOfLinesOfCode 1.0000000 ## count wmc rfc cbo ## Min. : 0.0000 Min. : 0.00 Min. : 0.00 Min. : 0.000 ## 1st Qu.: 0.0000 1st Qu.: 2.00 1st Qu.: 5.00 1st Qu.: 2.000 ## Median : 0.0000 Median : 12.00 Median : 18.00 Median : 6.000 ## Mean : 0.7531 Mean : 32.64 Mean : 58.34 Mean : 9.673 ## 3rd Qu.: 1.0000 3rd Qu.: 36.00 3rd Qu.: 61.50 3rd Qu.:13.250 ## Max. :13.0000 Max. :534.00 Max. :1009.00 Max. :77.000 ## lcom numberOfLinesOfCode ## Min. : 0.0 Min. : 0.00 ## 1st Qu.: 1.0 1st Qu.: 7.75 ## Median : 15.0 Median : 45.00 ## Mean : 124.2 Mean : 122.02 ## 3rd Qu.: 66.0 3rd Qu.: 130.00 ## Max. :2775.0 Max. :1805.00 7.1.11 Classical Regression First we fit a classical regression model using the variables cbo, lcom, wmc, rfc. Please note that the last section that compares models with different combinations of variables. The parameters of interest are the intercept and the slope coefficients ## ## Call: ## glm(formula = count ~ cbo + lcom + wmc + rfc, family = gaussian, ## data = equinox) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.5540 -0.4498 -0.0248 0.3129 6.2884 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.0816095 0.0749013 -1.090 0.277 ## cbo 0.0750713 0.0078977 9.505 &lt; 2e-16 *** ## lcom 0.0012674 0.0002786 4.549 7.68e-06 *** ## wmc 0.0154600 0.0034539 4.476 1.06e-05 *** ## rfc -0.0094888 0.0018973 -5.001 9.42e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.9355133) ## ## Null deviance: 686.25 on 323 degrees of freedom ## Residual deviance: 298.43 on 319 degrees of freedom ## AIC: 904.84 ## ## Number of Fisher Scoring iterations: 2 7.2 Poisson Regression Regression coefficients must be interpreted with the log transformation. ## ## Call: ## glm(formula = count ~ cbo + lcom + wmc + rfc, family = poisson, ## data = equinox) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.0940 -0.8936 -0.7937 0.5785 2.7547 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.1797490 0.1011751 -11.660 &lt; 2e-16 *** ## cbo 0.0518008 0.0056195 9.218 &lt; 2e-16 *** ## lcom 0.0001574 0.0001744 0.903 0.367 ## wmc 0.0114261 0.0018959 6.027 1.67e-09 *** ## rfc -0.0060023 0.0011625 -5.163 2.43e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 588.43 on 323 degrees of freedom ## Residual deviance: 316.67 on 319 degrees of freedom ## AIC: 632.15 ## ## Number of Fisher Scoring iterations: 5 7.3 Compare to Null Model (intercept) The chi-squared test on the difference of log likelihoods is used to compare the poisson model to the intercept. It yields a high significant p-value; thus, the model is statistically significant. All of the predictors in both the count and inflation portions of the model are statistically significant. This model fits the data significantly better than the null model, i.e., the intercept-only model. Since we have five predictor variables in the full model, the degrees of freedom for chi-squared test is 5. # five variables in the full model --&gt; df = 5 nullmodel &lt;- update(defects_pois, . ~ 1) pchisq(2 * (logLik(defects_pois) - logLik(nullmodel)), df = 5, lower.tail=FALSE) ## &#39;log Lik.&#39; 1.171544e-56 (df=5) 7.4 Negative Binomial # negative binomial regression defects_negbinom &lt;- glm.nb(count ~ cbo + lcom + wmc + rfc, data = equinox) ## Warning in glm.nb(count ~ cbo + lcom + wmc + rfc, data = equinox): ## alternation limit reached # summary of results summary(defects_negbinom) ## ## Call: ## glm.nb(formula = count ~ cbo + lcom + wmc + rfc, data = equinox, ## init.theta = 6.219388589, link = log) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4508 -0.8535 -0.7505 0.5116 2.5068 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.2780140 0.1140133 -11.209 &lt; 2e-16 *** ## cbo 0.0583198 0.0071047 8.209 2.24e-16 *** ## lcom 0.0001011 0.0002150 0.470 0.638 ## wmc 0.0118841 0.0024655 4.820 1.43e-06 *** ## rfc -0.0061535 0.0014455 -4.257 2.07e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(6.2194) family taken to be 1) ## ## Null deviance: 507.19 on 323 degrees of freedom ## Residual deviance: 278.33 on 319 degrees of freedom ## AIC: 628.55 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 6.22 ## Std. Err.: 3.14 ## Warning while fitting theta: alternation limit reached ## ## 2 x log-likelihood: -616.551 # overdispersion summary(defects_negbinom)$theta ## [1] 6.219389 7.4.1 Zero Inflated Poisson # zero-inflated Poisson regression # the | seperates the count model from the logistic model defects_zip &lt;- zeroinfl(count ~ cbo + lcom + wmc + rfc | numberOfLinesOfCode, data = equinox, link = &quot;logit&quot;, dist = &quot;poisson&quot;, trace = FALSE) # summary of results summary(defects_zip) ## ## Call: ## zeroinfl(formula = count ~ cbo + lcom + wmc + rfc | numberOfLinesOfCode, ## data = equinox, dist = &quot;poisson&quot;, link = &quot;logit&quot;, trace = FALSE) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -2.3683 -0.6880 -0.2784 0.4692 3.8402 ## ## Count model coefficients (poisson with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.8641763 0.1240264 -6.968 3.22e-12 *** ## cbo 0.0433784 0.0055946 7.754 8.93e-15 *** ## lcom 0.0002456 0.0001123 2.188 0.0287 * ## wmc 0.0097291 0.0019739 4.929 8.27e-07 *** ## rfc -0.0051994 0.0010795 -4.816 1.46e-06 *** ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.6749 0.6869 2.438 0.0147 * ## numberOfLinesOfCode -0.1657 0.1118 -1.483 0.1380 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Number of iterations in BFGS optimization: 28 ## Log-likelihood: -296.5 on 7 Df 7.4.2 Zero Inflated Negative Binomial # zero-inflated negative binomial regression # the | seperates the count model from the logistic model defects_zinb &lt;- zeroinfl(count ~ cbo + lcom + wmc + rfc | numberOfLinesOfCode, data = equinox, link = &quot;logit&quot;, dist = &quot;negbin&quot;, trace = FALSE, EM = FALSE) ## Warning in sqrt(diag(vc)[np]): NaNs produced # summary of results summary(defects_zinb) ## Warning in sqrt(diag(object$vcov)): NaNs produced ## ## Call: ## zeroinfl(formula = count ~ cbo + lcom + wmc + rfc | numberOfLinesOfCode, ## data = equinox, dist = &quot;negbin&quot;, link = &quot;logit&quot;, trace = FALSE, ## EM = FALSE) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -1.8809 -0.6721 -0.2745 0.4487 3.7922 ## ## Count model coefficients (negbin with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.9088131 0.1282227 -7.088 1.36e-12 *** ## cbo 0.0450277 0.0059527 7.564 3.90e-14 *** ## lcom 0.0002357 NA NA NA ## wmc 0.0100136 0.0022232 4.504 6.67e-06 *** ## rfc -0.0052856 0.0013063 -4.046 5.20e-05 *** ## Log(theta) 2.6591002 NA NA NA ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.7143 0.7618 2.250 0.0244 * ## numberOfLinesOfCode -0.1894 0.1332 -1.422 0.1551 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Theta = 14.2834 ## Number of iterations in BFGS optimization: 42 ## Log-likelihood: -295.8 on 8 Df 7.4.3 Comparing models with Vuong test The Vuong test compares the zero-inflated model with an ordinary Poisson regression model. The result of the test statistic is significant, indicating that the zero-inflated model is superior to the standard Poisson model. vuong(defects_zip, defects_pois) ## Vuong Non-Nested Hypothesis Test-Statistic: ## (test-statistic is asymptotically distributed N(0,1) under the ## null that the models are indistinguishible) ## ------------------------------------------------------------- ## Vuong z-statistic H_A p-value ## Raw 3.143494 model1 &gt; model2 0.00083472 ## AIC-corrected 2.713456 model1 &gt; model2 0.00332927 ## BIC-corrected 1.900524 model1 &gt; model2 0.02868219 vuong(defects_zip, defects_negbinom) ## Vuong Non-Nested Hypothesis Test-Statistic: ## (test-statistic is asymptotically distributed N(0,1) under the ## null that the models are indistinguishible) ## ------------------------------------------------------------- ## Vuong z-statistic H_A p-value ## Raw 2.468425 model1 &gt; model2 0.0067854 ## AIC-corrected 2.050671 model1 &gt; model2 0.0201495 ## BIC-corrected 1.260961 model1 &gt; model2 0.1036615 7.4.4 Compute Confidence Intervals for intercept and variables. ZIP version We may confidence intervals for all the parameters and the exponentiated parameters using bootstrapping. We may compare these results with the regular confidence intervals based on the standard errors. dput(round(coef(defects_zip, &quot;count&quot;), 4)) ## structure(c(-0.8642, 0.0434, 2e-04, 0.0097, -0.0052), .Names = c(&quot;(Intercept)&quot;, ## &quot;cbo&quot;, &quot;lcom&quot;, &quot;wmc&quot;, &quot;rfc&quot;)) dput(round(coef(defects_zip, &quot;zero&quot;), 4)) ## structure(c(1.6749, -0.1657), .Names = c(&quot;(Intercept)&quot;, &quot;numberOfLinesOfCode&quot; ## )) # change list of parameters 1, 3, 5 etc ### FINAL MODEL SELECTED ZERO INFLATED POISSON f &lt;- function(data, i) { require(pscl) m &lt;- zeroinfl(count ~ cbo + lcom + wmc + rfc | numberOfLinesOfCode, data = data[i, ], dist = &quot;poisson&quot;, start = list(count = c(-0.8642, 0.0434, 2e-04, 0.0097, -0.0052), zero = c(1.6749, -0.1657))) as.vector(t(do.call(rbind, coef(summary(m)))[, 1:2])) } set.seed(10) (res &lt;- boot(equinox, f, R = 1200, parallel = &quot;snow&quot;, ncpus = 4)) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = equinox, statistic = f, R = 1200, parallel = &quot;snow&quot;, ## ncpus = 4) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* -0.8641784331 -2.303129e-02 1.316306e-01 ## t2* 0.1240279149 1.058718e-02 3.483776e-02 ## t3* 0.0433783993 5.727146e-04 7.267716e-03 ## t4* 0.0055945857 1.619122e-03 2.941890e-03 ## t5* 0.0002456421 -1.532408e-05 2.769170e-04 ## t6* 0.0001122920 6.739868e-05 1.380387e-04 ## t7* 0.0097291337 4.351203e-04 2.957174e-03 ## t8* 0.0019739001 5.735321e-04 1.386695e-03 ## t9* -0.0051993761 -1.748856e-04 1.561095e-03 ## t10* 0.0010795051 3.518046e-04 5.972876e-04 ## t11* 1.6750251424 6.975388e-01 5.261811e+00 ## t12* 0.6869578264 9.237353e-01 2.058666e+01 ## t13* -0.1657573309 -9.363010e-02 5.468317e-01 ## t14* 0.1117748277 6.984320e-02 2.075220e+00 ## basic parameter estimates with percentile and bias adjusted CIs parms &lt;- t(sapply(c(1, 3, 5, 7, 9, 11, 13), function(i) { out &lt;- boot.ci(res, index = c(i, i + 1), type = c(&quot;perc&quot;, &quot;bca&quot;)) with(out, c(Est = t0, pLL = percent[4], pUL = percent[5], bcaLL = bca[4], bcaUL = bca[5])) })) print(&quot;Confidence intervals for ZIP, BOOTED&quot;) ## [1] &quot;Confidence intervals for ZIP, BOOTED&quot; ## add row names names(coef(defects_zip)) ## [1] &quot;count_(Intercept)&quot; &quot;count_cbo&quot; ## [3] &quot;count_lcom&quot; &quot;count_wmc&quot; ## [5] &quot;count_rfc&quot; &quot;zero_(Intercept)&quot; ## [7] &quot;zero_numberOfLinesOfCode&quot; nrow(parms) ## [1] 7 row.names(parms) &lt;- names(coef(defects_zip)) ## print results parms ## Est pLL pUL ## count_(Intercept) -0.8641784331 -1.155311776 -0.6405252226 ## count_cbo 0.0433783993 0.030104286 0.0594077144 ## count_lcom 0.0002456421 -0.000363527 0.0007406458 ## count_wmc 0.0097291337 0.005179901 0.0187585385 ## count_rfc -0.0051993761 -0.009329260 -0.0028504289 ## zero_(Intercept) 1.6750251424 0.712867236 5.0130270331 ## zero_numberOfLinesOfCode -0.1657573309 -0.594861582 -0.0682408298 ## bcaLL bcaUL ## count_(Intercept) -1.105649479 -0.565552024 ## count_cbo 0.026293665 0.058188886 ## count_lcom -0.000324895 0.000781505 ## count_wmc 0.005126268 0.018329267 ## count_rfc -0.009010388 -0.002658944 ## zero_(Intercept) 0.377152297 3.928757887 ## zero_numberOfLinesOfCode -0.547781601 -0.058402161 print(&quot;Confidence intervals for ZIP , NORMAL BASED APPROXIMATION&quot;) ## [1] &quot;Confidence intervals for ZIP , NORMAL BASED APPROXIMATION&quot; ## compare with normal based approximation confint(defects_zip) ## 2.5 % 97.5 % ## count_(Intercept) -1.107264e+00 -0.6210889471 ## count_cbo 3.241321e-02 0.0543435607 ## count_lcom 2.555382e-05 0.0004657304 ## count_wmc 5.860348e-03 0.0135978860 ## count_rfc -7.315162e-03 -0.0030835806 ## zero_(Intercept) 3.286748e-01 3.0212006348 ## zero_numberOfLinesOfCode -3.847732e-01 0.0532838577 Estimate the incident risk ratio (IRR) for the Poisson model and odds ratio (OR) for the logistic (zero inflation) model. ## exponentiated parameter estimates with percentile and bias adjusted CIs expparms &lt;- t(sapply(c(1, 3, 5, 7, 9, 11, 13), function(i) { out &lt;- boot.ci(res, index = c(i, i + 1), type = c(&quot;perc&quot;, &quot;bca&quot;), h = exp) with(out, c(Est = t0, pLL = percent[4], pUL = percent[5], bcaLL = bca[4], bcaUL = bca[5])) })) ## add row names row.names(expparms) &lt;- names(coef(defects_zip)) ## print results expparms ## Est pLL pUL bcaLL ## count_(Intercept) 0.4213976 0.3149594 0.5270156 0.3309958 ## count_cbo 1.0443330 1.0305620 1.0612078 1.0266424 ## count_lcom 1.0002457 0.9996365 1.0007409 0.9996752 ## count_wmc 1.0097766 1.0051933 1.0189356 1.0051394 ## count_rfc 0.9948141 0.9907141 0.9971536 0.9910301 ## zero_(Intercept) 5.3389294 2.0398324 150.3599485 1.4581264 ## zero_numberOfLinesOfCode 0.8472518 0.5516390 0.9340355 0.5782311 ## bcaUL ## count_(Intercept) 0.5680465 ## count_cbo 1.0599152 ## count_lcom 1.0007818 ## count_wmc 1.0184983 ## count_rfc 0.9973446 ## zero_(Intercept) 50.8437847 ## zero_numberOfLinesOfCode 0.9432705 7.5 Compare Models Fit. AIC and BIC There are several measures that can be used to assess model fit. Here we use information-based measures. The traditional Akaike’s information criterion (AIC) selects the model that has the smallest AIC value. The Schwartz’s Bayesian Information criterion (BIC) also minimizes model complexity. #Model Fit # AIC values AIC(defects_normal) ## [1] 904.8354 AIC(defects_pois) ## [1] 632.1547 AIC(defects_negbinom) ## [1] 628.5507 AIC(defects_zip) ## [1] 606.9155 AIC(defects_zinb) ## [1] 607.5639 # AIC weights compare_models &lt;- list( ) compare_models[[1]] &lt;- defects_normal compare_models[[2]] &lt;- defects_pois compare_models[[3]] &lt;- defects_negbinom compare_models[[4]] &lt;- defects_zip compare_models[[5]] &lt;- defects_zinb compare_names &lt;- c(&quot;Typical&quot;, &quot;Poisson&quot;, &quot;NB&quot;, &quot;ZIP&quot;, &quot;ZINB&quot;) names(compare_models) &lt;- compare_names compare_results &lt;- data.frame(models = compare_names) compare_results$aic.val &lt;- unlist(lapply(compare_models, AIC)) compare_results$aic.delta &lt;- compare_results$aic.val-min(compare_results$aic.val) compare_results$aic.likelihood &lt;- exp(-0.5* compare_results$aic.delta) compare_results$aic.weight &lt;- compare_results$aic.likelihood/sum(compare_results$aic.likelihood) # BIC values print(&quot;Schwarz&#39;s Bayesian criterion&quot;) ## [1] &quot;Schwarz&#39;s Bayesian criterion&quot; AIC(defects_normal, k = log(nrow(equinox))) ## [1] 927.5198 AIC(defects_pois, k = log(nrow(equinox))) ## [1] 651.0584 AIC(defects_negbinom, k = log(nrow(equinox))) ## [1] 651.2352 AIC(defects_zip, k = log(nrow(equinox))) ## [1] 633.3807 AIC(defects_zinb, k = log(nrow(equinox))) ## [1] 637.8098 7.6 Compare prediction of Defects # observed zero counts # actual sum(equinox$count &lt; 1) ## [1] 195 # typical sum(dnorm(0, fitted(defects_normal))) ## [1] 97.76806 # poisson sum(dpois(0, fitted(defects_pois))) ## [1] 188.7356 # nb sum(dnbinom(0, mu = fitted(defects_negbinom), size = defects_negbinom$theta)) ## [1] 195.8165 # zip sum(predict(defects_zip, type = &quot;prob&quot;)[,1]) ## [1] 195.7924 # zinb sum(predict(defects_zinb, type = &quot;prob&quot;)[,1]) ## [1] 198.2048 7.7 Plot predictions # histogram plot with fitted probabilities # predicted values for typical regression normal.y.hat &lt;- predict(defects_normal, type = &quot;response&quot;) normal.y &lt;- defects_normal$y normal.yUnique &lt;- 0:max(normal.y) normal.nUnique &lt;- length(normal.yUnique) phat.normal &lt;- matrix(NA, length(normal.y.hat), normal.nUnique) dimnames(phat.normal) &lt;- list(NULL, normal.yUnique) for (i in 1:normal.nUnique) { phat.normal[, i] &lt;- dnorm(mean = normal.y.hat, sd = sd(residuals(defects_normal)),x = normal.yUnique[i]) } # mean of the normal predicted probabilities for each value of the outcome phat.normal.mn &lt;- apply(phat.normal, 2, mean) # probability of observing each value and mean predicted probabilities for #count regression models phat.pois &lt;- predprob(defects_pois) phat.pois.mn &lt;- apply(phat.pois, 2, mean) phat.nb &lt;- predprob(defects_negbinom) phat.nb.mn &lt;- apply(phat.nb, 2, mean) phat.zip &lt;- predprob(defects_zip) phat.zip.mn &lt;- apply(phat.zip, 2, mean) phat.zinb &lt;- predprob(defects_zinb) phat.zinb.mn &lt;- apply(phat.zinb, 2, mean) # histogram 1 hist(equinox$count, prob = TRUE, col = &quot;gray90&quot;, breaks=seq(min(equinox$count)-0.5, max(equinox$count)+.5, 1), xlab = &quot;Skips Category&quot;, ylim=c(0,.8)) rangex &lt;- length(phat.normal.mn)-1 # overlay predicted values lines(x = seq(0, rangex, 1), y = phat.normal.mn, type = &quot;b&quot;, lwd=2, lty=1, col=&quot;black&quot;) lines(x = seq(0, rangex, 1), y = phat.pois.mn, type = &quot;b&quot;, lwd=2, lty=2, col=&quot;gray20&quot;) lines(x = seq(0, rangex, 1), y = phat.nb.mn, type = &quot;b&quot;, lwd=2, lty=3, col=&quot;gray40&quot;) # legend legend(5, 0.7, c(&quot;Typical (Normal)&quot;,&quot;Poisson&quot;, &quot;Negative Binomial&quot;), lty=seq(1:3), col = c(&quot;black&quot;,&quot;gray20&quot;,&quot;gray40&quot;), lwd=2) # histogram 2 hist(equinox$count, prob = TRUE, col = &quot;gray90&quot;, breaks=seq(min(equinox$count)-0.5, max(equinox$count)+.5, 1), xlab = &quot;Skips Category&quot;, ylim=c(0,.8)) rangex &lt;- length(phat.normal.mn)-1 # overlay predicted values lines(x = seq(0, rangex, 1), y = phat.pois.mn, type = &quot;b&quot;, lwd=2, lty=2, col=&quot;gray20&quot;) lines(x = seq(0, rangex, 1), y = phat.zip.mn, type = &quot;b&quot;, lwd=2, lty=4, col=&quot;gray60&quot;) lines(x = seq(0, rangex, 1), y = phat.zinb.mn, type = &quot;b&quot;, lwd=2, lty=5, col=&quot;gray80&quot;) # legend legend(5, 0.7, c(&quot;Poisson&quot;, &quot;Zero-Inflated Poisson&quot;, &quot;Zero-Inflated Negative Binomial&quot;), lty=seq(1:3), col = c(&quot;gray20&quot;,&quot;gray60&quot;,&quot;gray80&quot;), lwd=2) 7.8 Regression diagnostics Residual plots for the models # Diagnostics # normal residuals density plot plot(density(residuals(defects_normal))) # predicted vs. residual plots # typical plot(predict(defects_normal, type=&quot;response&quot;), residuals(defects_normal), main=&quot;Typical Regression&quot;, ylab=&quot;Residuals&quot;, xlab=&quot;Predicted&quot;, ylim=c(-2,5)) abline(h=0,lty=1,col=&quot;gray&quot;) lines(lowess(predict(defects_normal,type=&quot;response&quot;),residuals(defects_normal)), lwd=2, lty=2) # poisson plot(predict(defects_pois,type=&quot;response&quot;),residuals(defects_pois), main=&quot;Poisson Regression&quot;, ylab=&quot;Residuals&quot;, xlab=&quot;Predicted&quot;, ylim=c(-2,5)) abline(h=0,lty=1,col=&quot;gray&quot;) lines(lowess(predict(defects_pois,type=&quot;response&quot;),residuals(defects_pois)),lwd=2, lty=2) # negative binomial plot(predict(defects_negbinom,type=&quot;response&quot;),residuals(defects_negbinom), main=&quot;Negative Binomial Regression&quot;, ylab=&quot;Residuals&quot;, xlab=&quot;Predicted&quot;, ylim=c(-2,5)) abline(h=0,lty=1,col=&quot;gray&quot;) lines(lowess(predict(defects_negbinom,type=&quot;response&quot;),residuals(defects_negbinom)), lwd=2, lty=2) # ZIP plot(predict(defects_zip,type=&quot;response&quot;),residuals(defects_zip), main=&quot;ZIP Regression&quot;, ylab=&quot;Residuals&quot;, xlab=&quot;Predicted&quot;, ylim=c(-2,5)) abline(h=0,lty=1,col=&quot;gray&quot;) lines(lowess(predict(defects_zip,type=&quot;response&quot;),residuals(defects_zip)),lwd=2, lty=2) # ZINB plot(predict(defects_zinb,type=&quot;response&quot;),residuals(defects_zinb), main=&quot;ZINB Regression&quot;, ylab=&quot;Residuals&quot;, xlab=&quot;Predicted&quot;, ylim=c(-2,5)) abline(h=0,lty=1,col=&quot;gray&quot;) lines(lowess(predict(defects_zinb,type=&quot;response&quot;),residuals(defects_zinb)),lwd=2, lty=2) 7.8.1 Cook’s D for the ZIP Cook’s D Cook’s distance or Cook’s D is a commonly used for detecting highly influential observations. 7.8.2 ZIP Model variable selection for the Equinox using AIC and BIC # calculate and combine AIC, AIC weights, and BIC results &lt;- data.frame(models = model.names) kpar &lt;- log(nrow(equinox)) results$bic.val &lt;- unlist(lapply(cand.models, AIC, k = kpar)) results$bic.rank &lt;- rank(results$bic.val) results$aic.val &lt;- unlist(lapply(cand.models, AIC)) results$aic.delta &lt;- results$aic.val-min(results$aic.val) results$aic.likelihood &lt;- exp(-0.5* results$aic.delta) results$aic.weight &lt;- results$aic.likelihood/sum(results$aic.likelihood) # sort models by AIC weight results &lt;- results[rev(order(results[, &quot;aic.weight&quot;])),] results$cum.aic.weight &lt;- cumsum(results[, &quot;aic.weight&quot;]) results ## models bic.val bic.rank aic.val aic.delta ## 6 cbo_lcom_wmc_rfc 633.3807 1 606.9155 0.000000 ## 5 LOC+cbo+lcom+wmc+rfc 638.6139 2 608.3680 1.452498 ## 2 LOC+cbo 641.6528 3 622.7491 15.833574 ## 3 LOC+cbo+lcom 646.0690 4 623.3845 16.468999 ## 4 LOC+cbo+lcom+wmc 650.0853 5 623.6201 16.704643 ## 7 lcom_wmc_rfc 683.1122 6 660.4278 53.512284 ## 1 LOC 686.4209 7 671.2979 64.382419 ## 9 wmc 697.7706 8 682.6476 75.732128 ## 8 wmc_rfc 701.9812 9 683.0775 76.162002 ## 10 rfc 709.0393 10 693.9163 87.000801 ## aic.likelihood aic.weight cum.aic.weight ## 6 1.000000e+00 6.735886e-01 0.6735886 ## 5 4.837200e-01 3.258282e-01 0.9994168 ## 2 3.645717e-04 2.455714e-04 0.9996624 ## 3 2.653397e-04 1.787298e-04 0.9998411 ## 4 2.358484e-04 1.588648e-04 1.0000000 ## 7 2.398585e-12 1.615659e-12 1.0000000 ## 1 1.046009e-14 7.045794e-15 1.0000000 ## 9 3.589033e-17 2.417531e-17 1.0000000 ## 8 2.894885e-17 1.949962e-17 1.0000000 ## 10 1.282378e-19 8.637951e-20 1.0000000 # # final model defects_final_zip &lt;- zeroinfl(count ~ cbo + lcom + wmc + rfc| numberOfLinesOfCode, data = equinox, link = &quot;logit&quot;, dist = &quot;poisson&quot;, trace = FALSE) defects_final_zip ## ## Call: ## zeroinfl(formula = count ~ cbo + lcom + wmc + rfc | numberOfLinesOfCode, ## data = equinox, dist = &quot;poisson&quot;, link = &quot;logit&quot;, trace = FALSE) ## ## Count model coefficients (poisson with log link): ## (Intercept) cbo lcom wmc rfc ## -0.8641763 0.0433784 0.0002456 0.0097291 -0.0051994 ## ## Zero-inflation model coefficients (binomial with logit link): ## (Intercept) numberOfLinesOfCode ## 1.6749 -0.1657 7.9 References and R code used A. Alexander Beaujean, Grant B. Morgan, Tutorial on Using Regression Models with Count Outcomes using R. Practical Assessment, Research &amp; Evaluation, Volume 21, Number 2, February 2016 Achim Zeileis, Christian Kleiber and Simon Jackman, Regression Models for Count Data in R ZERO-INFLATED POISSON REGRESSION | R DATA ANALYSIS EXAMPLES. UCLA: Statistical Consulting Group, from http://stats.idre.ucla.edu/r/dae/zinb/ (accessed May 20, 2017) Taghi M. Khoshgoftaar , Kehan Gao &amp; Robert M. Szabo, (2005) Comparing software fault predictions of pure and zero-inflated Poisson regression models, International Journal of Systems Science, 36:11, 705-715, http://dx.doi.org/10.1080/00207720500159995 Taghi M. Khoshgoftaar , Kehan Gao &amp; Robert M. Szabo, An Application of Zero-Inflated Poisson Regression for Software Fault Prediction, 2001 Marco D’Ambros, Michele Lanza, Romain Robbes.Evaluating defect prediction approaches: a benchmark and an extensive comparison, Empir Software Eng (2012) 17:531–577 DOI 10.1007/s10664-011-9173-9 A.F. Zuur et al., Chapter 11 Zero-Truncated and Zero-Inflated Models for Count Data in Mixed Effects Models and Extensions in Ecology with R, Springer, 2009 "],
["evaluation-of-models.html", "Chapter 8 Evaluation of Models 8.1 Building and Validating a Model 8.2 Evaluation of Classification Models 8.3 Other Metrics used in Software Engineering with Classification 8.4 Graphical Evaluation 8.5 Numeric Prediction Evaluation 8.6 Underfitting vs. Overfitting", " Chapter 8 Evaluation of Models Once we obtain the model with the training data, we need to evaluate it with some new data (testing data). No Free Lunch theorem In the absence of any knowledge about the prediction problem, no model can be said to be uniformly better than any other 8.1 Building and Validating a Model We cannnot use the the same data for training and testing (it is like evaluating a student with the exercises previouly solved in class, the sudent’s marks will be “optimistic” and we do not know about student capability to generalise the learned concepts). Therefore, we should, at a minimun, divide the dataset into training and testing, learn the model with the training data and test it with the rest of data as explained next. 8.1.1 Holdout approach Holdout approach consists of dividing the dataset into training (typically approx. 2/3 of the data) and testing (approx 1/3 of the data). + Problems: Data can be skewed, missing classes, etc. if randomly divided. Stratification ensures that each class is represented with approximately equal proportions (e.g., if data contains aprox 45% of positive cases, the training and testing datasets should mantain similar proportion of positive cases). Holdout estimate can be made more reliable by repeating the process with different subsamples (repeated holdout method) The error rates on the different iterations are averaged (overall error rate) Usually, part of the data points are used for building the model and the remaining points are used for validating the model. There are several approaches to this process. Validation Set approach: it is the simplest method. It consists of randomly dividing the available set of oservations into two parts, a training set and a validation set or hold-out set. Usually 2/3 of the data points are used for training and 1/3 is used for testing purposes. Hold out validation 8.1.2 Cross Validation (CV) k-fold Cross-Validation involves randomly dividing the set of observations into \\(k\\) groups, or folds, of approximately equal size. One fold is treated as a validation set and the method is trained on the remaining \\(k-1\\) folds. This procedure is repeated \\(k\\) times. If \\(k\\) is equal to \\(n\\) we are in the previous method. 1st step: split dataset (\\(\\cal D\\)) into \\(k\\) subsets of approximatelly equal size \\(C_1, \\dots, C_k\\) 2nd step: we construct a dataset \\(D_i = D-C_i\\) used for training and test the accuracy of the classifier \\(D_i\\) on \\(C_i\\) subset for testing Having done this for all \\(k\\) we estimate the accuracy of the method by averaging the accuracy over the \\(k\\) cross-validation trials k-fold Leave-One-Out Cross-Validation: This is a special case of CV. Instead of creating two subsets for training and testing, a single observation is used for the validation set, and the remaining observations make up the training set. This approach is repeated n times (the total number of observations) and the estimate for the test mean squared error is the average of the n test estimates. Leave One Out 8.2 Evaluation of Classification Models The confusion matrix (which can be extended to multiclass problems) is a matrix that is often used to describe the performance of a classification model. The following table shows the possible outcomes for binary classification problems: \\(Pred Pos\\) \\(Pred Neg\\) \\(Act Pos\\) \\(TP\\) \\(FN\\) \\(Act Neg\\) \\(FP\\) \\(TN\\) where True Positives (\\(TP\\)) and True Negatives (\\(TN\\)) are respectively the number of positive and negative instances correctly classified, False Positives (\\(FP\\)) is the number of negative instances misclassified as positive (also called Type I errors), and False Negatives (\\(FN\\)) is the number of positive instances misclassified as negative (Type II errors). From the confusion matrix, we can calculate: True positive rate, recall or sensitivity (\\(TP_r = recall = senstivity = TP/TP+FN\\)) is the proportion of positive cases correctly classified as belonging to the positive class. False negative rate (\\(FN_r=FN/TP+FN\\)) is the proportion of positive cases misclassified as belonging to the negative class. False positive rate (\\(FP_r=FP/FP+TN\\)) is the proportion of negative cases misclassified as belonging to the positive class. True negative rate, or specificity (\\(TN_r=TN/FP+TN\\)) is the proportion of negative cases correctly classified as belonging to the negative class. There is a tradeoff between \\(FP_r\\) and \\(FN_r\\) as the objective is minimize both metrics (or conversely, maximize the true negative \\(TN_r\\) and positive rates \\(TP_r\\)). Accuracy is the overall accuracy of the model, \\(accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\\). Error rate or misclassification rate, \\(error = 1-accuracy = \\frac{FP+FN}{TP + TN + FP + FN}\\), as the complementary value to accuracy. Precision, fraction of true positive instances among all predicted as positive (How many selected items are relevant?), \\(precision = \\frac{TP}{TP+FP}\\) Recall, \\(sensitivity\\), or probability of detection (\\(PD\\)) is the fraction of true positive instances among the actual total of positive instances (How many relevant ites are selected?), \\(\\frac{TP}{TP+FN}\\) f-measure (f-score or F1) is the harmonic mean of precision and recall, \\(f1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}\\) Geometric mean, \\(Gmean = \\sqrt{Recall \\times Precision}\\) Geometric mean 2, \\(Gmean_2 = \\sqrt{Recall \\times Specificity}\\) J coefficient, \\(j-coeff = sensitivity + specificity - 1\\) A suitable and interesting performance metric for binary classification when data are imbalanced is the Matthew’s Correlation Coefficient (\\(MCC\\)) (Matthews 1975): \\[MCC=\\frac{TP\\times TN - FP\\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\\] \\(MCC\\) is also be calculated from the confusion matrix with the advantage that consider all four values (while other measures consider only two or tree). Its range goes from -1 to +1; the closer to one the better as it indicates perfect prediction whereas a value of 0 means that classification is not better than random prediction and negative values mean that predictions are worst than random. 8.2.1 Prediction in probabilistic classifiers A probabilistic classifier estimates the probability of each of the posible class values given the attribute values of the instance \\(P(c|{x})\\). Then, given a new instance, \\({x}\\), the class value with the highest a posteriori probability will be assigned to that new instance (the winner takes all approach): \\(\\psi({x}) = argmax_c (P(c|{x}))\\) 8.3 Other Metrics used in Software Engineering with Classification In the domain of defect prediction and when two classes are considered, it is also customary to refer to the probability of detection, (\\(pd\\)) which corresponds to the True Positive rate (\\(TP_{rate}\\), \\(recall\\) or \\(Sensitivity\\)). Also the probability of false alarm (\\(pf\\)) corresponds to the False Positive rate, \\(FP_r\\), (see (Menzies, Greenwald, and Frank 2007)). The objective is to find which techniques that maximise \\(pd\\) and minimise \\(pf\\). As stated by Menzies et al., the balance between these two measures depends on the project characteristics (e.g. real-time systems vs. information management systems) it is formulated as the Euclidean distance from the sweet spot \\(pf=0\\) and \\(pd=1\\) to a pair of \\((pf,pd)\\). \\[balance=1-\\frac{\\sqrt{(0-pf^2)+(1-pd^2)}}{\\sqrt{2}}\\] It is normalized by the maximum possible distance across the ROC square (\\(\\sqrt{2}, 2\\)), subtracted this value from 1, and expressed it as a percentage. However, reporting \\(pd\\) and \\(pf\\) only can be missleading as the discussion on Menzies et al paper (Menzies, Greenwald, and Frank 2007) by Zhang and Zhang (2007) comment that their precision is extremely low (just between 2 and 30 per cent) and authors should report also report. This is due to the fact that datasets are quite unbalanced. An analysis of this problem is well described by Gray et al (2011a). On the other hand, the Menzies et al (2007) replied to this problem arguing that in defect prediction such high-recall and low-prediction is still useful . Some other references: (Jiang, Cukic, and Ma 2008) - Techniques for evaluating fault prediction models 8.4 Graphical Evaluation 8.4.1 Receiver Operating Characteristic (ROC) The Receiver Operating Characteristic (\\(ROC\\)) curve which provides a graphical visualisation of the results (Fawcett 2006). Receiver Operating Characteristic The Area Under the ROC Curve (AUC) also provides a quality measure between positive and negative rates with a single value. A simple way to approximate the AUC is with the following equation: \\(AUC=\\frac{1+TP_{r}-FP_{r}}{2}\\) 8.4.2 Precision-Recall Curve (PRC) Similarly to ROC, another widely used evaluation technique is the Precision-Recall Curve (PRC), which depicts a trade off between precision and recall and can also be summarised into a single value as the Area Under the Precision-Recall Curve (AUPRC) (Davis and Goadrich 2006). AUPCR can be more accurate than the ROC measure for testing performances when dealing with imbalanced datasets. 8.4.3 Cost Curves This was originaly proposed by Drummond and Holte to visualize classifier performance and the cost of misclassification (Drummond and Holte 2006). Cost curves plot the probability cost function on the \\(x\\)-axis and the normalized expected misclassification cost on the \\(y\\)-axis. Jiang et al. used cost curves as evaluation measure in defect prediciton (Jiang, Cukic, and Menzies 2008). 8.5 Numeric Prediction Evaluation RSME Mean Square Error = \\(MSE\\) = \\(\\frac{(p_1-a_1)^2 + \\ldots +(p_n-a_n)^2}{n}\\) \\({MSE}=\\frac{1}{n}\\sum_{i=1}^n(\\hat{y_i} - y_i)^2\\) \\({RMSD}=\\sqrt{\\frac{\\sum_{t=1}^n (\\hat y_t - y)^2}{n}}\\) Mean-absolute error \\(MAE\\) \\(\\frac{|p_1-a_1| + \\ldots +|p_n-a_n|}{n}\\) Relative absolute error: \\(RAE = \\frac{ \\sum^N_{i=1} | \\hat{\\theta}_i - \\theta_i | } { \\sum^N_{i=1} | \\overline{\\theta} - \\theta_i | }\\) Root relative-squared error: \\(RAE = \\sqrt{ \\frac{ \\sum^N_{i=1} | \\hat{\\theta}_i - \\theta_i | } { \\sum^N_{i=1} | \\overline{\\theta} - \\theta_i | } }\\) where \\(\\hat{\\theta}\\) is a mean value of \\(\\theta\\). Relative-squared error \\(\\frac{(p_1-a_1)^2 + \\ldots +(p_n-a_n)^2}{(a_1-\\hat{a})^2 + \\ldots + (a_n-\\hat{a})^2}\\) (\\(\\hat{a}\\) is the mean value over the training data) Relative Absolut Error Correlation Coefficient Correlation coefficient between two random variables \\(X\\) and \\(Y\\) is defined as \\(\\rho(X,Y) = \\frac{{\\bf Cov}(X,Y)}{\\sqrt{{\\bf Var}(X){\\bf Var}(Y)}}\\). The sample correlation coefficient} \\(r\\) between two samples \\(x_i\\) and \\(y_j\\) is vvdefined as \\(r = S_{xy}/\\sqrt{S_{xx}S_{yy}}\\) Example: Is there any linear relationship between the effort estimates (\\(p_i\\)) and actual effort (\\(a_i\\))? \\(a\\|39,43,21,64,57,47,28,75,34,52\\) \\(p\\|65,78,52,82,92,89,73,98,56,75\\) p&lt;-c(39,43,21,64,57,47,28,75,34,52) a&lt;-c(65,78,52,82,92,89,73,98,56,75) # cor(p,a) ## [1] 0.8397859 \\(R^2\\) 8.6 Underfitting vs. Overfitting For example, increasing the tree size, decreases the training and testing errors. However, at some point after (tree complexity), training error keeps decreasing but testing error increases. Many algorithms have parameters to determine the model complexity (e.g., in decision trees is the prunning parameter) Overfitting in trees References "],
["data-mining-issues-in-defect-prediction.html", "Chapter 9 Data Mining Issues in Defect Prediction", " Chapter 9 Data Mining Issues in Defect Prediction There are many challenges related to Software Mining and fault prediction such as: fault prediction without fault data limited fault data noisy data cross-company vs within-company. Catal highlights some of these issues (Catal 2012). References "],
["outliers-missing-values-inconsistencies-data-noise.html", "Chapter 10 Outliers, missing values, inconsistencies data noise", " Chapter 10 Outliers, missing values, inconsistencies data noise Although this statistical problem is well known in the literature, it is not always properly reported. For example preprocessing of the NASA datasets hosted at PROMISE as some concerns about their quality have been raised (D. Gray et al. 2011). And these are by far the most used datasets. Some example of works/techniques to address noise and data quality in software engineering datasets are been proposed such as the works by (Khoshgoftaar and Van Hulse 2009). References "],
["redundant-and-irrelevant-attributes-and-instances.html", "Chapter 11 Redundant and irrelevant attributes and instances", " Chapter 11 Redundant and irrelevant attributes and instances It is also well known that the existence of irrelevant and redundant features in the datasets has a negative impact on most data mining algorithms, which assume a certain level of balance between the class attributes. Feature Selection has been applied and studied by the software engineering community (T. M. Khoshgoftaar, Gao, and Napolitano 2012a), Futhermore, feature selection algorithms do not perform well with unbalanced datasets (defect prediction datasets tend to be are highly unbalanced), resulting in a selection of metrics that are not adequate for the learning algorithms See (Wang et al. 2012)(Rodriguez et al. 2007). However instance selection that needs further research. For example, the JM1 dataset rom NASA’s defect prediction datasets in PROMISE has 8,000 repeated instances. s References "],
["overlapping-or-class-separability.html", "Chapter 12 Overlapping or class separability", " Chapter 12 Overlapping or class separability When dealing with classification, we may also face the problem of overlapping between classes in which a region of the data space contains samples from different values for the class. Many samples from the NASA dataset contained in the PROMISE repository are contradictory or inconsistent, many instances have the same values for all attributes with the exception of the class, making the induction of good predictive models difficult. "],
["data-shifting.html", "Chapter 13 Data shifting", " Chapter 13 Data shifting The data shift problem happens when the test data distribution differs from the training distribution. Turhan discusses the dataset shift problem in software engineering (effort estimation and defect prediction) (Turhan 2012). This can also happen when divide the data into \\(k\\)-folds for cross validation (Raudys and Jain 1991) References "],
["imbalance-datasets.html", "Chapter 14 Imbalance datasets", " Chapter 14 Imbalance datasets This happens when samples of some classes vastly outnumber the cases of other classes. Under this situation, when the imbalanced data is not considered, many learning algorithms generate distorted models for which the impact of some factors can be hidden and the prediction accuracy can be misleading (He and Garcia 2009). This is due to the fact that most data mining algorithms assume balanced datasets. The imbalance problem is known to affect many machine learning algorithms such as decision tress, neural networks or support vectors machines. References "],
["evaluation-metrics-and-the-evaluation-of-models.html", "Chapter 15 Evaluation metrics and the evaluation of models", " Chapter 15 Evaluation metrics and the evaluation of models Arisholm et al. (Arisholm, Briand, and Johannessen 2010) compare different data mining techniques (classification tree algorithm (C4.5), a coverage rule algorithm (PART), logistic regression, back–propagation neural work and support vector machines) over 13 releases of a Telecom middleware software developed in Java using three types metrics: (i) object oriented metrics, (ii) delta measures, amount of change between successive releases, and (iii) process measures from a configuration management system. The authors concluded that although there are no significant differences regarding the techniques used, large differences can be observed depending on the criteria used to compare them. The authors also propose a cost–effectiveness measure based on the AUC and number of statements so that larger modules are more expensive to test. The same approach of considering module size in conjunction with the AUC as evaluation measure has been explored by Mende and Koschke (Mende and Koschke 2010). These can be considered as cost-sensitive classification approaches. There is also another controversy regarding the AUC measure, which is widely used to compare classifiers particularly when data is imbalanced. Hand anlysed that the AUC is not a coherent measure and proposed the \\(H-measure\\) (Hand 2009). Another controversy concerns the use of the \\(t\\)-test for comparison of multiple algorithms and datasets. As this is a parametric test it does not seem to be the most appropriate (Demšar 2006). Also currently there is a tendency to rely less on p-values and more on confidence intervales and Equivalence Hipothesis Testing (Dolado et al. 2016) References "],
["cross-project-defect-prediction.html", "Chapter 16 Cross project defect prediction", " Chapter 16 Cross project defect prediction Most studies find classifiers in single defect prediction datasets. Some exceptions include: (Hosseini, Turhan, and Mantyla 2017) A benchmark study on the effectiveness of search-based data selection and feature selection for cross project defect prediction (Zimmermann et al. 2009) Cross-project Defect Prediction (Canfora et al. 2013) Multi-objective Cross-Project Defect Prediction (Panichella, Oliveto, and Lucia 2014) Cross-project defect prediction models: L’Union fait la force This paper concluded that different classifiers are not equivalent and they can complement each other. This is also confirmed by Bowes et al. (Bowes, Hall, and Petrić 2015) References "],
["feature-subsect-selection-in-defect-prediction.html", "Chapter 17 Feature Subsect Selection in Defect Prediction", " Chapter 17 Feature Subsect Selection in Defect Prediction Feature Subsect Selection (FSS) is important in different ways: A reduced volume of data allows different data mining or searching techniques to be applied. Irrelevant and redundant attributes can generate less accurate and more complex models. Furthermore, data mining algorithms can be executed faster. We can avoid the collection of data for those irrelevant and redundant attributes in the future. FSS algorithms search through candidate feature subsets guide by a certain evaluation measure which captures the goodness of each subset. An optimal (or near optimal) subset is selected when the search stops. There are two possibilities when applying FSS: The filter model relies on general characteristics of the data to evaluate and select feature subsets without involving any data mining algorithm. The wrapper model requires one predetermined mining algorithm and uses its performance as the evaluation criterion. It searches for features better suited to the mining algorithm aiming to improve mining performance, but it also tends to be more computationally expensive than filter model. We applied Feature Subset Selection (FSS) to several datasets publicly available (PROMISE repository), and different classifiers to improve the detection of faulty modules. (Rodriguez et al. 2007) Detecting Fault Modules Applying Feature Selection to Classifiers Khoshgoftaar et al have studied FS extensively together with imbalance, noise etc. For example in: (T. M. Khoshgoftaar, Gao, and Napolitano 2012b) Exploring an iterative feature selection technique for highly imbalanced data sets References "],
["imbalanced-data.html", "Chapter 18 Imbalanced data", " Chapter 18 Imbalanced data Most publicly available datasets in software defect prediction are highly imbalanced, i.e., samples of non-defective modules vastly outnumber the defective ones. Data mining algorithms generate poor models because they tend to optimize the overall accuracy or AUC but perform badly in classes with very few samples (minority class which is usually the one we are interested in). It is typically addressed by preprocessing the datasets with sampling techniques [Afzal2012IJSEKE] or considering cost in the data mining algorithms (making the algorithms more robust). This problem happens in many of the defect prediction datasets (e.g. the PROMISE repository has around 60 defect prediction datasets). The previous problems, redundant and irrelevant attributes, overlapping, data shifting and small datasets are made worse when datasets are imbalanced (Fernández, García, and Herrera 2011) and ad-hoc approaches may be needed (T. M. Khoshgoftaar, Gao, and Napolitano 2012b) in feature selection. Different approaches include: Sampling: Sampling techniques are classified as oversampling or undersampling and are based on adding or removing instances of the training dataset Random OverSampling (ROS) replicates instances from the minority class towards a more balanced distribution Random Under-Sampling (RUS) removes instances from the majority class More intelligent approaches that random sampling include SMOTE (Synthetic Minority Over-sampling Technique) that generates new instances based on a number of nearest neighbours (\\(k\\)-NN). There are other variations of SMOTE (Borderline SMOTE) Cost-Sensitive Classifiers (CSC) penalises differently the type of errors Ensembles: Bagging (Bootstrap aggregating), boosting and stacking (Stacked generalization) which combines different types of models Robust algorithms: algorithms or modification to algorithmsdesigned to work with unbalanced data (Ibarguren et al. 2017) (Seiffert et al. 2007) An empirical study of the classification performance of learners on imbalanced and noisy software quality data References "],
["subgroup-discovery.html", "Chapter 19 Subgroup Discovery", " Chapter 19 Subgroup Discovery Subgroup Discovery (SD) aims to find subgroups of data that are statistically different given a property of interest. (Klösgen 1996) (Wrobel 1997) (Wrobel 2001) A comprehensive survey by (Herrera et al. 2011). SD lies between predictive (finding rules given historical data and a property of interest) and descriptive tasks (discovering interesting patterns in data). An important difference with classification tasks is that the SD algorithms only focus on finding subgroups (e.g., inducing rules) for the property of interest and do not necessarily describe all instances in the dataset. In general, subgroups are represented through rules with the form {\\(Cond \\rightarrow Class\\)} having as consequent (\\(Class\\)) a specific value of an attribute of interest. The antecedent (\\(Cond\\)) is usually composed of a conjunction of attribute–value pairs through relational operators. Discrete attributes can have the form of \\(att=val\\) or \\(att \\neq val\\) and for continuous attributes ranges need to be defined, i.e., \\(val_1 \\leq att \\leq val_2\\). Classification Subgroup Discovery Induction Type Predictive Descriptive Output Set of classification rules Individual rules to describe subgroups Purpose To learn a classification model To find interesting and interretable patterns SD Rules It is worth noting that we do not cover all examples SD Gen Vs Spc This has been applied to deal with unbalanced data (Rodriguez et al. 2012) Classical algorithms include the SD, CN2-SD algorithms etc. Using the CN2-SD algorithm with the KC2 dataset: And with the SD algorithm: References "],
["semi-supervised-learning.html", "Chapter 20 Semi-supervised learning", " Chapter 20 Semi-supervised learning Semi-Supervised Learning (SSL) lies between supervised and unsupervised techniques, where a small number of instances in a dataset are labeled but a lot of unlabeled data is also available. Supervised all data labelled Semi-supervised both labelled and unlabelled data Unsupervised no class attribute (all unlabelled) Given a dataset, \\(\\mathcal{D}\\): \\(\\mathcal{D} = \\mathcal{L} \\bigcup \\mathcal{U}\\) Learner \\(f:\\mathcal{X} \\mapsto \\mathcal{Y}\\) Labeled data \\(\\mathcal{L}\\) = \\((X_i, Y_i) = \\{(x_{1:l}, y_{1:l}) \\}\\) Unlabeled data \\(\\mathcal{U}\\) = \\(X_u = \\{x_{l+1:n}\\}\\) \\(avalilable during training, usually \\(l &lt;&lt; n\\)) Test data \\(X_{test} = \\{x_{n+1:\\ldots} \\}\\) In SSL, threre are two distinct goals: Inductive. Predict the labels on future test data, i.e., learning models are applied to future test data (not avalilable during training). \\(\\{(x_{1:l}, y_{1:l}), x_{l+1:n}, x_{n+1:\\ldots} \\}\\) Transductive. It is concerned with predicting the labels on the unlabeled instances provided with the training sample. \\(\\{(x_{1:l}, y_{1:l}), x_{l+1:n} \\}\\) Self-training Algorithm Input: Labeled data \\(\\mathcal{L}\\), unlabeled data \\(\\mathcal{U}\\), and a supervised learning algorithm \\(\\mathcal{A}\\). Learn a classifier \\(f\\) using labeled data \\(\\mathcal{L}\\) with \\(f\\). Label unlabeled data \\(\\mathcal{U}\\) with \\(f\\). Add new labeled data to \\(\\mathcal{L}\\) and removed them from \\(\\mathcal{U}\\) For each class \\(C\\), select examples which \\(c\\) labels as \\(C\\) with high confidence, and add it to the labeled data. Repeat 1–3 until it converges or no more unlabeled example left.\\ (Deocadez, Harrison, and Rodriguez 2017) Preliminary Study on Applying Semi-Supervised Learning to App Store Analysis In Appstore, classifying reviews into: bugs, request and other. In defect prediction: (Lu, Cukic, and Culp 2012) Software defect prediction using semi-supervised learning with dimension reduction (Li et al. 2012) Sample-based software defect prediction with active and semi-supervised learning References "],
["learning-from-crowds.html", "Chapter 21 Learning from Crowds", " Chapter 21 Learning from Crowds Learning from Crowds ODC Classification "],
["multi-objective-rules-for-defect-prediction.html", "Chapter 22 Multi-objective Rules for defect prediction", " Chapter 22 Multi-objective Rules for defect prediction We are applying MOQAR (multi-objective evolutionary schemes to discover quantitative association rules) to defect prediciton rules (Martínez-Ballesteros et al. 2016) Improving a Multi-objective Evolutionary Algorithm to Discover Quantitative Association Rules (Canfora et al. 2015) Defect prediction as a multiobjective optimization problem References "],
["settings-thresholds-for-defect-prediction.html", "Chapter 23 Settings Thresholds for Defect Prediction 23.1 Use of Mean and Standard Deviation 23.2 Use of Weighted Benchmark Data 23.3 Use of Quantiles 23.4 Some further literature", " Chapter 23 Settings Thresholds for Defect Prediction 23.1 Use of Mean and Standard Deviation Some of the methods proposed in the literature suggest that the threshold for a given measure \\(X\\) should be based on its distribution. More specifically, both Erni and Lewerentz [10] and Lanza and Marinescu [19] describe methods to define thresholds based on the mean value and the standard deviation of measures. As an applicability precondition, both proposals assume that X follows a normal distribution. According to [10], the interval \\([\\mu − \\sigma, \\mu + \\sigma]\\) is regarded as the central range of normal values for \\(X\\), where \\(\\mu\\) and \\(\\sigma\\) are the average and the standard deviation of the distribution of \\(X\\), respectively. The proposal of [19] takes \\(\\mu - \\sigma\\) as the “low” threshold, \\(\\mu + \\sigma\\) as the “high” threshold, and \\(1.5(\\mu + \\sigma)\\) as the “very high”&quot; threshold. In addition, the proposal suggests the use of “meaningful”&quot; thresholds, which can be derived based on commonly-used fraction thresholds (e.g., 0.75) and thresholds with generally-accepted meaning, \\(\\mu - \\sigma\\) and \\(\\mu + \\sigma\\). 23.2 Use of Weighted Benchmark Data Alves et al. use data from multiple different software systems to derive thresholds that are expected to “(i) bring out the metric’s variability between systems and (ii) help focus on a reasonable percentage of the source code volume” (Alves, C.Ypma, and Visser 2010). 23.3 Use of Quantiles Quantiles can also be used to set thresholds 23.4 Some further literature (Benlarbi et al. 2000) Thresholds for object-oriented measures (Morasca and Lavazza 2016) Slope-based Fault-proneness Thresholds for Software Engineering Measures References "],
["references.html", "References", " References "]
]
