% This file was created with JabRef 2.10.
% Encoding: ISO8859_1


@Article{CanoHL07,
  Title                    = {Evolutionary stratified training set selection for extracting classification rules with trade off precision-interpretability },
  Author                   = {Jos\'e Ram\'on Cano and Francisco Herrera and Manuel Lozano},
  Journal                  = {Data \& Knowledge Engineering},
  Year                     = {2007},
  Note                     = {Intelligent Data Mining },
  Number                   = {1},
  Pages                    = {90--108},
  Volume                   = {60},

  Abstract                 = {The generation of predictive models is a frequent task in data mining with the objective of generating highly precise and interpretable models. The data reduction is an interesting preprocessing approach that can allow us to obtain predictive models with these characteristics in large size data sets. In this paper, we analyze the rule classification model based on decision trees using a training selected set via evolutionary stratified instance selection. This method faces the scaling problem that appears in the evaluation of large size data sets, and the trade off interpretability-precision of the generated models.},
  Doi                      = {http://dx.doi.org/10.1016/j.datak.2006.01.008},
  ISSN                     = {0169-023X},
  Keywords                 = {Training set selection},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0169023X0600019X}
}

@Article{GDCH12,
  Title                    = {Prototype Selection for Nearest Neighbor Classification: Taxonomy and Empirical Study},
  Author                   = {S. Garcia and J. Derrac and J. Cano and F. Herrera},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2012},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {417--435},
  Volume                   = {34},

  Abstract                 = {The nearest neighbor classifier is one of the most used and well-known techniques for performing recognition tasks. It has also demonstrated itself to be one of the most useful algorithms in data mining in spite of its simplicity. However, the nearest neighbor classifier suffers from several drawbacks such as high storage requirements, low efficiency in classification response, and low noise tolerance. These weaknesses have been the subject of study for many researchers and many solutions have been proposed. Among them, one of the most promising solutions consists of reducing the data used for establishing a classification rule (training data) by means of selecting relevant prototypes. Many prototype selection methods exist in the literature and the research in this area is still advancing. Different properties could be observed in the definition of them, but no formal categorization has been established yet. This paper provides a survey of the prototype selection methods proposed in the literature from a theoretical and empirical point of view. Considering a theoretical point of view, we propose a taxonomy based on the main characteristics presented in prototype selection and we analyze their advantages and drawbacks. Empirically, we conduct an experimental study involving different sizes of data sets for measuring their performance in terms of accuracy, reduction capabilities, and runtime. The results obtained by all the methods studied have been verified by nonparametric statistical tests. Several remarks, guidelines, and recommendations are made for the use of prototype selection for nearest neighbor classification.},
  Doi                      = {10.1109/TPAMI.2011.142},
  ISSN                     = {0162-8828},
  Keywords                 = {data mining;pattern classification;data mining;machine learning;nearest neighbor classification;prototype selection methods;Accuracy;Classification algorithms;Noise;Noise measurement;Prototypes;Taxonomy;Training;Prototype selection;classification.;condensation;edition;nearest neighbor;taxonomy}
}

