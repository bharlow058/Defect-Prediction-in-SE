@article{FernandezCBA14,
  author  = {Manuel Fern\'{a}ndez-Delgado and Eva Cernadas and Sen\'{e}n Barro and Dinani Amorim},
  title   = {Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  pages   = {3133--3181},
  url     = {http://jmlr.org/papers/v15/delgado14a.html}
}


@ARTICLE{Afzal2012IJSEKE,
  author = {Wasif Afzal and Richard Torkar and Robert Feldt},
  title = {Resampling methods in software quality classification},
  journal = {International Journal of Software Engineering and Knowledge Engineering},
  year = {2012},
  volume = {22},
  pages = {203--223},
  issue = {2},
  keywords = {Software quality; Prediction; Resampling; Empirical study},
  publisher = {World Scientific Publishing Company}
}

@ARTICLE{Alcala11,
  author = {J. {Alcal\'a-Fdez} and A. Fernandez and J. Luengo and J. Derrac,
	S. Garc\'aa and L. S\'anchez and F. Herrera},
  title = {{KEEL} Data-Mining Software Tool: Data Set Repository, Integration
	of Algorithms and Experimental Analysis Framework},
  journal = {Journal of Multiple-Valued Logic and Soft Computing},
  year = {2011},
  volume = {17},
  pages = {255-287},
  number = {2-3}
}

@ARTICLE{Alcala09,
  author = {J. {Alcal\'a-Fdez} and L. S\'anchez and S. Garc\'ia and M.J. del
	Jesus and S. Ventura and J.M. Garrell and J. Otero and C. Romero
	and J. Bacardit and V.M. Rivas and J.C. Fern\'andez and F. Herrera},
  title = {{KEEL}: A Software Tool to Assess Evolutionary Algorithms to Data
	Mining Problems},
  journal = {Soft Computing},
  year = {2009},
  volume = {13},
  pages = {307--318},
  number = {3}
}



@ARTICLE{Arisholm10,
  author = {Erik Arisholm and Lionel C. Briand and Eivind B. Johannessen},
  title = {A systematic and comprehensive investigation of methods to build
	and evaluate fault prediction models},
  journal = {Journal of Systems and Software},
  year = {2010},
  volume = {83},
  pages = {2--17},
  number = {1},
  address = {New York, NY, USA},
  doi = {http://dx.doi.org/10.1016/j.jss.2009.06.055},
  issn = {0164-1212},
  publisher = {Elsevier Science Inc.}
}


@INPROCEEDINGS{buse-icse-2012,
  author = {Raymond P.L. Buse and Thomas Zimmermann},
  title = {Information Needs for Software Development Analytics},
  booktitle = {Proceedings of the 34th International Conference on Software Engineering},
  year = {2012},
  month = {June},
  location = {Zurich, Switzerland}
}

@INPROCEEDINGS{Caglayan09,
  author = {Caglayan, Bora and Bener, Ayse and Koch, Stefan},
  title = {Merits of using repository metrics in defect prediction for open
	source projects},
  booktitle = {Proceedings of the 2009 ICSE Workshop on Emerging Trends in Free/Libre/Open
	Source Software Research and Development},
  year = {2009},
  series = {FLOSS'09},
  pages = {31--36},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid = {1572199},
  doi = {10.1109/FLOSS.2009.5071357},
  isbn = {978-1-4244-3720-7},
  numpages = {6},
  url = {http://dx.doi.org/10.1109/FLOSS.2009.5071357}
}

@ARTICLE{Catal_WIRED12,
  author = {Catal, Cagatay},
  title = {Software mining and fault prediction},
  journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  year = {2012},
  volume = {2},
  pages = {420--426},
  number = {5},
  abstract = {Mining software repositories (MSRs) such as source control repositories,
	bug repositories, deployment logs, and code repositories provide
	useful patterns for practitioners. Instead of using these repositories
	as record-keeping ones, we need to transform them into active repositories
	that can guide the decision processes inside the company. By MSRs
	with several data mining algorithms, effective software fault prediction
	models can be built and error-prone modules can be detected prior
	to the testing phase. We discuss numerous real-world challenges in
	building accurate fault prediction models and present some solutions
	to these challenges. à¤Š 2012 Wiley Periodicals, Inc.},
  doi = {10.1002/widm.1067},
  issn = {1942-4795},
  publisher = {John Wiley \& Sons, Inc.},
  url = {http://dx.doi.org/10.1002/widm.1067}
}

@ARTICLE{CC2009,
  author = {Cagatay Catal and Banu Diri},
  title = {A systematic review of software fault prediction studies},
  journal = {Expert Systems with Applications},
  year = {2009},
  volume = {36},
  pages = {7346--7354},
  number = {4},
  abstract = {This paper provides a systematic review of previous software fault
	prediction studies with a specific focus on metrics, methods, and
	datasets. The review uses 74 software fault prediction papers in
	11 journals and several conference proceedings. According to the
	review results, the usage percentage of public datasets increased
	significantly and the usage percentage of machine learning algorithms
	increased slightly since 2005. In addition, method-level metrics
	are still the most dominant metrics in fault prediction research
	area and machine learning algorithms are still the most popular methods
	for fault prediction. Researchers working on software fault prediction
	area should continue to use public datasets and machine learning
	algorithms to build better fault predictors. The usage percentage
	of class-level is beyond acceptable levels and they should be used
	much more than they are now in order to predict the faults earlier
	in design phase of software life cycle.},
  doi = {10.1016/j.eswa.2008.10.027},
  issn = {0957-4174},
  keywords = {Machine learning},
  owner = {drg},
  timestamp = {2012.09.15},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417408007215}
}


@ARTICLE{Demsar2006,
  author = {Dem\v{s}ar, Janez},
  title = {Statistical Comparisons of Classifiers over Multiple Data Sets},
  journal = {Journal of Machine Learning Research},
  year = {2006},
  volume = {7},
  pages = {1--30},
  month = dec,
  acmid = {1248548},
  issn = {1532-4435},
  issue_date = {12/1/2006},
  numpages = {30},
  publisher = {JMLR.org},
  url = {http://dl.acm.org/citation.cfm?id=1248547.1248548}
}


@INPROCEEDINGS{FGH11,
  author = {Alberto Fern\'andez and Salvador Garc\'ia and Francisco Herrera},
  title = {Addressing the Classification with Imbalanced Data: Open Problems
	and New Challenges on Class Distribution},
  booktitle = {6th International Conference on Hybrid Artificial Intelligence Systems
	(HAIS)},
  year = {2011},
  pages = {1--10},
  ee = {http://dx.doi.org/10.1007/978-3-642-21219-2_1}
}

@INPROCEEDINGS{FlachHR11,
  author = {Peter A. Flach and Jos{\'e} Hern{\'a}ndez-Orallo and C{\`e}sar Ferri
	Ramirez},
  title = {A Coherent Interpretation of AUC as a Measure of Aggregated Classification
	Performance},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning
	(ICML'11)},
  year = {2011},
  pages = {657--664},
  address = {Bellevue, Washington, USA},
  month = {June 28- July 2, 2011},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}


@INPROCEEDINGS{GrayBDSB11,
  author = {Gray, David and Bowes, David and Davey, Neil and Sun, Yi and Christianson,
	Bruce},
  title = {The misuse of the NASA metrics data program data sets for automated
	software defect prediction},
  booktitle = {Evaluation Assessment in Software Engineering (EASE 2011), 15th Annual
	Conference on},
  year = {2011},
  pages = {96 -103},
  month = {april},
  abstract = {Background: The NASA Metrics Data Program data sets have been heavily
	used in software defect prediction experiments. Aim: To demonstrate
	and explain why these data sets require significant pre-processing
	in order to be suitable for defect prediction. Method: A meticulously
	documented data cleansing process involving all 13 of the original
	NASA data sets. Results: Post our novel data cleansing process; each
	of the data sets had between 6 to 90 percent less of their original
	number of recorded values. Conclusions: One: Researchers need to
	analyse the data that forms the basis of their findings in the context
	of how it will be used. Two: Defect prediction data sets could benefit
	from lower level code metrics in addition to those more commonly
	used, as these will help to distinguish modules, reducing the likelihood
	of repeated data points. Three: The bulk of defect prediction experiments
	based on the NASA Metrics Data Program data sets may have led to
	erroneous findings. This is mainly due to repeated data points potentially
	causing substantial amounts of training and testing data to be identical.},
  doi = {10.1049/ic.2011.0012}
}

@ARTICLE{HBBGC11,
  author = {Tracy Hall and Sarah Beecham and David Bowes and David Gray and Steve
	Counsell},
  title = {A Systematic Literature Review on Fault Prediction Performance in
	Software Engineering},
  journal = {Transactions on Software Engineering},
  year = {In Press -- 2011},
  abstract = {Background: The accurate prediction of where faults are likely to	occur in code can help direct test effort, reduce costs and improve the quality of software. Objective: We investigate how the context of models, the independent variables used and the modelling techniques applied, influence the performance of fault prediction models.Method:We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesise the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modelling techniques such as Na\F6\80\81\8Eve Bayes or Logistic Regression. Combinations of independent variables have been used by models that	perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their	context, methodology and performance comprehensively.},
}

@ARTICLE{Hand09,
  author = {Hand, David J.},
  title = {Measuring classifier performance: a coherent alternative to the area
	under the ROC curve},
  journal = {Machine Learning},
  year = {2009},
  volume = {77},
  pages = {103--123},
  number = {1},
  month = oct,
  acmid = {1613009},
  address = {Hingham, MA, USA},
  doi = {10.1007/s10994-009-5119-5},
  issn = {0885-6125},
  issue_date = {October 2009},
  keywords = {AUC, Classification, Cost, Error rate, Loss, Misclassification rate,
	ROC curves, Sensitivity, Specificity},
  numpages = {21},
  publisher = {Kluwer Academic Publishers},
  url = {http://dx.doi.org/10.1007/s10994-009-5119-5}
}


@ARTICLE{He_KDE09_Imbalance,
  author = {Haibo He and E.A. Garcia},
  title = {Learning from Imbalanced Data},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  year = {2009},
  volume = {21},
  pages = {1263--1284},
  number = {9},
  month = {Sept.},
  abstract = {With the continuous expansion of data availability in many large-scale,
	complex, and networked systems, such as surveillance, security, Internet,
	and finance, it becomes critical to advance the fundamental understanding
	of knowledge discovery and analysis from raw data to support decision-making
	processes. Although existing knowledge discovery and data engineering
	techniques have shown great success in many real-world applications,
	the problem of learning from imbalanced data (the imbalanced learning
	problem) is a relatively new challenge that has attracted growing
	attention from both academia and industry. The imbalanced learning
	problem is concerned with the performance of learning algorithms
	in the presence of underrepresented data and severe class distribution
	skews. Due to the inherent complex characteristics of imbalanced
	data sets, learning from such data requires new understandings, principles,
	algorithms, and tools to transform vast amounts of raw data efficiently
	into information and knowledge representation. In this paper, we
	provide a comprehensive review of the development of research in
	learning from imbalanced data. Our focus is to provide a critical
	review of the nature of the problem, the state-of-the-art technologies,
	and the current assessment metrics used to evaluate learning performance
	under the imbalanced learning scenario. Furthermore, in order to
	stimulate future research in this field, we also highlight the major
	opportunities and challenges, as well as potential important research
	directions for learning from imbalanced data.},
  doi = {10.1109/TKDE.2008.239},
  issn = {1041-4347},
  keywords = {complex systems;data availability;data engineering;decision making;imbalanced
	data;knowledge discovery;large-scale systems;learning;networked systems;data
	mining;decision making;large-scale systems;learning (artificial intelligence);}
}


@ARTICLE{Khoshgoftaar09,
  author = {Khoshgoftaar, T.M. and Van Hulse, J.},
  title = {Empirical Case Studies in Attribute Noise Detection},
  journal = {Systems, Man, and Cybernetics, Part C: Applications and Reviews,
	IEEE Transactions on},
  year = {2009},
  volume = {39},
  pages = {379 -388},
  number = {4},
  month = {july },
  abstract = {The quality of data is an important issue in any domain-specific data
	mining and knowledge discovery initiative. The validity of solutions
	produced by data-driven algorithms can be diminished if the data
	being analyzed are of low quality. The quality of data is often realized
	in terms of data noise present in the given dataset and can include
	noisy attributes or labeling errors. Hence, tools for improving the
	quality of data are important to the data mining analyst. We present
	a comprehensive empirical investigation of our new and innovative
	technique for ranking attributes in a given dataset from most to
	least noisy. Upon identifying the noisy attributes, specific treatments
	can be applied depending on how the data are to be used. In a classification
	setting, for example, if the class label is determined to contain
	the most noise, processes to cleanse this important attribute may
	be undertaken. Independent variables or predictors that have a low
	correlation to the class attribute and appear noisy may be eliminated
	from the analysis. Several case studies using both real-world and
	synthetic datasets are presented in this study. The noise detection
	performance is evaluated by injecting noise into multiple attributes
	at different noise levels. The empirical results demonstrate conclusively
	that our technique provides a very accurate and useful ranking of
	noisy attributes in a given dataset.},
  doi = {10.1109/TSMCC.2009.2013815},
  issn = {1094-6977},
  keywords = {attribute noise detection;data quality;domain-specific data mining;knowledge
	discovery;data analysis;data mining;}
}


@ARTICLE{KhoshgoftaarGN12,
  author = {Taghi M. Khoshgoftaar and Kehan Gao and Amri Napolitano},
  title = {An Empirical Study of Feature Ranking Techniques for Software Quality
	Prediction},
  journal = {International Journal of Software Engineering and Knowledge Engineering},
  year = {2012},
  volume = {22},
  pages = {161-183},
  number = {2},
  ee = {http://dx.doi.org/10.1142/S0218194012400013}
}


@INPROCEEDINGS{Lincke2008,
  author = {Lincke, R\"{u}diger and Lundberg, Jonas and L\"{o}we, Welf},
  title = {Comparing software metrics tools},
  booktitle = {Proceedings of the 2008 International Symposium on Software Testing
	and Analysis (ISSTA'08)},
  year = {2008},
  pages = {131--142},
  publisher = {ACM},
  numpages = {12}
}


@INPROCEEDINGS{Mende2010,
  author = {Mende, Thilo},
  title = {Replication of defect prediction studies: problems, pitfalls and
	recommendations},
  booktitle = {Proceedings of the 6th International Conference on Predictive Models
	in Software Engineering},
  year = {2010},
  series = {PROMISE'10},
  pages = {5:1--5:10},
  address = {New York, NY, USA},
  publisher = {ACM},
  acmid = {1868336},
  articleno = {5},
  doi = {10.1145/1868328.1868336},
  isbn = {978-1-4503-0404-7},
  keywords = {defect prediction model, replication},
  location = {Timisoara, Romania},
  numpages = {10},
  url = {http://doi.acm.org/10.1145/1868328.1868336}
}

@INPROCEEDINGS{Mende10,
  author = {Mende, Thilo and Koschke, Rainer},
  title = {Effort-Aware Defect Prediction Models},
  booktitle = {Proceedings of the 2010 14th European Conference on Software Maintenance
	and Reengineering (CSMR'10)},
  year = {2010},
  series = {CSMR'10},
  pages = {107--116},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid = {1955974},
  doi = {10.1109/CSMR.2010.18},
  isbn = {978-0-7695-4321-5},
  keywords = {Defect Prediction Models, Evaluation, Cost-Benefits},
  numpages = {10},
  url = {http://dx.doi.org/10.1109/CSMR.2010.18}
}


@ARTICLE{Menzies07,
  author = {T. Menzies and J. Greenwald and A. Frank},
  title = {Data Mining Static Code Attributes to Learn Defect Predictors},
  journal = {IEEE Transactions on Software Engineering},
  year = {2007},
  optmonth = {January},
  optnumber = {1},
  optpages = {2--13},
  optvolume = {33},
  owner = {drg},
  timestamp = {2012.09.15}
}



@INPROCEEDINGS{NZZH10,
  author = {Nachiappan Nagappan and Andreas Zeller and Thomas Zimmermann and
	Kim Herzig and Brendan Murphy},
  title = {Change Bursts as Defect Predictors},
  booktitle = {Proceedings of the 21st IEEE International Symposium on Software
	Reliability Engineering (ISSRE 2012)},
  year = {2010},
  month = {November},
  location = {San Jose, California, USA}
}


@ARTICLE{RaudysJ91,
  author = {Raudys, S.J. and Jain, A.K.},
  title = {Small sample size effects in statistical pattern recognition: recommendations
	for practitioners},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {1991},
  volume = {13},
  pages = {252--264},
  number = {3},
  month = {mar},
  doi = {10.1109/34.75512},
  issn = {0162-8828},
  keywords = {classifiers;error estimation;error rates;feature selection;learning;sample
	size effects;statistical pattern recognition;pattern recognition;statistical
	analysis;}
}

@INPROCEEDINGS{Rob10,
  author = {Robles, G.},
  title = {Replicating {MSR}: A study of the potential replicability of papers
	published in the Mining Software Repositories proceedings},
  booktitle = {7th IEEE Working Conference on Mining Software Repositories (MSR
	2010)},
  year = {2010},
  pages = {171--180},
  month = {may},
  doi = {10.1109/MSR.2010.5463348},
  keywords = {MSR;MSR replication;data sources;mining software repositories proceedings;potential
	replicability;software projects;data mining;}
}

@ARTICLE{RGIH09,
  author = {Gregorio Robles and Jesus M. Gonzalez-Barahona and Daniel Izquierdo-Cortazar
	and Israel Herraiz},
  title = {Tools for the study of the usual data sources found in libre software
	projects},
  journal = {International Journal of Open Source Software and Processes},
  year = {2009},
  volume = {1},
  pages = {24--45},
  number = {1},
  month = {Jan-March}
}

@ARTICLE{RodriguezEtAlINS12,
  author = {D. Rodr\'iguez and R. Ruiz and J.C. Riquelme and J.S. Aguilar-Ruiz},
  title = {Searching for rules to detect defective modules: A subgroup discovery
	approach},
  journal = {Information Sciences},
  year = {2012},
  volume = {191},
  pages = {14--30},
  number = {0},
  doi = {10.1016/j.ins.2011.01.039},
  issn = {0020-0255},
  keywords = {Defect prediction}
}

@ARTICLE{RodriguezEtAl12,
  author = {D. Rodr\'iguez and M.A. Sicilia and E. Garc\'ia and R. Harrison},
  title = {Empirical findings on team size and productivity in software development},
  journal = {Journal of Systems and Software},
  year = {2012},
  volume = {85},
  pages = {562--570},
  number = {3},
  doi = {10.1016/j.jss.2011.09.009},
  issn = {0164-1212},
  keywords = {Team size}
}

@INPROCEEDINGS{RRCA07,
  author = {Rodriguez, D. and Ruiz, R. and Cuadrado-Gallego, J. and Aguilar-Ruiz,
	J.},
  title = {Detecting Fault Modules Applying Feature Selection to Classifiers},
  booktitle = {Information Reuse and Integration, 2007. IRI 2007. IEEE International
	Conference on},
  year = {2007},
  pages = {667--672},
  month = {aug.},
  doi = {10.1109/IRI.2007.4296696},
  keywords = {PROMISE repository;attribute selection techniques;automated data collection
	tools;classifier learning;data mining algorithms;fault module detection;feature
	selection;project management;software engineering databases;data
	mining;feature extraction;learning (artificial intelligence);pattern
	classification;project management;software management;}
}



@ARTICLE{Shepperd2012,
  author = {Martin Shepperd and Steve MacDonell},
  title = {Evaluating prediction systems in software project estimation},
  journal = {Information and Software Technology},
  year = {2012},
  pages = {-},
  doi = {10.1016/j.infsof.2011.12.008},
  issn = {0950-5849}
}


@ARTICLE{TurhanESE12,
  author = {Turhan, Burak},
  title = {On the dataset shift problem in software engineering prediction models},
  journal = {Empirical Software Engineering},
  year = {2012},
  volume = {17},
  pages = {62--74},
  note = {10.1007/s10664-011-9182-8},
  affiliation = {Department of Information Processing Science, University of Oulu,
	POB.3000, 90014 Oulu, Finland},
  issn = {1382-3256},
  issue = {1},
  keyword = {Computer Science},
  publisher = {Springer Netherlands}
}


@INPROCEEDINGS{WangKWN12,
  author = {Huanjing Wang and Taghi M. Khoshgoftaar and Randall Wald and Amri
	Napolitano},
  title = {A Comparative Study on the Stability of Software Metric Selection
	Techniques},
  booktitle = {11th International Conference on Machine Learning and Applications,
	ICMLA},
  year = {2012},
  pages = {301--307},
  ee = {http://dx.doi.org/10.1109/ICMLA.2012.142}
}



@INPROCEEDINGS{ZPZ07,
  author = {Zimmermann, Thomas and Premraj, Rahul and Zeller, Andreas},
  title = {Predicting Defects for Eclipse},
  booktitle = {Proceedings of the Third International Workshop on Predictor Models
	in Software Engineering (PROMISE'07)},
  year = {2007},
  series = {PROMISE '07},
  pages = {9--},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid = {1269057},
  doi = {http://dx.doi.org/10.1109/PROMISE.2007.10},
  isbn = {0-7695-2954-2}
}


@ARTICLE{Elish08,
  author = {Karim O. Elish and Mahmoud O. Elish},
  title = {Predicting defect-prone software modules using support vector machines},
  journal = {Journal of Systems and Software},
  year = {2008},
  volume = {81},
  pages = {649--660},
  number = {5},
  abstract = {Effective prediction of defect-prone software modules can enable software
	developers to focus quality assurance activities and allocate effort
	and resources more efficiently. Support vector machines (SVM) have
	been successfully applied for solving both classification and regression
	problems in many applications. This paper evaluates the capability
	of SVM in predicting defect-prone software modules and compares its
	prediction performance against eight statistical and machine learning
	models in the context of four NASA datasets. The results indicate
	that the prediction performance of SVM is generally better than,
	or at least, is competitive against the compared models.},
  doi = {10.1016/j.jss.2007.07.040},
  issn = {0164-1212},
  keywords = {Software metrics},
  url = {http://www.sciencedirect.com/science/article/pii/S016412120700235X}
}



@INPROCEEDINGS{GBDSC11,
  author = {David Gray and David Bowes and Neil Davey and Yi Sun and Bruce Christianson},
  title = {The misuse of the NASA Metrics Data Program data sets for automated
	software defect prediction
	
	
	Durham, UK, 11-12 April 2011, ISBN: 978-1-84919-509-6},
  booktitle = {15th Annual Conference on Evaluation \& Assessment in Software Engineering
	(EASE 2011)},
  year = {2011},
  pages = {96--103},
  url = {http://dx.doi.org/10.1049/ic.2011.0012}
}

@ARTICLE{HCGJ11,
  author = {Francisco Herrera and Crist\'{o}bal J. {Carmona del Jesus} and Pedro
	Gonz\'{a}lez and Mar\'{i}a Jos\'e {del Jesus}},
  title = {An overview on Subgroup Discovery: Foundations and Applications},
  journal = {Knowledge and Information Systems},
  year = {2011},
  volume = {29},
  pages = {495--525}
}


@ARTICLE{KL2006,
  author = {Kav\v{s}ek, Branko and Lavra\v{c}, Nada},
  title = {{APRIORI-SD}: Adapting Association Rule Learning to Subgroup Discovery},
  journal = {Applied Artificial Intelligence},
  year = {2006},
  volume = {20},
  pages = {543-583},
  number = {7},
  abstract = { This paper presents a subgroup discovery algorithm APRIORI-SD, developed
	by adapting association rule learning to subgroup discovery. The
	paper contributes to subgroup discovery, to a better understanding
	of the weighted covering algorithm, and the properties of the weighted
	relative accuracy heuristic by analyzing their performance in the
	ROC space. An experimental comparison with rule learners CN2, RIPPER,
	and APRIORI-C on UCI data sets demonstrates that APRIORI-SD produces
	substantially smaller rulesets, where individual rules have higher
	coverage and significance. APRIORI-SD is also compared to subgroup
	discovery algorithms CN2-SD and SubgroupMiner. The comparisons performed
	on U.K. traffic accident data show that APRIORI-SD is a competitive
	subgroup discovery algorithm. },
  doi = {10.1080/08839510600779688},
  eprint = {http://www.tandfonline.com/doi/pdf/10.1080/08839510600779688},
  url = {http://www.tandfonline.com/doi/abs/10.1080/08839510600779688}
}


@ARTICLE{Lavrac04,
  author = {Nada Lavra\v{c} and Branko Kav\v{s}ek and Peter Flach and Ljup\v{c}o
	Todorovski},
  title = {Subgroup Discovery with {CN2-SD}},
  journal = {The Journal of Machine Learning Research},
  year = {2004},
  volume = {5},
  pages = {153--188},
  address = {Cambridge, MA, USA},
  issn = {1532-4435},
  publisher = {MIT Press}
}


@ARTICLE{Klosgen96,
  author = {Willi Kl\"{o}sgen},
  title = {Explora: a multipattern and multistrategy discovery assistant},
  year = {1996},
  pages = {249--271},
  address = {Menlo Park, CA, USA},
  book = {Advances in knowledge discovery and data mining},
  isbn = {0-262-56097-6},
  publisher = {American Association for Artificial Intelligence}
}


@Article{Breiman96,
  Title                    = {Bagging predictors},
  Author                   = {Breiman, Leo},
  Journal                  = {Machine Learning},
  Year                     = {1996},
  Pages                    = {123-140},
  Volume                   = {24},

  Doi                      = {10.1007/BF00058655},
  ISSN                     = {0885-6125},
  Issue                    = {2},
  Keywords                 = {Aggregation; Bootstrap; Averaging; Combining},
  Language                 = {English},
  Publisher                = {Kluwer Academic Publishers},
  Url                      = {http://dx.doi.org/10.1007/BF00058655}
}

@Book{Breiman1984,
  Title                    = {Classification and Regression Trees},
  Author                   = {Leo Breiman and Jerome H. Friedman and Richard A. Olshen and Charles J. Stone},
  Publisher                = {Wadsworth International Group},
  Year                     = {1984},

  Address                  = {Belmont, California}
}

@Article{CBHK2002,
  Title                    = {SMOTE: Synthetic Minority Over-sampling TEchnique},
  Author                   = {N.V. Chawla and K.W. Bowyer and L.O. Hall and W.P. Kegelmeyer},
  Journal                  = {Journal of Artificial Intelligence Research },
  Year                     = {2002},
  Pages                    = {321--357},
  Volume                   = {16}
}

@InProceedings{CLHB2003,
  Title                    = {SMOTEBoost: Improving prediction of the minority class in boosting},
  Author                   = {N.V. Chawla and A. Lazarevic and L.O. Hall and K.W. Bowyer},
  Booktitle                = {7th European Conference on Principles and Practice of Knowledge Discovery in Databases({PKDD 2003})},
  Year                     = {2003},
  Pages                    = {107--119},

  City                     = {Cavtat Dubrovnik},
  Country                  = {Croatia}
}

@Article{ChawlaBHK02,
  Title                    = {SMOTE: Synthetic Minority Over-sampling Technique},
  Author                   = {Nitesh V. Chawla and Kevin W. Bowyer and Lawrence O. Hall and W. Philip Kegelmeyer},
  Journal                  = {J. Artif. Intell. Res. (JAIR)},
  Year                     = {2002},
  Pages                    = {321--357},
  Volume                   = {16},

  Bibsource                = {DBLP, http://dblp.uni-trier.de},
  Ee                       = {http://dx.doi.org/10.1613/jair.953}
}


@article{freund1999short,
  title={A short introduction to boosting},
  author={Freund, Yoav and Schapire, Robert and Abe, Naoki},
  journal={Journal-Japanese Society For Artificial Intelligence},
  volume={14},
  number={771-780},
  pages={1612},
  year={1999},
  publisher={JAPANESE SOC ARTIFICIAL INTELL}
}


@article{Martinez2016,
 author = {Mart\'{\i}nez-Ballesteros, M. and Troncoso, A. and Mart\'{\i}nez-\'{A}lvarez, F. and Riquelme, J. C.},
 title = {Improving a Multi-objective Evolutionary Algorithm to Discover Quantitative Association Rules},
 journal = {Knowl. Inf. Syst.},
 issue_date = {November 2016},
 volume = {49},
 number = {2},
 month = nov,
 year = {2016},
 issn = {0219-1377},
 pages = {481--509},
 numpages = {29},
 url = {http://dx.doi.org/10.1007/s10115-015-0911-y},
 doi = {10.1007/s10115-015-0911-y},
 acmid = {3008257},
 publisher = {Springer-Verlag New York, Inc.},
 address = {New York, NY, USA},
 keywords = {Association rules, Data mining, Evolutionary computation, Pareto-optimization},
} 

