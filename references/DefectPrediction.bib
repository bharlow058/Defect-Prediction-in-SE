@ARTICLE{HallBBGC2012,
author={T. Hall and S. Beecham and D. Bowes and D. Gray and S. Counsell},
journal={IEEE Transactions on Software Engineering},
title={A Systematic Literature Review on Fault Prediction Performance in Software Engineering},
year={2012},
volume={38},
number={6},
pages={1276--1304},
abstract={Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.},
keywords={Bayes methods;regression analysis;software fault tolerance;software quality;contextual information;cost reduction;fault prediction models;fault prediction performance;fault prediction study;feature selection;independent variables;logistic regression;methodological information;naive Bayes;predictive performance;reliable methodology;simple modeling techniques;software engineering;software quality;systematic literature review;Analytical models;Context modeling;Data models;Fault diagnosis;Predictive models;Software testing;Systematics;Systematic literature review;software fault prediction},
doi={10.1109/TSE.2011.103},
ISSN={0098-5589},
month={Nov},}


@article{Catal2011,
author = {Catal, Cagatay},
doi = {10.1016/j.eswa.2010.10.024},
issn = {09574174},
journal = {Expert Systems with Applications},
month = {apr},
number = {4},
pages = {4626--4636},
title = {Software fault prediction: A literature review and current trends},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417410011681},
volume = {38},
year = {2011}
}


@article{KhoshgoftaarEtAl:05,
   Author = {T.M. Khoshgoftaar and N. Seliya and K. Gao},
   Title = {Assessment of a New Three-Group Software Quality Classification Technique: An Empirical Case Study},
   Journal = {Empirical Software Engineering},
   Volume = {10},
   Number = {2},
   Pages = {183-218},
   Year = {2005}
}

@INPROCEEDINGS{MoserPS2008, 
author={R. Moser and W. Pedrycz and G. Succi}, 
booktitle={2008 ACM/IEEE 30th International Conference on Software Engineering}, 
title={A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction}, 
year={2008}, 
pages={181-190}, 
abstract={In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, naive Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: >75\% percentage of correctly classified files, a recall of >80\%, and a false positive rate <30\%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.}, 
keywords={Bayes methods;Java;decision trees;learning (artificial intelligence);pattern classification;program diagnostics;regression analysis;software metrics;software quality;Eclipse project;Java file classification;change metrics;comparative analysis;cost-sensitive classification;decision tree;defect prediction;logistic regression;machine learning;naive Bayes method;process related software metrics;product related software metrics;software quality;static code attribute;Classification tree analysis;Costs;Java;Logistics;Permission;Predictive models;Resource management;Software engineering;Software metrics;Testing;cost-sensitive classification;defect prediction;software metrics}, 
doi={10.1145/1368088.1368114}, 
ISSN={0270-5257}, 
month={May},}





@INPROCEEDINGS{HuangNPGBT2011, 
author={LiGuo Huang and V. Ng and I. Persing and Ruili Geng and Xu Bai and Jeff Tian}, 
booktitle={26th IEEE/ACM International Conference on Automated Software Engineering (ASE'2011)}, 
title={{AutoODC}: Automated generation of Orthogonal Defect Classifications}, 
year={2011}, 
pages={412--415}, 
abstract={Orthogonal Defect Classification (ODC), the most influential framework for software defect classification and analysis, provides valuable in-process feedback to system development and maintenance. Conducting ODC classification on existing organizational defect reports is human intensive and requires experts' knowledge of both ODC and system domains. This paper presents AutoODC, an approach and tool for automating ODC classification by casting it as a supervised text classification problem. Rather than merely apply the standard machine learning framework to this task, we seek to acquire a better ODC classification system by integrating experts' ODC experience and domain knowledge into the learning process via proposing a novel Relevance Annotation Framework. We evaluated AutoODC on an industrial defect report from the social network domain. AutoODC is a promising approach: not only does it leverage minimal human effort beyond the human annotations typically required by standard machine learning approaches, but it achieves an overall accuracy of 80.2\% when using manual classifications as a basis of comparison.}, 
keywords={learning (artificial intelligence);program debugging;AutoODC;automated generation of orthogonal defect classifications;influential framework;machine learning framework;relevance annotation framework;social network;software bug;software defect analysis;software defect classification;text classification problem;Accuracy;Humans;Machine learning;Manuals;Support vector machines;Text categorization;Training;Orthogonal Defect Classification (ODC);natural language processing;text classification}, 
doi={10.1109/ASE.2011.6100086}, 
ISSN={1938-4300}, 
month={Nov},}


@INPROCEEDINGS{YasutakaEtAl:07,
  author = {Y. Kamei and A. Monden and S. Matsumoto and T. Kakimoto and K. Matsumoto},
  title = {The effects of over and under sampling on fault--prone module detection},
  booktitle = {Empirical Software Engineering and Measurement (ESEM)},
  year = {2007},
  pages = {196--204}
}

@ARTICLE{02Kho,
  author = {T. Khoshgoftaar and E. Allen and J. Deng},
  title = {Using regression trees to classify fault-prone software modules},
  journal = {IEEE Transactions on Reliability},
  year = {2002},
  optnumber = {4},
  optvolume = {51}
}


@ARTICLE{KhoshgoftaarEtAl:05,
  author = {T.M. Khoshgoftaar and N. Seliya and K. Gao},
  title = {Assessment of a New Three-Group Software Quality Classification Technique:
	An Empirical Case Study},
  journal = {Empirical Software Engineering},
  year = {2005},
  volume = {10},
  pages = {183-218},
  number = {2}
}


@ARTICLE{Khoshgoftaar97,
  author = {T. M. Khoshgoftaar and E. Allen and J. Hudepohl and S. Aud},
  title = {Application of Neural Networks to Software Quality Modeling of a
	Very Large Telecommunications System},
  journal = {IEEE Transactions on Neural Networks},
  year = {1997},
  volume = {8},
  pages = {902--909},
  number = {4}
}

@ARTICLE{Khoshgoftaar2004,
  author = {Taghi M. Khoshgoftaar and Naeem Seliya},
  title = {Comparative Assessment of Software Quality Classification Techniques:
	An Empirical Case Study},
  journal = {Empirical Software Engineering},
  year = {2004},
  volume = {9},
  pages = {229--257},
  number = {3},
  month = sep,
  acmid = {990393},
  address = {Hingham, MA, USA},
  doi = {10.1023/B:EMSE.0000027781.18360.9b},
  issn = {1382-3256},
  issue_date = {September 2004},
  keywords = {Software quality classification, analysis of variance, case-based
	reasoning, decision trees, expected cost of misclassification, logistic
	regression},
  numpages = {29},
  publisher = {Kluwer Academic Publishers},
  url = {http://dx.doi.org/10.1023/B:EMSE.0000027781.18360.9b}
}

@ARTICLE{Khoshgoftaar03,
  author = {Taghi M. Khoshgoftaar and Naeem Seliya},
  title = {Analogy-Based Practical Classification Rules for Software Quality
	Estimation},
  journal = {Empirical Software Engineering},
  year = {2003},
  volume = {8},
  pages = {325--350},
  number = {4},
  address = {Hingham, MA, USA},
  doi = {http://dx.doi.org/10.1023/A:1025316301168},
  issn = {1382-3256},
  publisher = {Kluwer Academic Publishers}
}



@ARTICLE{LBMP08,
  author = {Lessmann, S. and Baesens, B. and Mues, C. and Pietsch, S.},
  title = {Benchmarking Classification Models for Software Defect Prediction:
	A Proposed Framework and Novel Findings},
  journal = {IEEE Transactions on Software Engineering},
  year = {2008},
  volume = {34},
  pages = {485--496},
  number = {4},
  month = {July-Aug.},
  abstract = {Software defect prediction strives to improve software quality and
	testing efficiency by constructing predictive classification models
	from code attributes to enable a timely identification of fault-prone
	modules. Several classification models have been evaluated for this
	task. However, due to inconsistent findings regarding the superiority
	of one classifier over another and the usefulness of metric-based
	classification in general, more research is needed to improve convergence
	across studies and further advance confidence in experimental results.
	We consider three potential sources for bias: comparing classifiers
	over one or a small number of proprietary data sets, relying on accuracy
	indicators that are conceptually inappropriate for software defect
	prediction and cross-study comparisons, and, finally, limited use
	of statistical testing procedures to secure empirical findings. To
	remedy these problems, a framework for comparative software defect
	prediction experiments is proposed and applied in a large-scale empirical
	comparison of 22 classifiers over 10 public domain data sets from
	the NASA Metrics Data repository. Overall, an appealing degree of
	predictive accuracy is observed, which supports the view that metric-based
	classification is useful. However, our results indicate that the
	importance of the particular classification algorithm may be less
	than previously assumed since no significant performance differences
	could be detected among the top 17 classifiers.},
  doi = {10.1109/TSE.2008.35},
  issn = {0098-5589},
  keywords = {benchmarking classification models;code attributes;fault-prone modules;metric-based
	classification;predictive classification models;proprietary data
	sets;software defect prediction;software quality;statistical testing
	procedures;testing efficiency;benchmark testing;software quality;statistical
	testing;}
}


@ARTICLE{Rodriguez2012,
  author = {D. Rodriguez and R. Ruiz and J.C. Riquelme and J.S. Aguilar-Ruiz},
  title = {Searching for rules to detect defective modules: A subgroup discovery
	approach},
  journal = {Information Sciences},
  year = {2012},
  volume = {191},
  pages = {14--30},
  abstract = {Data mining methods in software engineering are becoming increasingly
	important as they can support several aspects of the software development
	life-cycle such as quality. In this work, we present a data mining
	approach to induce rules extracted from static software metrics characterising
	fault-prone modules. Due to the special characteristics of the defect
	prediction data (imbalanced, inconsistency, redundancy) not all classification
	algorithms are capable of dealing with this task conveniently. To
	deal with these problems, Subgroup Discovery (SD) algorithms can
	be used to find groups of statistically different data given a property
	of interest. We propose EDER-SD (Evolutionary Decision Rules for
	Subgroup Discovery), a SD algorithm based on evolutionary computation
	that induces rules describing only fault-prone modules. The rules
	are a well-known model representation that can be easily understood
	and applied by project managers and quality engineers. Thus, rules
	can help them to develop software systems that can be justifiably
	trusted. Contrary to other approaches in SD, our algorithm has the
	advantage of working with continuous variables as the conditions
	of the rules are defined using intervals. We describe the rules obtained
	by applying our algorithm to seven publicly available datasets from
	the PROMISE repository showing that they are capable of characterising
	subgroups of fault-prone modules. We also compare our results with
	three other well known SD algorithms and the EDER-SD algorithm performs
	well in most cases.},
  doi = {10.1016/j.ins.2011.01.039},
  issn = {0020-0255},
  keywords = {Defect prediction},
  url = {http://www.sciencedirect.com/science/article/pii/S0020025511000661}
}

@INPROCEEDINGS{SeiffertEtAl07,
  author = {C. Seiffert and T.M. Khoshgoftaar and J. Van Hulse and A. Folleco},
  title = {An empirical study of the classification performance of learners
	on imbalanced and noisy software quality data},
  booktitle = {2007 IEEE International Conference on Information Reuse and Integration,
	IEEE IRI-2007},
  year = {2007},
  pages = {651--658}
}

@ARTICLE{Shatnawi2010,
  author = {Shatnawi, Raed and Li, Wei and Swain, James and Newman, Tim},
  title = {Finding software metrics threshold values using ROC curves},
  journal = {Journal of Software Maintenance and Evolution: Research and Practice},
  year = {2010},
  volume = {22},
  pages = {1--16},
  number = {1},
  abstract = {An empirical study of the relationship between object-oriented (OO)
	metrics and error-severity categories is presented. The focus of
	the study is to identify threshold values of software metrics using
	receiver operating characteristic curves. The study used the three
	releases of the Eclipse project and found threshold values for some
	OO metrics that separated no-error classes from classes that had
	high-impact errors. Although these thresholds cannot predict whether
	a class will definitely have errors in the future, they can provide
	a more scientific method to assess class error proneness and can
	be used by engineers easily.},
  doi = {10.1002/smr.404},
  issn = {1532-0618},
  keywords = {object-oriented design, object-oriented metrics, thresholds, ROC curve},
  publisher = {John Wiley \& Sons, Ltd.},
  url = {http://dx.doi.org/10.1002/smr.404}
}




@ARTICLE{SinghEDtAl2010,
  author = {Singh, Yogesh and Kaur, Arvinder and Malhotra, Ruchika},
  title = {Empirical validation of object-oriented metrics for predicting fault
	proneness models},
  journal = {Software Quality Journal},
  year = {2010},
  volume = {18},
  pages = {3--35},
  note = {10.1007/s11219-009-9079-6},
  abstract = {Empirical validation of software metrics used to predict software
	quality attributes is important to ensure their practical relevance
	in software organizations. The aim of this work is to find the relation
	of object-oriented (OO) metrics with fault proneness at different
	severity levels of faults. For this purpose, different prediction
	models have been developed using regression and machine learning
	methods. We evaluate and compare the performance of these methods
	to find which method performs better at different severity levels
	of faults and empirically validate OO metrics given by Chidamber
	and Kemerer. The results of the empirical study are based on public
	domain NASA data set. The performance of the predicted models was
	evaluated using Receiver Operating Characteristic (ROC) analysis.
	The results show that the area under the curve (measured from the
	ROC analysis) of models predicted using high severity faults is low
	as compared with the area under the curve of the model predicted
	with respect to medium and low severity faults. However, the number
	of faults in the classes correctly classified by predicted models
	with respect to high severity faults is not low. This study also
	shows that the performance of machine learning methods is better
	than logistic regression method with respect to all the severities
	of faults. Based on the results, it is reasonable to claim that models
	targeted at different severity levels of faults could help for planning
	and executing testing by focusing resources on fault-prone parts
	of the design and code that are likely to cause serious failures.},
  affiliation = {University School of Information Technology, GGS Indraprastha University
	Delhi 110403 India},
  issn = {0963-9314},
  issue = {1},
  keyword = {Computer Science},
  publisher = {Springer Netherlands},
  url = {http://dx.doi.org/10.1007/s11219-009-9079-6}
}

@ARTICLE{SongTSE96,
  author = {Q. Song and M. Shepperd and M. Cartwright and C. Mair},
  title = {Software defect association mining and defect correction effort prediction},
  journal = {IEEE Transactions on Software Engineering},
  year = {2006},
  volume = {32},
  pages = {69--82},
  number = {2},
  month = {Feb.},
  doi = {10.1109/TSE.2006.1599417},
  issn = {0098-5589},
  keywords = {Accuracy;Association rules;Data mining;Inspection;Job shop scheduling;Project
	management;Resource management;Software development management;Software
	quality;Software systems; data mining; program testing; software
	process improvement; software quality; SEL defect data; association
	rule mining; defect correction effort prediction; software defect
	association prediction; software quality; Software defect prediction;
	defect association; defect correction effort.; defect isolation effort;}
}

@INPROCEEDINGS{VanHulseEtAl:2007,
  author = {J. {Van Hulse} and T.~M. Khoshgoftaar and A. Napolitano},
  title = {Experimental perspectives on learning from imbalanced data},
  booktitle = {Proceedings of the 24th International Conference on Machine Learning
	(ICML07)},
  year = {2007},
  address = {New York, USA},
  publisher = {ACM}
}

@ARTICLE{08MSR,
  author = {O. Vandecruys and D. Martens and B. Baesens and C. Mues and M. Backer
	and R. Haesen},
  title = {Mining software repositories for comprehensible software fault prediction
	models},
  journal = {Journal of Systems and Software},
  year = {2008},
  volume = {81},
  pages = {823--839},
  number = {5}
}

@ARTICLE{Vandecruys2008,
  author = {Olivier Vandecruys and David Martens and Bart Baesens and Christophe
	Mues and Manu {De Backer} and Raf Haesen},
  title = {Mining software repositories for comprehensible software fault prediction
	models},
  journal = {Journal of Systems and Software},
  year = {2008},
  volume = {81},
  pages = {823--839},
  number = {5},
  abstract = {Software managers are routinely confronted with software projects
	that contain errors or inconsistencies and exceed budget and time
	limits. By mining software repositories with comprehensible data
	mining techniques, predictive models can be induced that offer software
	managers the insights they need to tackle these quality and budgeting
	problems in an efficient way. This paper deals with the role that
	the Ant Colony Optimization (ACO)-based classification technique
	AntMiner+ can play as a comprehensible data mining technique to predict
	erroneous software modules. In an empirical comparison on three real-world
	public datasets, the rule-based models produced by AntMiner+ are
	shown to achieve a predictive accuracy that is competitive to that
	of the models induced by several other included classification techniques,
	such as C4.5, logistic regression and support vector machines. In
	addition, we will argue that the intuitiveness and comprehensibility
	of the AntMiner+ models can be considered superior to the latter
	models.},
  doi = {DOI: 10.1016/j.jss.2007.07.034},
  issn = {0164-1212},
  keywords = {Classification},
  url = {http://www.sciencedirect.com/science/article/B6V0N-4PHSC3R-2/2/129ce670cc51b090011f548066bb5711}
}

@BOOK{WFH11,
  title = {Data Mining: Practical machine learning tools and techniques},
  publisher = {Morgan Kaufmann},
  year = {2011},
  author = {I.H. Witten and E. Frank and M.A. Hall},
  address = {San Francisco},
  edition = {3rd Edition}
}

@BOOK{Wohlin2000ESE,
  title = {Experimentation in software engineering: an introduction},
  publisher = {Kluwer Academic Publishers},
  year = {2000},
  author = {Wohlin, Claes and Runeson, Per and H\"{o}st, Martin and Ohlsson,
	Magnus C. and Regnell, Bj\"{o}orn and Wessl{\'e}n, Anders},
  address = {Norwell, MA, USA},
  isbn = {0-7923-8682-5}
}

@INBOOK{Wrobel01,
  chapter = {An algorithm for multi-relational discovery of subgroups},
  pages = {74--101},
  title = {Relational Data Mining},
  publisher = {Springer},
  year = {2001},
  editor = {Saso Dzeroski and Nada Lavra\u{c}},
  author = {Stefan Wrobel}
}

@INPROCEEDINGS{Wrobel97,
  author = {Stefan Wrobel},
  title = {An algorithm for multi-relational discovery of subgroups},
  booktitle = {Proceedings of the 1st European Symposium on Principles of Data Mining},
  year = {1997},
  pages = {78--87}
}


@ARTICLE{Menzies07b,
  author = {Tim Menzies and Alex Dekhtyar and Justin Distefano and Jeremy Greenwald},
  title = {Problems with Precision: A Response to Comments on Data Mining Static
	Code Attributes to Learn Defect Predictors},
  journal = {IEEE Transactions on Software Engineering},
  year = {2007},
  volume = {33},
  pages = {637--640},
  number = {9},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/TSE.2007.70721},
  issn = {0098-5589},
  publisher = {IEEE Computer Society}
}

@ARTICLE{Menzies07,
  author = {T. Menzies and J. Greenwald and A. Frank},
  title = {Data Mining Static Code Attributes to Learn Defect Predictors},
  journal = {IEEE Transactions on Software Engineering},
  year = {2007},
  optmonth = {January},
  optnumber = {1},
  optpages = {2--13},
  optvolume = {33}
}

@ARTICLE{Zhang07,
  author = {Hongyu Zhang and Xiuzhen Zhang},
  title = {Comments on "Data Mining Static Code Attributes to Learn Defect Predictors"},
  journal = {IEEE Transactions on Software Engineering},
  year = {2007},
  volume = {33},
  pages = {635--637},
  number = {9},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/TSE.2007.70706},
  issn = {0098-5589},
  publisher = {IEEE Computer Society}
}

@INPROCEEDINGS{Zimmermann07Eclipse,
  author = {Zimmermann, T. and Premraj, R. and Zeller, A.},
  title = {Predicting Defects for Eclipse},
  booktitle = {International Workshop on Predictor Models in Software Engineering
	(PROMISE'07)},
  year = {2007},
  pages = {9},
  month = {may},
  abstract = {We have mapped defects from the bug database of eclipse (one of the
	largest open-source projects) to source code locations. The resulting
	data set lists the number of pre- and post-release defects for every
	package and file in the eclipse releases 2.0, 2.1, and 3.0. We additionally
	annotated the data with common complexity metrics. All data is publicly
	available and can serve as a benchmark for defect prediction models.},
  doi = {10.1109/PROMISE.2007.10},
  keywords = {Eclipse;bug database;common complexity metrics;defect prediction models;open-source
	projects;source code locations;program debugging;public domain software;}
}

@INPROCEEDINGS{KhoshgoftaarGKN12,
  author = {Khoshgoftaar, Taghi M. and Gao, Kehan and Napolitano, Amri},
  title = {Exploring an iterative feature selection technique for highly imbalanced
	data sets},
  booktitle = {IEEE 13th International Conference on Information Reuse and
	Integration (IRI'2012)},
  year = {2012},
  pages = {101--108},
  month = {aug.},
  abstract = {The quality of a classification model is affected by two factors in
	a training data set: (1) the presence of excessive features and (2)
	the presence of imbalanced distributions between two classes in a
	binary classification problem. This paper presents an iterative feature
	selection method to deal with these two problems. The proposed method
	consists of an iterative process of data sampling followed by feature
	ranking and finally aggregating the results generated during the
	iterative process. In this study, we investigate a number of feature
	ranking techniques and a data sampling method with two different
	post-sampling proportions between the two classes. We compare the
	iterative feature selection technique to the one where a data sampling
	and a feature ranking technique are used together but only once (without
	iteration). The empirical study is carried out on two groups of highly
	imbalanced data sets from a real-world software system. The results
	demonstrate that our proposed iterative feature selection technique
	performs on average better than the method without iteration.},
  doi = {10.1109/IRI.2012.6302997}
}

@article{Hosseini2017,
title = "A benchmark study on the effectiveness of search-based data selection and feature selection for cross project defect prediction",
journal = "Information and Software Technology",
volume = "",
number = "",
pages = "-",
year = "2017",
note = "",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2017.06.004",
url = "http://www.sciencedirect.com/science/article/pii/S0950584916303500",
author = "Seyedrebvar Hosseini and Burak Turhan and Mika Mantyla",
keywords = "Cross project defect prediction",
keywords = "Search based optimization",
keywords = "Genetic algorithms",
keywords = "Instance selection",
keywords = "Training data selection "
}


@inproceedings{Zimmermann2009,
 author = {Zimmermann, Thomas and Nagappan, Nachiappan and Gall, Harald and Giger, Emanuel and Murphy, Brendan},
 title = {Cross-project Defect Prediction: A Large Scale Experiment on Data vs. Domain vs. Process},
 booktitle = {Proceedings of the the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
 series = {ESEC/FSE'09},
 year = {2009},
 isbn = {978-1-60558-001-2},
 location = {Amsterdam, The Netherlands},
 pages = {91--100},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1595696.1595713},
 doi = {10.1145/1595696.1595713},
 acmid = {1595713},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {churn, cross-project, decision trees, defect prediction, logistic regression, prediction quality},
} 


@INPROCEEDINGS{Canfora2013, 
author={G. Canfora and A. De Lucia and M. Di Penta and R. Oliveto and A. Panichella and S. Panichella}, 
booktitle={IEEE Sixth International Conference on Software Testing, Verification and Validation}, 
title={Multi-objective Cross-Project Defect Prediction}, 
year={2013}, 
pages={252--261}, 
abstract={Cross-project defect prediction is very appealing because (i) it allows predicting defects in projects for which the availability of data is limited, and (ii) it allows producing generalizable prediction models. However, existing research suggests that cross-project prediction is particularly challenging and, due to heterogeneity of projects, prediction accuracy is not always very good. This paper proposes a novel, multi-objective approach for cross-project defect prediction, based on a multi-objective logistic regression model built using a genetic algorithm. Instead of providing the software engineer with a single predictive model, the multi-objective approach allows software engineers to choose predictors achieving a compromise between number of likely defect-prone artifacts (effectiveness) and LOC to be analyzed/tested (which can be considered as a proxy of the cost of code inspection). Results of an empirical evaluation on 10 datasets from the Promise repository indicate the superiority and the usefulness of the multi-objective approach with respect to single-objective predictors. Also, the proposed approach outperforms an alternative approach for cross-project prediction, based on local prediction upon clusters of similar classes.}, 
keywords={genetic algorithms;pattern clustering;regression analysis;search problems;software engineering;software management;GA;LOC analysis;LOC testing;Promise repository;defect-prone artifacts;generalizable prediction models;genetic algorithm;multiobjective cross-project defect prediction accuracy;multiobjective logistic regression model;project heterogeneity;software engineers;Accuracy;Data models;Inspection;Logistics;Measurement;Predictive models;Software;Cross-project defect prediction;multi-objective optimization;search-based software engineering}, 
doi={10.1109/ICST.2013.38}, 
ISSN={2159-4848}, 
month={March},}


@INPROCEEDINGS{Panichella2014, 
author={A. Panichella and R. Oliveto and A. De Lucia}, 
booktitle={2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)}, 
title={Cross-project defect prediction models: L'Union fait la force}, 
year={2014}, 
pages={164--173}, 
abstract={Existing defect prediction models use product or process metrics and machine learning methods to identify defect-prone source code entities. Different classifiers (e.g., linear regression, logistic regression, or classification trees) have been investigated in the last decade. The results achieved so far are sometimes contrasting and do not show a clear winner. In this paper we present an empirical study aiming at statistically analyzing the equivalence of different defect predictors. We also propose a combined approach, coined as CODEP (COmbined DEfect Predictor), that employs the classification provided by different machine learning techniques to improve the detection of defect-prone entities. The study was conducted on 10 open source software systems and in the context of cross-project defect prediction, that represents one of the main challenges in the defect prediction field. The statistical analysis of the results indicates that the investigated classifiers are not equivalent and they can complement each other. This is also confirmed by the superior prediction accuracy achieved by CODEP when compared to stand-alone defect predictors.}, 
keywords={learning (artificial intelligence);pattern classification;program debugging;public domain software;statistical analysis;CODEP;L'Union fait la force;classification;combined defect predictor;cross-project defect prediction models;defect-prone entity detection;machine learning techniques;open source software systems;statistical analysis;Accuracy;Context;Logistics;Measurement;Predictive models;Regression tree analysis;Software}, 
doi={10.1109/CSMR-WCRE.2014.6747166}, 
month={Feb},}


@inproceedings{Bowes2015,
 author = {Bowes, David and Hall, Tracy and Petri\'{c}, Jean},
 title = {Different Classifiers Find Different Defects Although With Different Level of Consistency},
 booktitle = {Proceedings of the 11th International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE'15},
 series = {PROMISE'15},
 year = {2015},
 isbn = {978-1-4503-3715-1},
 location = {Beijing, China},
 pages = {3:1--3:10},
 articleno = {3},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2810146.2810149},
 doi = {10.1145/2810146.2810149},
 acmid = {2810149},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{DHR2017,
 author = {Deocadez, Roger and Harrison, Rachel and Rodriguez, Daniel},
 title = {Preliminary Study on Applying Semi-Supervised Learning to App Store Analysis},
 booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering (EASE'17)},
 series = {EASE'17},
 year = {2017},
 isbn = {978-1-4503-4804-1},
 location = {Karlskrona, Sweden},
 pages = {320--323},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/3084226.3084285},
 doi = {10.1145/3084226.3084285},
 acmid = {3084285},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Apps reviews, Mobile apps, Semi-supervised Learning},
} 

@article{RodriguezEtAlINS12,
title = "Searching for rules to detect defective modules: A subgroup discovery approach",
journal = "Information Sciences",
volume = "191",
number = "0",
pages = "14--30",
year = "2012",
note = "<ce:title>Data Mining for Software Trustworthiness</ce:title>",
issn = "0020-0255",
doi = "10.1016/j.ins.2011.01.039",
url = "http://www.sciencedirect.com/science/article/pii/S0020025511000661",
author = "D. Rodriguez and R. Ruiz and J.C. Riquelme and J.S. Aguilarâ€“Ruiz",
keywords = "Defect prediction",
keywords = "Subgroup discovery",
keywords = "Imbalanced datasets",
keywords = "Rules"
}


@article{DOLADO2016,
title = "Evaluation of estimation models using the Minimum Interval of Equivalence",
journal = "Applied Soft Computing",
volume = "49",
number = "",
pages = "956--967",
year = "2016",
note = "",
issn = "1568-4946",
doi = "http://dx.doi.org/10.1016/j.asoc.2016.03.026",
url = "http://www.sciencedirect.com/science/article/pii/S1568494616301557",
author = "José Javier Dolado and Daniel Rodriguez and Mark Harman and William B. Langdon and Federica Sarro",
keywords = "Software estimations",
keywords = "Soft computing",
keywords = "Equivalence Hypothesis Testing",
keywords = "Credible intervals",
keywords = "Bootstrap",
abstract = "This article proposes a new measure to compare soft computing methods for software estimation. This new measure is based on the concepts of Equivalence Hypothesis Testing (EHT). Using the ideas of EHT, a dimensionless measure is defined using the Minimum Interval of Equivalence and a random estimation. The dimensionless nature of the metric allows us to compare methods independently of the data samples used. The motivation of the current proposal comes from the biases that other criteria show when applied to the comparison of software estimation methods. In this work, the level of error for comparing the equivalence of methods is set using EHT. Several soft computing methods are compared, including genetic programming, neural networks, regression and model trees, linear regression (ordinary and least mean squares) and instance-based methods. The experimental work has been performed on several publicly available datasets. Given a dataset and an estimation method we compute the upper point of Minimum Interval of Equivalence, MIEu, on the confidence intervals of the errors. Afterwards, the new measure, MIEratio, is calculated as the relative distance of the MIEu to the random estimation. Finally, the data distributions of the MIEratios are analysed by means of probability intervals, showing the viability of this approach. In this experimental work, it can be observed that there is an advantage for the genetic programming and linear regression methods by comparing the values of the intervals."
}


@INPROCEEDINGS{LuCC2012, 
author={H. Lu and B. Cukic and M. Culp}, 
booktitle={27th IEEE/ACM International Conference on Automated Software Engineering (ASE'12)}, 
title={Software defect prediction using semi-supervised learning with dimension reduction}, 
year={2012}, 
pages={314--317}, 
abstract={Accurate detection of fault prone modules offers the path to high quality software products while minimizing non essential assurance expenditures. This type of quality modeling requires the availability of software modules with known fault content developed in similar environment. Establishing whether a module contains a fault or not can be expensive. The basic idea behind semi-supervised learning is to learn from a small number of software modules with known fault content and supplement model training with modules for which the fault information is not available. In this study, we investigate the performance of semi-supervised learning for software fault prediction. A preprocessing strategy, multidimensional scaling, is embedded in the approach to reduce the dimensional complexity of software metrics. Our results show that the semi-supervised learning algorithm with dimension-reduction preforms significantly better than one of the best performing supervised learning algorithms, random forest, in situations when few modules with known fault content are available for training.}, 
keywords={learning (artificial intelligence);software metrics;software quality;dimension reduction;fault content;fault prone module detection;high quality software products;model training;multidimensional scaling;preprocessing strategy;random forest;semisupervised learning;software defect prediction;software fault prediction;software metrics dimensional complexity;software modules availability;Software fault prediction;dimension reduction;semi-supervised learning;software metrics}, 
doi={10.1145/2351676.2351734}, 
month={Sept},}


@INPROCEEDINGS{RRCA07, 
	author={Rodriguez, D. and Ruiz, R. and Cuadrado, J. and Aguilar-Ruiz, J.}, 
	booktitle={IEEE International Conference on Information Reuse and Integration (IRI 2007)}, 
	title={Detecting Fault Modules Applying Feature Selection to Classifiers}, 
	year={2007}, 
	month={aug.}, 
	volume={}, 
	number={}, 
	pages={667--672}, 
	keywords={PROMISE repository;attribute selection techniques;automated data collection tools;classifier learning;data mining algorithms;fault module detection;feature selection;project management;software engineering databases;data mining;feature extraction;learning (artificial intelligence);pattern classification;project management;software management;}, 
	doi={10.1109/IRI.2007.4296696}, 
	ISSN={}
}


@INPROCEEDINGS{Alves2010, 
author={T.L. Alves and C.Ypma and J. Visser}, 
booktitle={IEEE International Conference on Software Maintenance (ICSM'2010)}, 
title={Deriving metric thresholds from benchmark data}, 
year={2010}, 
pages={1--10}, 
abstract={A wide variety of software metrics have been proposed and a broad range of tools is available to measure them. However, the effective use of software metrics is hindered by the lack of meaningful thresholds. Thresholds have been proposed for a few metrics only, mostly based on expert opinion and a small number of observations. Previously proposed methodologies for systematically deriving metric thresholds have made unjustified assumptions about the statistical properties of source code metrics. As a result, the general applicability of the derived thresholds is jeopardized. We designed a method that determines metric thresholds empirically from measurement data. The measurement data for different software systems are pooled and aggregated after which thresholds are selected that (i) bring out the metric's variability between systems and (ii) help focus on a reasonable percentage of the source code volume. Our method respects the distributions and scales of source code metrics, and it is resilient against outliers in metric values or system size. We applied our method to a benchmark of 100 object-oriented software systems, both proprietary and open-source, to derive thresholds for metrics included in the SIG maintainability model.}, 
keywords={object-oriented methods;public domain software;software metrics;benchmark data;metric threshold;object oriented software system;open source software;software metrics;source code metrics;Benchmark testing;Complexity theory;Histograms;Java;Measurement;Software systems}, 
doi={10.1109/ICSM.2010.5609747}, 
ISSN={1063-6773}, 
month={Sept},}


@INPROCEEDINGS{Benlarbi2000, 
author={S. Benlarbi and K. El Emam and N. Goel and S. Rai}, 
booktitle={Proceedings 11th International Symposium on Software Reliability Engineering (ISSRE 2000)}, 
title={Thresholds for object-oriented measures}, 
year={2000}, 
pages={24--38}, 
abstract={A practical application of object oriented measures is to predict which classes are likely to contain a fault. This is contended to be meaningful because object oriented measures are believed to be indicators of psychological complexity, and classes that are more complex are likely to be faulty. Recently, a cognitive theory was proposed suggesting that there are threshold effects for many object oriented measures. This means that object oriented classes are easy to understand as long as their complexity is below a threshold. Above that threshold their understandability decreases rapidly, leading to an increased probability of a fault. This occurs, according to the theory, due to an overflow of short-term human memory. If this theory is confirmed, then it would provide a mechanism that would explain the introduction of faults into object oriented systems, and would also provide some practical guidance on how to design object oriented programs. The authors empirically test this theory on two C++ telecommunications systems. They test for threshold effects in a subset of the Chidamber and Kemerer (CK) suite of measures (S. Chidamber and C. Kemerer, 1994). The dependent variable was the incidence of faults that lead to field failures. The results indicate that there are no threshold effects for any of the measures studied. This means that there is no value for the studied CK measures where the fault-proneness changes from being steady to rapidly increasing. The results are consistent across the two systems. Therefore, we can provide no support to the posited cognitive theory}, 
keywords={C++ language;bibliographies;object-oriented programming;reverse engineering;software metrics;software performance evaluation;software quality;telecommunication computing;C++ telecommunications systems;CK measures;cognitive theory;dependent variable;fault introduction;fault-proneness;field failures;object oriented classes;object oriented measure thresholds;object oriented programs;object oriented systems;psychological complexity;short-term human memory overflow;threshold effects;understandability;Councils;Hospitals;Humans;Inspection;Object oriented modeling;Particle measurements;Psychology;Quality management;Software measurement;System testing}, 
doi={10.1109/ISSRE.2000.885858}, 
ISSN={1071-9458}, 
month={},}


@inproceedings{Morasca2016,
 author = {Morasca, Sandro and Lavazza, Luigi},
 title = {Slope-based Fault-proneness Thresholds for Software Engineering Measures},
 booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
 series = {EASE '16},
 year = {2016},
 isbn = {978-1-4503-3691-8},
 location = {Limerick, Ireland},
 pages = {12:1--12:10},
 articleno = {12},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2915970.2915997},
 doi = {10.1145/2915970.2915997},
 acmid = {2915997},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fault-proneness, faultiness, logistic regression, probit regression, software measures, threshold},
} 


@INPROCEEDINGS{Gray2011, 
author={D. Gray and D. Bowes and N. Davey and Y. Sun and B. Christianson}, 
booktitle={15th Annual Conference on Evaluation Assessment in Software Engineering (EASE 2011)}, 
title={The misuse of the NASA metrics data program data sets for automated software defect prediction}, 
year={2011}, 
pages={96--103}, 
abstract={Background: The NASA Metrics Data Program data sets have been heavily used in software defect prediction experiments. Aim: To demonstrate and explain why these data sets require significant pre-processing in order to be suitable for defect prediction. Method: A meticulously documented data cleansing process involving all 13 of the original NASA data sets. Results: Post our novel data cleansing process; each of the data sets had between 6 to 90 percent less of their original number of recorded values. Conclusions: One: Researchers need to analyse the data that forms the basis of their findings in the context of how it will be used. Two: Defect prediction data sets could benefit from lower level code metrics in addition to those more commonly used, as these will help to distinguish modules, reducing the likelihood of repeated data points. Three: The bulk of defect prediction experiments based on the NASA Metrics Data Program data sets may have led to erroneous findings. This is mainly due to repeated data points potentially causing substantial amounts of training and testing data to be identical.}, 
keywords={data mining;fault tolerant computing;NASA metrics data program data set;automated software defect prediction;data cleansing process}, 
doi={10.1049/ic.2011.0012}, 
month={April},}



@ARTICLE{Shepperd2013,
author={Shepperd, M. and Qinbao Song and Zhongbin Sun and Mair, C.},
journal={IEEE Transactions on Software Engineering},
title={Data Quality: Some Comments on the NASA Software Defect Datasets},
year={2013},
volume={39},
number={9},
pages={1208--1215},
keywords={data analysis;learning (artificial intelligence);pattern classification;software reliability;IEEE Transactions on Software Engineering;
          NASA software defect dataset;National Aeronautics and Space Administration;data preprocessing;data quality;data replication;
          dataset provenance;defect-prone classification;machine learning;not-defect-prone classification;
          software module classification;Abstracts;Communities;Educational institutions;NASA;PROM;Software;Sun;Empirical software engineering;
          data quality;defect prediction;machine learning},
doi={10.1109/TSE.2013.11},
ISSN={0098-5589},
month={Sept},}


@Article{CC2009,
  Title                    = {A systematic review of software fault prediction studies},
  Author                   = {Cagatay Catal and Banu Diri},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2009},
  Number                   = {4},
  Pages                    = {7346--7354},
  Volume                   = {36},

  Abstract                 = {This paper provides a systematic review of previous software fault prediction studies with a specific focus on metrics, methods, and datasets. The review uses 74 software fault prediction papers in 11 journals and several conference proceedings. According to the review results, the usage percentage of public datasets increased significantly and the usage percentage of machine learning algorithms increased slightly since 2005. In addition, method-level metrics are still the most dominant metrics in fault prediction research area and machine learning algorithms are still the most popular methods for fault prediction. Researchers working on software fault prediction area should continue to use public datasets and machine learning algorithms to build better fault predictors. The usage percentage of class-level is beyond acceptable levels and they should be used much more than they are now in order to predict the faults earlier in design phase of software life cycle.},
  Doi                      = {10.1016/j.eswa.2008.10.027},
  ISSN                     = {0957-4174},
  Keywords                 = {Machine learning},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417408007215}
}

@Article{Li2012,
author="Li, Ming
and Zhang, Hongyu
and Wu, Rongxin
and Zhou, Zhi-Hua",
title="Sample-based software defect prediction with active and semi-supervised learning",
journal="Automated Software Engineering",
year="2012",
month="Jun",
day="01",
volume="19",
number="2",
pages="201--230",
abstract="Software defect prediction can help us better understand and control software quality. Current defect prediction techniques are mainly based on a sufficient amount of historical project data. However, historical data is often not available for new projects and for many organizations. In this case, effective defect prediction is difficult to achieve. To address this problem, we propose sample-based methods for software defect prediction. For a large software system, we can select and test a small percentage of modules, and then build a defect prediction model to predict defect-proneness of the rest of the modules. In this paper, we describe three methods for selecting a sample: random sampling with conventional machine learners, random sampling with a semi-supervised learner and active sampling with active semi-supervised learner. To facilitate the active sampling, we propose a novel active semi-supervised learning method ACoForest which is able to sample the modules that are most helpful for learning a good prediction model. Our experiments on PROMISE datasets show that the proposed methods are effective and have potential to be applied to industrial practice.",
issn="1573-7535",
doi="10.1007/s10515-011-0092-1",
url="http://dx.doi.org/10.1007/s10515-011-0092-1"
}


@INPROCEEDINGS{Ibarguren2017, 
author={Igor Ibarguren and J.M. P\'erez and Javier Muguerza and Daniel Rodriguez and Rachel Harrison}, 
booktitle={Evolutionary Methods and Machine Learning in SE, Testing and SE Repositories. Proceedings of the 2017 IEEE Congress on Evolutionary Computation (CEC2017)}, 
title={The Consolidated Tree Construction Algorithm in Imbalanced Defect Prediction Datasets}, 
year={2017}, 
pages={96--103}, 
}

@ARTICLE{TurhanESE12,
  author = {Turhan, Burak},
  title = {On the dataset shift problem in software engineering prediction models},
  journal = {Empirical Software Engineering},
  year = {2012},
  volume = {17},
  pages = {62--74},
  note = {10.1007/s10664-011-9182-8},
  affiliation = {Department of Information Processing Science, University of Oulu,
	POB.3000, 90014 Oulu, Finland},
  issn = {1382-3256},
  issue = {1},
  keyword = {Computer Science},
  publisher = {Springer Netherlands}
}


@INPROCEEDINGS{WangKWN12,
  author = {Huanjing Wang and Taghi M. Khoshgoftaar and Randall Wald and Amri
	Napolitano},
  title = {A Comparative Study on the Stability of Software Metric Selection
	Techniques},
  booktitle = {11th International Conference on Machine Learning and Applications,
	ICMLA},
  year = {2012},
  pages = {301--307},
  ee = {http://dx.doi.org/10.1109/ICMLA.2012.142}
}



@INPROCEEDINGS{ZPZ07,
  author = {Zimmermann, Thomas and Premraj, Rahul and Zeller, Andreas},
  title = {Predicting Defects for Eclipse},
  booktitle = {Proceedings of the Third International Workshop on Predictor Models
	in Software Engineering (PROMISE'07)},
  year = {2007},
  series = {PROMISE '07},
  pages = {9--},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid = {1269057},
  doi = {http://dx.doi.org/10.1109/PROMISE.2007.10},
  isbn = {0-7695-2954-2}
}


@ARTICLE{Elish08,
  author = {Karim O. Elish and Mahmoud O. Elish},
  title = {Predicting defect-prone software modules using support vector machines},
  journal = {Journal of Systems and Software},
  year = {2008},
  volume = {81},
  pages = {649--660},
  number = {5},
  abstract = {Effective prediction of defect-prone software modules can enable software
	developers to focus quality assurance activities and allocate effort
	and resources more efficiently. Support vector machines (SVM) have
	been successfully applied for solving both classification and regression
	problems in many applications. This paper evaluates the capability
	of SVM in predicting defect-prone software modules and compares its
	prediction performance against eight statistical and machine learning
	models in the context of four NASA datasets. The results indicate
	that the prediction performance of SVM is generally better than,
	or at least, is competitive against the compared models.},
  doi = {10.1016/j.jss.2007.07.040},
  issn = {0164-1212},
  keywords = {Software metrics},
  url = {http://www.sciencedirect.com/science/article/pii/S016412120700235X}
}

@INPROCEEDINGS{GBDSC11,
  author = {David Gray and David Bowes and Neil Davey and Yi Sun and Bruce Christianson},
  title = {The misuse of the NASA Metrics Data Program data sets for automated
	software defect prediction
	
	
	Durham, UK, 11-12 April 2011, ISBN: 978-1-84919-509-6},
  booktitle = {15th Annual Conference on Evaluation \& Assessment in Software Engineering
	(EASE 2011)},
  year = {2011},
  pages = {96--103},
  url = {http://dx.doi.org/10.1049/ic.2011.0012}
}

@INPROCEEDINGS{NZZH10,
  author = {Nachiappan Nagappan and Andreas Zeller and Thomas Zimmermann and
	Kim Herzig and Brendan Murphy},
  title = {Change Bursts as Defect Predictors},
  booktitle = {Proceedings of the 21st IEEE International Symposium on Software
	Reliability Engineering (ISSRE 2012)},
  year = {2010},
  month = {November},
  location = {San Jose, California, USA}
}

@InProceedings{Zhang2009,
  Title                    = {An investigation of the relationships between lines of code and defects},
  Author                   = {H. Zhang},
  Booktitle                = {IEEE International Conference on Software Maintenance},
  Year                     = {2009},
  Month                    = {Sept},
  Pages                    = {274--283},

  Abstract                 = {It is always desirable to understand the quality of a software system based on static code metrics. In this paper, we analyze the relationships between lines of code (LOC) and defects (including both pre-release and post-release defects). We confirm the ranking ability of LOC discovered by Fenton and Ohlsson. Furthermore, we find that the ranking ability of LOC can be formally described using Weibull functions. We can use defect density values calculated from a small percentage of largest modules to predict the number of total defects accurately. We also find that, given LOC we can predict the number of defective components reasonably well using typical classification techniques. We perform an extensive experiment using the public Eclipse dataset, and replicate the study using the NASA dataset. Our results confirm that simple static code attributes such as LOC can be useful predictors of software quality.},
  Doi                      = {10.1109/ICSM.2009.5306304},
  ISSN                     = {1063-6773},
  Keywords                 = {program diagnostics;software metrics;software quality;Eclipse dataset;lines of code;post-release defects;pre-release defects;software system quality;static code metrics;Density measurement;Lab-on-a-chip;Laboratories;NASA;Packaging machines;Predictive models;Software metrics;Software quality;Software systems;System testing}
}

@InProceedings{ZimmermannPR2007,
  Title                    = {Predicting Defects for Eclipse},
  Author                   = {Zimmermann, Thomas and Premraj, Rahul and Zeller, Andreas},
  Booktitle                = {Proceedings of the Third International Workshop on Predictor Models in Software Engineering},
  Year                     = {2007},

  Address                  = {Washington, DC, USA},
  Pages                    = {9--},
  Publisher                = {IEEE Computer Society},
  Series                   = {PROMISE'07},

  Doi                      = {10.1109/PROMISE.2007.10},
  ISBN                     = {0-7695-2954-2},
  Url                      = {http://dx.doi.org/10.1109/PROMISE.2007.10}
}

@Article{Madeyski2015,
  Title                    = {Which process metrics can significantly improve defect prediction models? An empirical study},
  Author                   = {Madeyski, Lech and Jureczko, Marian},
  Journal                  = {Software Quality Journal},
  Year                     = {2015},
  Number                   = {3},
  Pages                    = {393--422},
  Volume                   = {23},

  Abstract                 = {The knowledge about the software metrics which serve as defect indicators is vital for the efficient allocation of resources for quality assurance. It is the process metrics, although sometimes difficult to collect, which have recently become popular with regard to defect prediction. However, in order to identify rightly the process metrics which are actually worth collecting, we need the evidence validating their ability to improve the product metric-based defect prediction models. This paper presents an empirical evaluation in which several process metrics were investigated in order to identify the ones which significantly improve the defect prediction models based on product metrics. Data from a wide range of software projects (both, industrial and open source) were collected. The predictions of the models that use only product metrics (simple models) were compared with the predictions of the models which used product metrics, as well as one of the process metrics under scrutiny (advanced models). To decide whether the improvements were significant or not, statistical tests were performed and effect sizes were calculated. The advanced defect prediction models trained on a data set containing product metrics and additionally Number of Distinct Committers (NDC) were significantly better than the simple models without NDC, while the effect size was medium and the probability of superiority (PS) of the advanced models over simple ones was high p=.016, which is a substantial finding useful in defect prediction. A similar result with slightly smaller PS was achieved by the advanced models trained on a data set containing product metrics and additionally all of the investigated process metrics p=.038. The advanced models trained on a data set containing product metrics and additionally Number of Modified Lines (NML) were significantly better than the simple models without NML, but the effect size was small p=.038. Hence, it is reasonable to recommend the NDC process metric in building the defect prediction models.},
  Doi                      = {10.1007/s11219-014-9241-7},
  ISSN                     = {1573-1367},
  Url                      = {http://dx.doi.org/10.1007/s11219-014-9241-7}
}

@InProceedings{JureczkoS10,
  Title                    = {Using Object-Oriented Design Metrics to Predict Software Defects},
  Author                   = {Jureczko, Marian and Spinellis, Diomidis},
  Booktitle                = {Models and Methodology of System Dependability. Proceedings of {RELCOMEX} 2010 Fifth International Conference on Dependability of Computer Systems {DepCoS}},
  Year                     = {2010},

  Address                  = {Wroc{\l}aw, Poland},
  Pages                    = {69--81},
  Publisher                = {Oficyna Wydawnicza Politechniki Wroc{\l}awskiej},
  Series                   = {Monographs of System Dependability},

  ISBN                     = {978-83-7493-526-5},
  Url                      = {http://www.dmst.aueb.gr/dds/pubs/conf/2010-DepCoS-RELCOMEX-ckjm-defects/html/JS10.html}
}

@InProceedings{NZZH12,
  Title                    = {Change Bursts as Defect Predictors},
  Author                   = {Nachiappan Nagappan and Andreas Zeller and Thomas Zimmermann and Kim Herzig and Brendan Murphy},
  Booktitle                = {21st IEEE International Symposium on Software Reliability Engineering (ISSRE 2012)},
  Year                     = {2012},
  Month                    = {November},

  Location                 = {San Jose, California, USA}
}




