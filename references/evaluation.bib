% This file was created with JabRef 2.10.
% Encoding: ISO8859_1


@InProceedings{Davis2006,
  Title                    = {The Relationship Between Precision-Recall and ROC Curves},
  Author                   = {Davis, Jesse and Goadrich, Mark},
  Booktitle                = {23rd International Conference on Machine Learning},
  Year                     = {2006},

  Address                  = {New York, NY, USA},
  Pages                    = {233--240},
  Publisher                = {ACM},
  Series                   = {ICML'06},

  Acmid                    = {1143874},
  Doi                      = {10.1145/1143844.1143874},
  ISBN                     = {1-59593-383-2},
  Location                 = {Pittsburgh, Pennsylvania, USA},
  Numpages                 = {8},
  Url                      = {http://doi.acm.org/10.1145/1143844.1143874}
}

@Article{DrummondH2006,
  Title                    = {Cost curves: An improved method for visualizing classifier performance},
  Author                   = {Drummond, Chris and Holte, Robert C.},
  Journal                  = {Machine Learning},
  Year                     = {2006},

  Month                    = {Oct},
  Number                   = {1},
  Pages                    = {95--130},
  Volume                   = {65},

  Abstract                 = {This paper introduces cost curves, a graphical technique for visualizing the performance (error rate or expected cost) of 2-class classifiers over the full range of possible class distributions and misclassification costs. Cost curves are shown to be superior to ROC curves for visualizing classifier performance for most purposes. This is because they visually support several crucial types of performance assessment that cannot be done easily with ROC curves, such as showing confidence intervals on a classifier's performance, and visualizing the statistical significance of the difference in performance of two classifiers. A software tool supporting all the cost curve analysis described in this paper is available from the authors.},
  Day                      = {01},
  Doi                      = {10.1007/s10994-006-8199-5},
  ISSN                     = {1573-0565},
  Url                      = {http://dx.doi.org/10.1007/s10994-006-8199-5}
}

@Article{Fawcett2006,
  Title                    = {An introduction to ROC analysis},
  Author                   = {Tom Fawcett},
  Journal                  = {Pattern Recognition Letters},
  Year                     = {2006},
  Note                     = {ROC Analysis in Pattern Recognition},
  Number                   = {8},
  Pages                    = {861--874},
  Volume                   = {27},

  Abstract                 = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.},
  Doi                      = {http://dx.doi.org/10.1016/j.patrec.2005.10.010},
  ISSN                     = {0167-8655},
  Keywords                 = {ROC analysis},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S016786550500303X}
}

@Book{japkowicz2011evaluating,
  Title                    = {Evaluating Learning Algorithms: A Classification Perspective},
  Author                   = {Japkowicz, N. and Shah, M.},
  Publisher                = {Cambridge University Press},
  Year                     = {2011},
  Series                   = {Evaluating Learning Algorithms: A Classification Perspective},

  ISBN                     = {9780521196000},
  Lccn                     = {2010048733},
  Url                      = {http://books.google.es/books?id=VoWIIOKVzR4C}
}

@Article{Kononenko1991,
  Title                    = {Information-Based Evaluation Criterion for Classifier's Performance},
  Author                   = {Kononenko, Igor and Bratko, Ivan},
  Journal                  = {Machine Learning},
  Year                     = {1991},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {67--80},
  Volume                   = {6},

  Abstract                 = {In the past few years many systems for learning decision rules from examples were developed. As different systems allow different types of answers when classifying new instances, it is difficult to appropriately evaluate the systems' classification power in comparison with other classification systems or in comparison with human experts. Classification accuracy is usually used as a measure of classification performance. This measure is, however, known to have several defects. A fair evaluation criterion should exclude the influence of the class probabilities which may enable a completely uninformed classifier to trivially achieve high classification accuracy. In this paper a method for evaluating the information score of a classifier's answers is proposed. It excludes the influence of prior probabilities, deals with various types of imperfect or probabilistic answers and can be used also for comparing the performance in different domains.},
  Day                      = {01},
  Doi                      = {10.1023/A:1022642017308},
  ISSN                     = {1573-0565},
  Url                      = {http://dx.doi.org/10.1023/A:1022642017308}
}

@Article{Matthews1975Comparison,
  Title                    = {Comparison of the predicted and observed secondary structure of T4 phage lysozyme},
  Author                   = {Matthews, B. W.},
  Journal                  = {Biochimica et biophysica acta},
  Year                     = {1975},

  Month                    = oct,
  Number                   = {2},
  Pages                    = {442--451},
  Volume                   = {405},

  Abstract                 = {Predictions of the secondary structure of T4 phage lysozyme, made by a number of investigators on the basis of the amino acid sequence, are compared with the structure of the protein determined experimentally by X-ray crystallography. Within the amino terminal half of the molecule the locations of helices predicted by a number of methods agree moderately well with the observed structure, however within the carboxyl half of the molecule the overall agreement is poor. For eleven different helix predictions, the coefficients giving the correlation between prediction and observation range from 0.14 to 0.42. The accuracy of the predictions for both beta-sheet regions and for turns are generally lower than for the helices, and in a number of instances the agreement between prediction and observation is no better than would be expected for a random selection of residues. The structural predictions for T4 phage lysozyme are much less successful than was the case for adenylate kinase (Schulz et al. (1974) Nature 250, 140-142). No one method of prediction is clearly superior to all others, and although empirical predictions based on larger numbers of known protein structure tend to be more accurate than those based on a limited sample, the improvement in accuracy is not dramatic, suggesting that the accuracy of current empirical predictive methods will not be substantially increased simply by the inclusion of more data from additional protein structure determinations.},
  ISSN                     = {0006-3002},
  Keywords                 = {classification, diplomarbeit, evaluation, ml},
  Priority                 = {2},
  Url                      = {http://view.ncbi.nlm.nih.gov/pubmed/1180967}
}

