% This file was created with JabRef 2.10.
% Encoding: ISO8859_1


@InProceedings{SliwerskiZZ2005,
  Title                    = {When Do Changes Induce Fixes?},
  Author                   = {\'{S}liwerski, Jacek and Zimmermann, Thomas and Zeller, Andreas},
  Booktitle                = {Proceedings of the 2005 International Workshop on Mining Software Repositories},
  Year                     = {2005},

  Address                  = {New York, NY, USA},
  Pages                    = {1--5},
  Publisher                = {ACM},
  Series                   = {MSR'05},

  Acmid                    = {1083147},
  Doi                      = {10.1145/1082983.1083147},
  ISBN                     = {1-59593-123-6},
  Location                 = {St. Louis, Missouri},
  Numpages                 = {5},
  Url                      = {http://doi.acm.org/10.1145/1082983.1083147}
}

@InProceedings{Alves2010,
  Title                    = {Deriving metric thresholds from benchmark data},
  Author                   = {T.L. Alves and C.Ypma and J. Visser},
  Booktitle                = {IEEE International Conference on Software Maintenance (ICSM'2010)},
  Year                     = {2010},
  Month                    = {Sept},
  Pages                    = {1--10},

  Abstract                 = {A wide variety of software metrics have been proposed and a broad range of tools is available to measure them. However, the effective use of software metrics is hindered by the lack of meaningful thresholds. Thresholds have been proposed for a few metrics only, mostly based on expert opinion and a small number of observations. Previously proposed methodologies for systematically deriving metric thresholds have made unjustified assumptions about the statistical properties of source code metrics. As a result, the general applicability of the derived thresholds is jeopardized. We designed a method that determines metric thresholds empirically from measurement data. The measurement data for different software systems are pooled and aggregated after which thresholds are selected that (i) bring out the metric's variability between systems and (ii) help focus on a reasonable percentage of the source code volume. Our method respects the distributions and scales of source code metrics, and it is resilient against outliers in metric values or system size. We applied our method to a benchmark of 100 object-oriented software systems, both proprietary and open-source, to derive thresholds for metrics included in the SIG maintainability model.},
  Doi                      = {10.1109/ICSM.2010.5609747},
  ISSN                     = {1063-6773},
  Keywords                 = {object-oriented methods;public domain software;software metrics;benchmark data;metric threshold;object oriented software system;open source software;software metrics;source code metrics;Benchmark testing;Complexity theory;Histograms;Java;Measurement;Software systems}
}

@InProceedings{Benlarbi2000,
  Title                    = {Thresholds for object-oriented measures},
  Author                   = {S. Benlarbi and K. El Emam and N. Goel and S. Rai},
  Booktitle                = {Proceedings 11th International Symposium on Software Reliability Engineering (ISSRE 2000)},
  Year                     = {2000},
  Pages                    = {24--38},

  Abstract                 = {A practical application of object oriented measures is to predict which classes are likely to contain a fault. This is contended to be meaningful because object oriented measures are believed to be indicators of psychological complexity, and classes that are more complex are likely to be faulty. Recently, a cognitive theory was proposed suggesting that there are threshold effects for many object oriented measures. This means that object oriented classes are easy to understand as long as their complexity is below a threshold. Above that threshold their understandability decreases rapidly, leading to an increased probability of a fault. This occurs, according to the theory, due to an overflow of short-term human memory. If this theory is confirmed, then it would provide a mechanism that would explain the introduction of faults into object oriented systems, and would also provide some practical guidance on how to design object oriented programs. The authors empirically test this theory on two C++ telecommunications systems. They test for threshold effects in a subset of the Chidamber and Kemerer (CK) suite of measures (S. Chidamber and C. Kemerer, 1994). The dependent variable was the incidence of faults that lead to field failures. The results indicate that there are no threshold effects for any of the measures studied. This means that there is no value for the studied CK measures where the fault-proneness changes from being steady to rapidly increasing. The results are consistent across the two systems. Therefore, we can provide no support to the posited cognitive theory},
  Doi                      = {10.1109/ISSRE.2000.885858},
  ISSN                     = {1071-9458},
  Keywords                 = {C++ language;bibliographies;object-oriented programming;reverse engineering;software metrics;software performance evaluation;software quality;telecommunication computing;C++ telecommunications systems;CK measures;cognitive theory;dependent variable;fault introduction;fault-proneness;field failures;object oriented classes;object oriented measure thresholds;object oriented programs;object oriented systems;psychological complexity;short-term human memory overflow;threshold effects;understandability;Councils;Hospitals;Humans;Inspection;Object oriented modeling;Particle measurements;Psychology;Quality management;Software measurement;System testing}
}

@InProceedings{Bowes2015,
  Title                    = {Different Classifiers Find Different Defects Although With Different Level of Consistency},
  Author                   = {Bowes, David and Hall, Tracy and Petri\'{c}, Jean},
  Booktitle                = {Proceedings of the 11th International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE'15},
  Year                     = {2015},

  Address                  = {New York, NY, USA},
  Pages                    = {3:1--3:10},
  Publisher                = {ACM},
  Series                   = {PROMISE'15},

  Acmid                    = {2810149},
  Articleno                = {3},
  Doi                      = {10.1145/2810146.2810149},
  ISBN                     = {978-1-4503-3715-1},
  Location                 = {Beijing, China},
  Numpages                 = {10},
  Url                      = {http://doi.acm.org/10.1145/2810146.2810149}
}

@Article{Canfora2015,
  Title                    = {Defect Prediction As a Multiobjective Optimization Problem},
  Author                   = {Canfora, Gerardo and {De Lucia}, Andrea and {Di Penta}, Massimiliano and Oliveto, Rocco and Panichella, Annibale and Panichella, Sebastiano},
  Journal                  = {Software Testing, Verification \& Reliability},
  Year                     = {2015},

  Month                    = jun,
  Number                   = {4},
  Pages                    = {426--459},
  Volume                   = {25},

  Acmid                    = {2858637},
  Address                  = {Chichester, UK},
  Doi                      = {10.1002/stvr.1570},
  ISSN                     = {0960-0833},
  Issue_date               = {June 2015},
  Keywords                 = {cost-effectiveness, cross-project defect prediction, defect prediction, multiobjective optimization},
  Numpages                 = {34},
  Publisher                = {John Wiley and Sons Ltd.},
  Url                      = {http://dx.doi.org/10.1002/stvr.1570}
}

@InProceedings{Canfora2013,
  Title                    = {Multi-objective Cross-Project Defect Prediction},
  Author                   = {G. Canfora and A. De Lucia and M. Di Penta and R. Oliveto and A. Panichella and S. Panichella},
  Booktitle                = {IEEE Sixth International Conference on Software Testing, Verification and Validation},
  Year                     = {2013},
  Month                    = {March},
  Pages                    = {252--261},

  Abstract                 = {Cross-project defect prediction is very appealing because (i) it allows predicting defects in projects for which the availability of data is limited, and (ii) it allows producing generalizable prediction models. However, existing research suggests that cross-project prediction is particularly challenging and, due to heterogeneity of projects, prediction accuracy is not always very good. This paper proposes a novel, multi-objective approach for cross-project defect prediction, based on a multi-objective logistic regression model built using a genetic algorithm. Instead of providing the software engineer with a single predictive model, the multi-objective approach allows software engineers to choose predictors achieving a compromise between number of likely defect-prone artifacts (effectiveness) and LOC to be analyzed/tested (which can be considered as a proxy of the cost of code inspection). Results of an empirical evaluation on 10 datasets from the Promise repository indicate the superiority and the usefulness of the multi-objective approach with respect to single-objective predictors. Also, the proposed approach outperforms an alternative approach for cross-project prediction, based on local prediction upon clusters of similar classes.},
  Doi                      = {10.1109/ICST.2013.38},
  ISSN                     = {2159-4848},
  Keywords                 = {genetic algorithms;pattern clustering;regression analysis;search problems;software engineering;software management;GA;LOC analysis;LOC testing;Promise repository;defect-prone artifacts;generalizable prediction models;genetic algorithm;multiobjective cross-project defect prediction accuracy;multiobjective logistic regression model;project heterogeneity;software engineers;Accuracy;Data models;Inspection;Logistics;Measurement;Predictive models;Software;Cross-project defect prediction;multi-objective optimization;search-based software engineering}
}

@Article{Catal2011,
  Title                    = {Software fault prediction: A literature review and current trends},
  Author                   = {Catal, Cagatay},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2011},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {4626--4636},
  Volume                   = {38},

  Doi                      = {10.1016/j.eswa.2010.10.024},
  ISSN                     = {09574174},
  Url                      = {http://linkinghub.elsevier.com/retrieve/pii/S0957417410011681}
}

@Article{CC2009,
  Title                    = {A systematic review of software fault prediction studies},
  Author                   = {Cagatay Catal and Banu Diri},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2009},
  Number                   = {4},
  Pages                    = {7346--7354},
  Volume                   = {36},

  Abstract                 = {This paper provides a systematic review of previous software fault prediction studies with a specific focus on metrics, methods, and datasets. The review uses 74 software fault prediction papers in 11 journals and several conference proceedings. According to the review results, the usage percentage of public datasets increased significantly and the usage percentage of machine learning algorithms increased slightly since 2005. In addition, method-level metrics are still the most dominant metrics in fault prediction research area and machine learning algorithms are still the most popular methods for fault prediction. Researchers working on software fault prediction area should continue to use public datasets and machine learning algorithms to build better fault predictors. The usage percentage of class-level is beyond acceptable levels and they should be used much more than they are now in order to predict the faults earlier in design phase of software life cycle.},
  Doi                      = {10.1016/j.eswa.2008.10.027},
  ISSN                     = {0957-4174},
  Keywords                 = {Machine learning},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417408007215}
}

@InProceedings{DHR2017,
  Title                    = {Preliminary Study on Applying Semi-Supervised Learning to App Store Analysis},
  Author                   = {Deocadez, Roger and Harrison, Rachel and Rodriguez, Daniel},
  Booktitle                = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering (EASE'17)},
  Year                     = {2017},

  Address                  = {New York, NY, USA},
  Pages                    = {320--323},
  Publisher                = {ACM},
  Series                   = {EASE'17},

  Acmid                    = {3084285},
  Doi                      = {10.1145/3084226.3084285},
  ISBN                     = {978-1-4503-4804-1},
  Keywords                 = {Apps reviews, Mobile apps, Semi-supervised Learning},
  Location                 = {Karlskrona, Sweden},
  Numpages                 = {4},
  Url                      = {http://doi.acm.org/10.1145/3084226.3084285}
}

@Article{DOLADO2016,
  Title                    = {Evaluation of estimation models using the Minimum Interval of Equivalence},
  Author                   = {José Javier Dolado and Daniel Rodriguez and Mark Harman and William B. Langdon and Federica Sarro},
  Journal                  = {Applied Soft Computing},
  Year                     = {2016},
  Pages                    = {956--967},
  Volume                   = {49},

  Abstract                 = {This article proposes a new measure to compare soft computing methods for software estimation. This new measure is based on the concepts of Equivalence Hypothesis Testing (EHT). Using the ideas of EHT, a dimensionless measure is defined using the Minimum Interval of Equivalence and a random estimation. The dimensionless nature of the metric allows us to compare methods independently of the data samples used. The motivation of the current proposal comes from the biases that other criteria show when applied to the comparison of software estimation methods. In this work, the level of error for comparing the equivalence of methods is set using EHT. Several soft computing methods are compared, including genetic programming, neural networks, regression and model trees, linear regression (ordinary and least mean squares) and instance-based methods. The experimental work has been performed on several publicly available datasets. Given a dataset and an estimation method we compute the upper point of Minimum Interval of Equivalence, MIEu, on the confidence intervals of the errors. Afterwards, the new measure, MIEratio, is calculated as the relative distance of the MIEu to the random estimation. Finally, the data distributions of the MIEratios are analysed by means of probability intervals, showing the viability of this approach. In this experimental work, it can be observed that there is an advantage for the genetic programming and linear regression methods by comparing the values of the intervals.},
  Doi                      = {http://dx.doi.org/10.1016/j.asoc.2016.03.026},
  ISSN                     = {1568-4946},
  Keywords                 = {Software estimations},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1568494616301557}
}

@Article{Elish08,
  Title                    = {Predicting defect-prone software modules using support vector machines},
  Author                   = {Karim O. Elish and Mahmoud O. Elish},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2008},
  Number                   = {5},
  Pages                    = {649--660},
  Volume                   = {81},

  Abstract                 = {Effective prediction of defect-prone software modules can enable software developers to focus quality assurance activities and allocate effort and resources more efficiently. Support vector machines (SVM) have been successfully applied for solving both classification and regression problems in many applications. This paper evaluates the capability of SVM in predicting defect-prone software modules and compares its prediction performance against eight statistical and machine learning models in the context of four NASA datasets. The results indicate that the prediction performance of SVM is generally better than, or at least, is competitive against the compared models.},
  Doi                      = {10.1016/j.jss.2007.07.040},
  ISSN                     = {0164-1212},
  Keywords                 = {Software metrics},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S016412120700235X}
}

@InProceedings{Gray2011,
  Title                    = {The misuse of the NASA metrics data program data sets for automated software defect prediction},
  Author                   = {D. Gray and D. Bowes and N. Davey and Y. Sun and B. Christianson},
  Booktitle                = {15th Annual Conference on Evaluation Assessment in Software Engineering (EASE 2011)},
  Year                     = {2011},
  Month                    = {April},
  Pages                    = {96--103},

  Abstract                 = {Background: The NASA Metrics Data Program data sets have been heavily used in software defect prediction experiments. Aim: To demonstrate and explain why these data sets require significant pre-processing in order to be suitable for defect prediction. Method: A meticulously documented data cleansing process involving all 13 of the original NASA data sets. Results: Post our novel data cleansing process; each of the data sets had between 6 to 90 percent less of their original number of recorded values. Conclusions: One: Researchers need to analyse the data that forms the basis of their findings in the context of how it will be used. Two: Defect prediction data sets could benefit from lower level code metrics in addition to those more commonly used, as these will help to distinguish modules, reducing the likelihood of repeated data points. Three: The bulk of defect prediction experiments based on the NASA Metrics Data Program data sets may have led to erroneous findings. This is mainly due to repeated data points potentially causing substantial amounts of training and testing data to be identical.},
  Doi                      = {10.1049/ic.2011.0012},
  Keywords                 = {data mining;fault tolerant computing;NASA metrics data program data set;automated software defect prediction;data cleansing process}
}

@InProceedings{GrayBDSC2011,
  Title                    = {Further thoughts on precision},
  Author                   = {D. Gray and D. Bowes and N. Davey and Y. Sun and B. Christianson},
  Booktitle                = {15th Annual Conference on Evaluation Assessment in Software Engineering (EASE 2011)},
  Year                     = {2011},
  Month                    = {April},
  Pages                    = {129--133},

  Abstract                 = {Background: There has been much discussion amongst automated software defect prediction researchers regarding use of the precision and false positive rate classifier performance metrics. Aim: To demonstrate and explain why failing to report precision when using data with highly imbalanced class distributions may provide an overly optimistic view of classifier performance. Method: Well documented examples of how dependent class distribution affects the suitability of performance measures. Conclusions: When using data where the minority class represents less than around 5 to 10 percent of data points in total, failing to report precision may be a critical mistake. Furthermore, deriving the precision values omitted from studies can reveal valuable insight into true classifier performance.},
  Doi                      = {10.1049/ic.2011.0016},
  Keywords                 = {data mining;learning (artificial intelligence);data mining;data points;false positive rate classifier;machine learning;metric performance;software defect automation}
}

@Article{Gupta2017,
  Title                    = {A set of measures designed to identify overlapped instances in software defect prediction},
  Author                   = {Gupta, Shivani
and Gupta, Atul},
  Journal                  = {Computing},
  Year                     = {2017},

  Month                    = {Jan},

  Abstract                 = {The performance of the learning models will intensely rely on the characteristics of the training data. The previous outcomes recommend that the overlapping between classes and the presence of noise have the most grounded impact on the performance of learning algorithm, and software defect datasets are no exceptions. The class overlap problem is concerned with the performance of machine learning classifiers critical problem is class overlap in which data samples appear as valid examples of more than one class which may be responsible for the presence of noise in datasets. We aim to investigate how the presence of overlapped instances in a dataset influences the classifier's performance, and how to deal with class overlapping problem. To have a close estimate of class overlapping, we have proposed four different measures namely, nearest enemy ratio, subconcept ratio, likelihood ratio and soft margin ratio. We performed our investigations using 327 binary defect classification datasets obtained from 54 software projects, where we first identified overlapped datasets using three data complexity measures proposed in the literature. We also include treatment effort into the prediction process. Subsequently, we used our proposed measures to find overlapped instances in the identified overlapped datasets. Our results indicated that by training a classifier on a training data free from overlapped instances led to an improved classifier performance on the test data containing overlapped instances. The classifiers perform significantly better when the evaluation measure takes the effort into account.},
  Day                      = {10},
  Doi                      = {10.1007/s00607-016-0538-1},
  ISSN                     = {1436-5057},
  Url                      = {http://dx.doi.org/10.1007/s00607-016-0538-1}
}

@Article{HallBBGC2012,
  Title                    = {A Systematic Literature Review on Fault Prediction Performance in Software Engineering},
  Author                   = {T. Hall and S. Beecham and D. Bowes and D. Gray and S. Counsell},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2012},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {1276--1304},
  Volume                   = {38},

  Abstract                 = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.},
  Doi                      = {10.1109/TSE.2011.103},
  ISSN                     = {0098-5589},
  Keywords                 = {Bayes methods;regression analysis;software fault tolerance;software quality;contextual information;cost reduction;fault prediction models;fault prediction performance;fault prediction study;feature selection;independent variables;logistic regression;methodological information;naive Bayes;predictive performance;reliable methodology;simple modeling techniques;software engineering;software quality;systematic literature review;Analytical models;Context modeling;Data models;Fault diagnosis;Predictive models;Software testing;Systematics;Systematic literature review;software fault prediction}
}

@Article{He2015,
  Title                    = {An empirical study on software defect prediction with a simplified metric set},
  Author                   = {Peng He and Bing Li and Xiao Liu and Jun Chen and Yutao Ma},
  Journal                  = {Information and Software Technology},
  Year                     = {2015},
  Pages                    = {170--190},
  Volume                   = {59},

  Abstract                 = {Software defect prediction plays a crucial role in estimating the most defect-prone components of software, and a large number of studies have pursued improving prediction accuracy within a project or across projects. However, the rules for making an appropriate decision between within- and cross-project defect prediction when available historical data are insufficient remain unclear. The objective of this work is to validate the feasibility of the predictor built with a simplified metric set for software defect prediction in different scenarios, and to investigate practical guidelines for the choice of training data, classifier and metric subset of a given project. First, based on six typical classifiers, three types of predictors using the size of software metric set were constructed in three scenarios. Then, we validated the acceptable performance of the predictor based on Top-k metrics in terms of statistical methods. Finally, we attempted to minimize the Top-k metric subset by removing redundant metrics, and we tested the stability of such a minimum metric subset with one-way ANOVA tests. The study has been conducted on 34 releases of 10 open-source projects available at the PROMISE repository. The findings indicate that the predictors built with either Top-k metrics or the minimum metric subset can provide an acceptable result compared with benchmark predictors. The guideline for choosing a suitable simplified metric set in different scenarios is presented in Table 12. The experimental results indicate that (1) the choice of training data for defect prediction should depend on the specific requirement of accuracy; (2) the predictor built with a simplified metric set works well and is very useful in case limited resources are supplied; (3) simple classifiers (e.g., Naïve Bayes) also tend to perform well when using a simplified metric set for defect prediction; and (4) in several cases, the minimum metric subset can be identified to facilitate the procedure of general defect prediction with acceptable loss of prediction precision in practice.},
  Doi                      = {http://dx.doi.org/10.1016/j.infsof.2014.11.006},
  ISSN                     = {0950-5849},
  Keywords                 = {Defect prediction},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950584914002523}
}

@InProceedings{Herbold2013,
  Title                    = {Training Data Selection for Cross-project Defect Prediction},
  Author                   = {Herbold, Steffen},
  Booktitle                = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering (PROMISE'13)},
  Year                     = {2013},

  Address                  = {New York, NY, USA},
  Pages                    = {6:1--6:10},
  Publisher                = {ACM},
  Series                   = {PROMISE'13},

  Acmid                    = {2499395},
  Articleno                = {6},
  Doi                      = {10.1145/2499393.2499395},
  ISBN                     = {978-1-4503-2016-0},
  Keywords                 = {cross-project prediction, defect-prediction, machine learning},
  Location                 = {Baltimore, Maryland, USA},
  Numpages                 = {10},
  Url                      = {http://doi.acm.org/10.1145/2499393.2499395}
}

@Article{Hosseini2017,
  Title                    = {A benchmark study on the effectiveness of search-based data selection and feature selection for cross project defect prediction},
  Author                   = {Seyedrebvar Hosseini and Burak Turhan and Mika Mantyla},
  Journal                  = {Information and Software Technology},
  Year                     = {2017},
  Pages                    = {-},

  Doi                      = {https://doi.org/10.1016/j.infsof.2017.06.004},
  ISSN                     = {0950-5849},
  Keywords                 = {Cross project defect prediction},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950584916303500}
}

@InProceedings{HuangNPGBT2011,
  Title                    = {{AutoODC}: Automated generation of Orthogonal Defect Classifications},
  Author                   = {LiGuo Huang and V. Ng and I. Persing and Ruili Geng and Xu Bai and Jeff Tian},
  Booktitle                = {26th IEEE/ACM International Conference on Automated Software Engineering (ASE'2011)},
  Year                     = {2011},
  Month                    = {Nov},
  Pages                    = {412--415},

  Abstract                 = {Orthogonal Defect Classification (ODC), the most influential framework for software defect classification and analysis, provides valuable in-process feedback to system development and maintenance. Conducting ODC classification on existing organizational defect reports is human intensive and requires experts' knowledge of both ODC and system domains. This paper presents AutoODC, an approach and tool for automating ODC classification by casting it as a supervised text classification problem. Rather than merely apply the standard machine learning framework to this task, we seek to acquire a better ODC classification system by integrating experts' ODC experience and domain knowledge into the learning process via proposing a novel Relevance Annotation Framework. We evaluated AutoODC on an industrial defect report from the social network domain. AutoODC is a promising approach: not only does it leverage minimal human effort beyond the human annotations typically required by standard machine learning approaches, but it achieves an overall accuracy of 80.2\% when using manual classifications as a basis of comparison.},
  Doi                      = {10.1109/ASE.2011.6100086},
  ISSN                     = {1938-4300},
  Keywords                 = {learning (artificial intelligence);program debugging;AutoODC;automated generation of orthogonal defect classifications;influential framework;machine learning framework;relevance annotation framework;social network;software bug;software defect analysis;software defect classification;text classification problem;Accuracy;Humans;Machine learning;Manuals;Support vector machines;Text categorization;Training;Orthogonal Defect Classification (ODC);natural language processing;text classification}
}

@InProceedings{Ibarguren2017,
  Title                    = {The Consolidated Tree Construction Algorithm in Imbalanced Defect Prediction Datasets},
  Author                   = {Igor Ibarguren and J.M. P\'erez and Javier Muguerza and Daniel Rodriguez and Rachel Harrison},
  Booktitle                = {Evolutionary Methods and Machine Learning in SE, Testing and SE Repositories. Proceedings of the 2017 IEEE Congress on Evolutionary Computation (CEC2017)},
  Year                     = {2017},
  Pages                    = {96--103}
}

@Article{JiangCM2008,
  Title                    = {Techniques for evaluating fault prediction models},
  Author                   = {Jiang, Yue and Cukic, Bojan and Ma, Yan},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2008},

  Month                    = {Oct},
  Number                   = {5},
  Pages                    = {561--595},
  Volume                   = {13},

  Abstract                 = {Many statistical techniques have been proposed to predict fault-proneness of program modules in software engineering. Choosing the ``best'' candidate among many available models involves performance assessment and detailed comparison, but these comparisons are not simple due to the applicability of varying performance measures. Classifying a software module as fault-prone implies the application of some verification activities, thus adding to the development cost. Misclassifying a module as fault free carries the risk of system failure, also associated with cost implications. Methodologies for precise evaluation of fault prediction models should be at the core of empirical software engineering research, but have attracted sporadic attention. In this paper, we overview model evaluation techniques. In addition to many techniques that have been used in software engineering studies before, we introduce and discuss the merits of cost curves. Using the data from a public repository, our study demonstrates the strengths and weaknesses of performance evaluation techniques and points to a conclusion that the selection of the best model cannot be made without considering project cost characteristics, which are specific in each development environment.},
  Doi                      = {10.1007/s10664-008-9079-3},
  ISSN                     = {1573-7616},
  Url                      = {http://dx.doi.org/10.1007/s10664-008-9079-3}
}

@InProceedings{Jiang2008,
  Title                    = {Cost Curve Evaluation of Fault Prediction Models},
  Author                   = {Y. Jiang and B. Cukic and T. Menzies},
  Booktitle                = {19th International Symposium on Software Reliability Engineering (ISSRE)},
  Year                     = {2008},
  Month                    = {Nov},
  Pages                    = {197--206},

  Abstract                 = {Prediction of fault prone software components is one of the most researched problems in software engineering. Many statistical techniques have been proposed but there is no consensus on the methodology to select the "best model" for the specific project. In this paper, we introduce and discuss the merits of cost curve analysis of fault prediction models. Cost curves allow software quality engineers to introduce project-specific cost of module misclassification into model evaluation. Classifying a software module as fault-prone implies the application of some verification activities, thus adding to the development cost. Misclassifying a module as fault free carries the risk of system failure, also associated with cost implications. Through the analysis of sixteen projects from public repositories, we observe that software quality does not necessarily benefit from the prediction of fault prone components. The inclusion of misclassification cost in model evaluation may indicate that even the "best" models achieve performance no better than trivial classification. Our results support a recommendation to adopt cost curves as one of the standard methods for software quality model performance evaluation.},
  Doi                      = {10.1109/ISSRE.2008.54},
  ISSN                     = {1071-9458},
  Keywords                 = {program verification;software cost estimation;software fault tolerance;software quality;cost curve evaluation;fault prediction model;fault prone software component;module misclassification;project-specific cost;software quality;statistical technique;Application software;Computer science;Costs;Fault diagnosis;Predictive models;Quality assurance;Reliability engineering;Software quality;Software reliability;Testing;classification;machine learning;software quality;verification and validation}
}

@InProceedings{JureczkoS10,
  Title                    = {Using Object-Oriented Design Metrics to Predict Software Defects},
  Author                   = {Jureczko, Marian and Spinellis, Diomidis},
  Booktitle                = {Models and Methodology of System Dependability. Proceedings of {RELCOMEX} 2010 Fifth International Conference on Dependability of Computer Systems {DepCoS}},
  Year                     = {2010},

  Address                  = {Wroc{\l}aw, Poland},
  Pages                    = {69--81},
  Publisher                = {Oficyna Wydawnicza Politechniki Wroc{\l}awskiej},
  Series                   = {Monographs of System Dependability},

  ISBN                     = {978-83-7493-526-5},
  Url                      = {http://www.dmst.aueb.gr/dds/pubs/conf/2010-DepCoS-RELCOMEX-ckjm-defects/html/JS10.html}
}

@InProceedings{YasutakaEtAl:07,
  Title                    = {The effects of over and under sampling on fault--prone module detection},
  Author                   = {Y. Kamei and A. Monden and S. Matsumoto and T. Kakimoto and K. Matsumoto},
  Booktitle                = {Empirical Software Engineering and Measurement (ESEM)},
  Year                     = {2007},
  Pages                    = {196--204}
}

@Article{02Kho,
  Title                    = {Using regression trees to classify fault-prone software modules},
  Author                   = {T. Khoshgoftaar and E. Allen and J. Deng},
  Journal                  = {IEEE Transactions on Reliability},
  Year                     = {2002},

  Optnumber                = {4},
  Optvolume                = {51}
}

@Article{KhoshgoftaarEtAl:05,
  Title                    = {Assessment of a New Three-Group Software Quality Classification Technique: An Empirical Case Study},
  Author                   = {T.M. Khoshgoftaar and N. Seliya and K. Gao},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2005},
  Number                   = {2},
  Pages                    = {183-218},
  Volume                   = {10}
}

@Article{Khoshgoftaar97,
  Title                    = {Application of Neural Networks to Software Quality Modeling of a Very Large Telecommunications System},
  Author                   = {T. M. Khoshgoftaar and E. Allen and J. Hudepohl and S. Aud},
  Journal                  = {IEEE Transactions on Neural Networks},
  Year                     = {1997},
  Number                   = {4},
  Pages                    = {902--909},
  Volume                   = {8}
}

@InProceedings{KhoshgoftaarGKN12,
  Title                    = {Exploring an iterative feature selection technique for highly imbalanced data sets},
  Author                   = {Khoshgoftaar, Taghi M. and Gao, Kehan and Napolitano, Amri},
  Booktitle                = {IEEE 13th International Conference on Information Reuse and Integration (IRI'2012)},
  Year                     = {2012},
  Month                    = {aug.},
  Pages                    = {101--108},

  Abstract                 = {The quality of a classification model is affected by two factors in a training data set: (1) the presence of excessive features and (2) the presence of imbalanced distributions between two classes in a binary classification problem. This paper presents an iterative feature selection method to deal with these two problems. The proposed method consists of an iterative process of data sampling followed by feature ranking and finally aggregating the results generated during the iterative process. In this study, we investigate a number of feature ranking techniques and a data sampling method with two different post-sampling proportions between the two classes. We compare the iterative feature selection technique to the one where a data sampling and a feature ranking technique are used together but only once (without iteration). The empirical study is carried out on two groups of highly imbalanced data sets from a real-world software system. The results demonstrate that our proposed iterative feature selection technique performs on average better than the method without iteration.},
  Doi                      = {10.1109/IRI.2012.6302997}
}

@Article{Khoshgoftaar2004,
  Title                    = {Comparative Assessment of Software Quality Classification Techniques: An Empirical Case Study},
  Author                   = {Taghi M. Khoshgoftaar and Naeem Seliya},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2004},

  Month                    = sep,
  Number                   = {3},
  Pages                    = {229--257},
  Volume                   = {9},

  Acmid                    = {990393},
  Address                  = {Hingham, MA, USA},
  Doi                      = {10.1023/B:EMSE.0000027781.18360.9b},
  ISSN                     = {1382-3256},
  Issue_date               = {September 2004},
  Keywords                 = {Software quality classification, analysis of variance, case-based reasoning, decision trees, expected cost of misclassification, logistic regression},
  Numpages                 = {29},
  Publisher                = {Kluwer Academic Publishers},
  Url                      = {http://dx.doi.org/10.1023/B:EMSE.0000027781.18360.9b}
}

@Article{Khoshgoftaar03,
  Title                    = {Analogy-Based Practical Classification Rules for Software Quality Estimation},
  Author                   = {Taghi M. Khoshgoftaar and Naeem Seliya},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2003},
  Number                   = {4},
  Pages                    = {325--350},
  Volume                   = {8},

  Address                  = {Hingham, MA, USA},
  Doi                      = {http://dx.doi.org/10.1023/A:1025316301168},
  ISSN                     = {1382-3256},
  Publisher                = {Kluwer Academic Publishers}
}

@InProceedings{KimZPW2006,
  Title                    = {Automatic Identification of Bug-Introducing Changes},
  Author                   = {Kim, Sunghun and Zimmermann, Thomas and Pan, Kai and Whitehead, E. James Jr.},
  Booktitle                = {Proceedings of the 21st IEEE/ACM International Conference on Automated Software Engineering},
  Year                     = {2006},

  Address                  = {Washington, DC, USA},
  Pages                    = {81--90},
  Publisher                = {IEEE Computer Society},
  Series                   = {ASE'06},

  Acmid                    = {1169308},
  Doi                      = {10.1109/ASE.2006.23},
  ISBN                     = {0-7695-2579-2},
  Numpages                 = {10},
  Url                      = {http://dx.doi.org/10.1109/ASE.2006.23}
}

@Article{LBMP08,
  Title                    = {Benchmarking Classification Models for Software Defect Prediction: A Proposed Framework and Novel Findings},
  Author                   = {Lessmann, S. and Baesens, B. and Mues, C. and Pietsch, S.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2008},

  Month                    = {July-Aug.},
  Number                   = {4},
  Pages                    = {485--496},
  Volume                   = {34},

  Abstract                 = {Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the NASA Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.},
  Doi                      = {10.1109/TSE.2008.35},
  ISSN                     = {0098-5589},
  Keywords                 = {benchmarking classification models;code attributes;fault-prone modules;metric-based classification;predictive classification models;proprietary data sets;software defect prediction;software quality;statistical testing procedures;testing efficiency;benchmark testing;software quality;statistical testing;}
}

@Article{Li2012,
  Title                    = {Sample-based software defect prediction with active and semi-supervised learning},
  Author                   = {Li, Ming
and Zhang, Hongyu
and Wu, Rongxin
and Zhou, Zhi-Hua},
  Journal                  = {Automated Software Engineering},
  Year                     = {2012},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {201--230},
  Volume                   = {19},

  Abstract                 = {Software defect prediction can help us better understand and control software quality. Current defect prediction techniques are mainly based on a sufficient amount of historical project data. However, historical data is often not available for new projects and for many organizations. In this case, effective defect prediction is difficult to achieve. To address this problem, we propose sample-based methods for software defect prediction. For a large software system, we can select and test a small percentage of modules, and then build a defect prediction model to predict defect-proneness of the rest of the modules. In this paper, we describe three methods for selecting a sample: random sampling with conventional machine learners, random sampling with a semi-supervised learner and active sampling with active semi-supervised learner. To facilitate the active sampling, we propose a novel active semi-supervised learning method ACoForest which is able to sample the modules that are most helpful for learning a good prediction model. Our experiments on PROMISE datasets show that the proposed methods are effective and have potential to be applied to industrial practice.},
  Day                      = {01},
  Doi                      = {10.1007/s10515-011-0092-1},
  ISSN                     = {1573-7535},
  Url                      = {http://dx.doi.org/10.1007/s10515-011-0092-1}
}

@InProceedings{LuCC2012,
  Title                    = {Software defect prediction using semi-supervised learning with dimension reduction},
  Author                   = {H. Lu and B. Cukic and M. Culp},
  Booktitle                = {27th IEEE/ACM International Conference on Automated Software Engineering (ASE'12)},
  Year                     = {2012},
  Month                    = {Sept},
  Pages                    = {314--317},

  Abstract                 = {Accurate detection of fault prone modules offers the path to high quality software products while minimizing non essential assurance expenditures. This type of quality modeling requires the availability of software modules with known fault content developed in similar environment. Establishing whether a module contains a fault or not can be expensive. The basic idea behind semi-supervised learning is to learn from a small number of software modules with known fault content and supplement model training with modules for which the fault information is not available. In this study, we investigate the performance of semi-supervised learning for software fault prediction. A preprocessing strategy, multidimensional scaling, is embedded in the approach to reduce the dimensional complexity of software metrics. Our results show that the semi-supervised learning algorithm with dimension-reduction preforms significantly better than one of the best performing supervised learning algorithms, random forest, in situations when few modules with known fault content are available for training.},
  Doi                      = {10.1145/2351676.2351734},
  Keywords                 = {learning (artificial intelligence);software metrics;software quality;dimension reduction;fault content;fault prone module detection;high quality software products;model training;multidimensional scaling;preprocessing strategy;random forest;semisupervised learning;software defect prediction;software fault prediction;software metrics dimensional complexity;software modules availability;Software fault prediction;dimension reduction;semi-supervised learning;software metrics}
}

@Article{Madeyski2015,
  Title                    = {Which process metrics can significantly improve defect prediction models? An empirical study},
  Author                   = {Madeyski, Lech and Jureczko, Marian},
  Journal                  = {Software Quality Journal},
  Year                     = {2015},
  Number                   = {3},
  Pages                    = {393--422},
  Volume                   = {23},

  Abstract                 = {The knowledge about the software metrics which serve as defect indicators is vital for the efficient allocation of resources for quality assurance. It is the process metrics, although sometimes difficult to collect, which have recently become popular with regard to defect prediction. However, in order to identify rightly the process metrics which are actually worth collecting, we need the evidence validating their ability to improve the product metric-based defect prediction models. This paper presents an empirical evaluation in which several process metrics were investigated in order to identify the ones which significantly improve the defect prediction models based on product metrics. Data from a wide range of software projects (both, industrial and open source) were collected. The predictions of the models that use only product metrics (simple models) were compared with the predictions of the models which used product metrics, as well as one of the process metrics under scrutiny (advanced models). To decide whether the improvements were significant or not, statistical tests were performed and effect sizes were calculated. The advanced defect prediction models trained on a data set containing product metrics and additionally Number of Distinct Committers (NDC) were significantly better than the simple models without NDC, while the effect size was medium and the probability of superiority (PS) of the advanced models over simple ones was high p=.016, which is a substantial finding useful in defect prediction. A similar result with slightly smaller PS was achieved by the advanced models trained on a data set containing product metrics and additionally all of the investigated process metrics p=.038. The advanced models trained on a data set containing product metrics and additionally Number of Modified Lines (NML) were significantly better than the simple models without NML, but the effect size was small p=.038. Hence, it is reasonable to recommend the NDC process metric in building the defect prediction models.},
  Doi                      = {10.1007/s11219-014-9241-7},
  ISSN                     = {1573-1367},
  Url                      = {http://dx.doi.org/10.1007/s11219-014-9241-7}
}

@InProceedings{Mahmood2015,
  Title                    = {What is the Impact of Imbalance on Software Defect Prediction Performance?},
  Author                   = {Mahmood, Zaheed and Bowes, David and Lane, Peter C. R. and Hall, Tracy},
  Booktitle                = {Proceedings of the 11th International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE'15)},
  Year                     = {2015},

  Address                  = {New York, NY, USA},
  Pages                    = {4:1--4:4},
  Publisher                = {ACM},
  Series                   = {PROMISE'15},

  Acmid                    = {2810150},
  Articleno                = {4},
  Doi                      = {10.1145/2810146.2810150},
  ISBN                     = {978-1-4503-3715-1},
  Keywords                 = {Data Imbalance, Defect Prediction, Machine Learning},
  Location                 = {Beijing, China},
  Numpages                 = {4},
  Url                      = {http://doi.acm.org/10.1145/2810146.2810150}
}

@Article{Menzies07b,
  Title                    = {Problems with Precision: A Response to Comments on Data Mining Static Code Attributes to Learn Defect Predictors},
  Author                   = {Tim Menzies and Alex Dekhtyar and Justin Distefano and Jeremy Greenwald},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2007},
  Number                   = {9},
  Pages                    = {637--640},
  Volume                   = {33},

  Address                  = {Los Alamitos, CA, USA},
  Doi                      = {http://doi.ieeecomputersociety.org/10.1109/TSE.2007.70721},
  ISSN                     = {0098-5589},
  Publisher                = {IEEE Computer Society}
}

@Article{Menzies07,
  Title                    = {Data Mining Static Code Attributes to Learn Defect Predictors},
  Author                   = {T. Menzies and J. Greenwald and A. Frank},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2007},

  Optmonth                 = {January},
  Optnumber                = {1},
  Optpages                 = {2--13},
  Optvolume                = {33}
}

@InProceedings{Morasca2016,
  Title                    = {Slope-based Fault-proneness Thresholds for Software Engineering Measures},
  Author                   = {Morasca, Sandro and Lavazza, Luigi},
  Booktitle                = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
  Year                     = {2016},

  Address                  = {New York, NY, USA},
  Pages                    = {12:1--12:10},
  Publisher                = {ACM},
  Series                   = {EASE '16},

  Acmid                    = {2915997},
  Articleno                = {12},
  Doi                      = {10.1145/2915970.2915997},
  ISBN                     = {978-1-4503-3691-8},
  Keywords                 = {fault-proneness, faultiness, logistic regression, probit regression, software measures, threshold},
  Location                 = {Limerick, Ireland},
  Numpages                 = {10},
  Url                      = {http://doi.acm.org/10.1145/2915970.2915997}
}

@InProceedings{MoserPS2008,
  Title                    = {A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction},
  Author                   = {R. Moser and W. Pedrycz and G. Succi},
  Booktitle                = {2008 ACM/IEEE 30th International Conference on Software Engineering},
  Year                     = {2008},
  Month                    = {May},
  Pages                    = {181-190},

  Abstract                 = {In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, naive Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: >75\% percentage of correctly classified files, a recall of >80\%, and a false positive rate <30\%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.},
  Doi                      = {10.1145/1368088.1368114},
  ISSN                     = {0270-5257},
  Keywords                 = {Bayes methods;Java;decision trees;learning (artificial intelligence);pattern classification;program diagnostics;regression analysis;software metrics;software quality;Eclipse project;Java file classification;change metrics;comparative analysis;cost-sensitive classification;decision tree;defect prediction;logistic regression;machine learning;naive Bayes method;process related software metrics;product related software metrics;software quality;static code attribute;Classification tree analysis;Costs;Java;Logistics;Permission;Predictive models;Resource management;Software engineering;Software metrics;Testing;cost-sensitive classification;defect prediction;software metrics}
}

@InProceedings{NZZH12,
  Title                    = {Change Bursts as Defect Predictors},
  Author                   = {Nachiappan Nagappan and Andreas Zeller and Thomas Zimmermann and Kim Herzig and Brendan Murphy},
  Booktitle                = {21st IEEE International Symposium on Software Reliability Engineering (ISSRE 2012)},
  Year                     = {2012},
  Month                    = {November},

  Location                 = {San Jose, California, USA}
}

@InProceedings{Nam2013,
  Title                    = {Transfer defect learning},
  Author                   = {J. Nam and S. J. Pan and S. Kim},
  Booktitle                = {35th International Conference on Software Engineering (ICSE'2013)},
  Year                     = {2013},
  Month                    = {May},
  Pages                    = {382--391},

  Abstract                 = {Many software defect prediction approaches have been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project defect prediction). Unfortunately, the performance of cross-project defect prediction is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance.},
  Doi                      = {10.1109/ICSE.2013.6606584},
  ISSN                     = {0270-5257},
  Keywords                 = {learning (artificial intelligence);public domain software;software engineering;TCA+;cross-project defect prediction;feature distributions;open-source projects;software defect prediction approaches;source projects;transfer defect learning;within-project prediction settings;Data models;Measurement;Predictive models;Software;Standards;Training;Vectors;cross-project defect prediction;empirical software engineering;transfer learning}
}

@InProceedings{Panichella2014,
  Title                    = {Cross-project defect prediction models: L'Union fait la force},
  Author                   = {A. Panichella and R. Oliveto and A. De Lucia},
  Booktitle                = {2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)},
  Year                     = {2014},
  Month                    = {Feb},
  Pages                    = {164--173},

  Abstract                 = {Existing defect prediction models use product or process metrics and machine learning methods to identify defect-prone source code entities. Different classifiers (e.g., linear regression, logistic regression, or classification trees) have been investigated in the last decade. The results achieved so far are sometimes contrasting and do not show a clear winner. In this paper we present an empirical study aiming at statistically analyzing the equivalence of different defect predictors. We also propose a combined approach, coined as CODEP (COmbined DEfect Predictor), that employs the classification provided by different machine learning techniques to improve the detection of defect-prone entities. The study was conducted on 10 open source software systems and in the context of cross-project defect prediction, that represents one of the main challenges in the defect prediction field. The statistical analysis of the results indicates that the investigated classifiers are not equivalent and they can complement each other. This is also confirmed by the superior prediction accuracy achieved by CODEP when compared to stand-alone defect predictors.},
  Doi                      = {10.1109/CSMR-WCRE.2014.6747166},
  Keywords                 = {learning (artificial intelligence);pattern classification;program debugging;public domain software;statistical analysis;CODEP;L'Union fait la force;classification;combined defect predictor;cross-project defect prediction models;defect-prone entity detection;machine learning techniques;open source software systems;statistical analysis;Accuracy;Context;Logistics;Measurement;Predictive models;Regression tree analysis;Software}
}

@InProceedings{RRCA07,
  Title                    = {Detecting Fault Modules Applying Feature Selection to Classifiers},
  Author                   = {Rodriguez, D. and Ruiz, R. and Cuadrado, J. and Aguilar-Ruiz, J.},
  Booktitle                = {IEEE International Conference on Information Reuse and Integration (IRI 2007)},
  Year                     = {2007},
  Month                    = {aug.},
  Pages                    = {667--672},

  Doi                      = {10.1109/IRI.2007.4296696},
  Keywords                 = {PROMISE repository;attribute selection techniques;automated data collection tools;classifier learning;data mining algorithms;fault module detection;feature selection;project management;software engineering databases;data mining;feature extraction;learning (artificial intelligence);pattern classification;project management;software management;}
}

@Article{Rodriguez2012,
  Title                    = {Searching for rules to detect defective modules: A subgroup discovery approach},
  Author                   = {D. Rodriguez and R. Ruiz and J.C. Riquelme and J.S. Aguilar-Ruiz},
  Journal                  = {Information Sciences},
  Year                     = {2012},
  Pages                    = {14--30},
  Volume                   = {191},

  Abstract                 = {Data mining methods in software engineering are becoming increasingly important as they can support several aspects of the software development life-cycle such as quality. In this work, we present a data mining approach to induce rules extracted from static software metrics characterising fault-prone modules. Due to the special characteristics of the defect prediction data (imbalanced, inconsistency, redundancy) not all classification algorithms are capable of dealing with this task conveniently. To deal with these problems, Subgroup Discovery (SD) algorithms can be used to find groups of statistically different data given a property of interest. We propose EDER-SD (Evolutionary Decision Rules for Subgroup Discovery), a SD algorithm based on evolutionary computation that induces rules describing only fault-prone modules. The rules are a well-known model representation that can be easily understood and applied by project managers and quality engineers. Thus, rules can help them to develop software systems that can be justifiably trusted. Contrary to other approaches in SD, our algorithm has the advantage of working with continuous variables as the conditions of the rules are defined using intervals. We describe the rules obtained by applying our algorithm to seven publicly available datasets from the PROMISE repository showing that they are capable of characterising subgroups of fault-prone modules. We also compare our results with three other well known SD algorithms and the EDER-SD algorithm performs well in most cases.},
  Doi                      = {10.1016/j.ins.2011.01.039},
  ISSN                     = {0020-0255},
  Keywords                 = {Defect prediction},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0020025511000661}
}

@InProceedings{SeiffertEtAl07,
  Title                    = {An empirical study of the classification performance of learners on imbalanced and noisy software quality data},
  Author                   = {C. Seiffert and T.M. Khoshgoftaar and J. Van Hulse and A. Folleco},
  Booktitle                = {2007 IEEE International Conference on Information Reuse and Integration, IEEE IRI-2007},
  Year                     = {2007},
  Pages                    = {651--658}
}

@Article{Shatnawi2010,
  Title                    = {Finding software metrics threshold values using ROC curves},
  Author                   = {Shatnawi, Raed and Li, Wei and Swain, James and Newman, Tim},
  Journal                  = {Journal of Software Maintenance and Evolution: Research and Practice},
  Year                     = {2010},
  Number                   = {1},
  Pages                    = {1--16},
  Volume                   = {22},

  Abstract                 = {An empirical study of the relationship between object-oriented (OO) metrics and error-severity categories is presented. The focus of the study is to identify threshold values of software metrics using receiver operating characteristic curves. The study used the three releases of the Eclipse project and found threshold values for some OO metrics that separated no-error classes from classes that had high-impact errors. Although these thresholds cannot predict whether a class will definitely have errors in the future, they can provide a more scientific method to assess class error proneness and can be used by engineers easily.},
  Doi                      = {10.1002/smr.404},
  ISSN                     = {1532-0618},
  Keywords                 = {object-oriented design, object-oriented metrics, thresholds, ROC curve},
  Publisher                = {John Wiley \& Sons, Ltd.},
  Url                      = {http://dx.doi.org/10.1002/smr.404}
}

@Article{Shepperd2013,
  Title                    = {Data Quality: Some Comments on the NASA Software Defect Datasets},
  Author                   = {Shepperd, M. and Qinbao Song and Zhongbin Sun and Mair, C.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2013},

  Month                    = {Sept},
  Number                   = {9},
  Pages                    = {1208--1215},
  Volume                   = {39},

  Doi                      = {10.1109/TSE.2013.11},
  ISSN                     = {0098-5589},
  Keywords                 = {data analysis;learning (artificial intelligence);pattern classification;software reliability;IEEE Transactions on Software Engineering;
 NASA software defect dataset;National Aeronautics and Space Administration;data preprocessing;data quality;data replication;
 dataset provenance;defect-prone classification;machine learning;not-defect-prone classification;
 software module classification;Abstracts;Communities;Educational institutions;NASA;PROM;Software;Sun;Empirical software engineering;
 data quality;defect prediction;machine learning}
}

@Article{SinghEDtAl2010,
  Title                    = {Empirical validation of object-oriented metrics for predicting fault proneness models},
  Author                   = {Singh, Yogesh and Kaur, Arvinder and Malhotra, Ruchika},
  Journal                  = {Software Quality Journal},
  Year                     = {2010},
  Note                     = {10.1007/s11219-009-9079-6},
  Pages                    = {3--35},
  Volume                   = {18},

  Abstract                 = {Empirical validation of software metrics used to predict software quality attributes is important to ensure their practical relevance in software organizations. The aim of this work is to find the relation of object-oriented (OO) metrics with fault proneness at different severity levels of faults. For this purpose, different prediction models have been developed using regression and machine learning methods. We evaluate and compare the performance of these methods to find which method performs better at different severity levels of faults and empirically validate OO metrics given by Chidamber and Kemerer. The results of the empirical study are based on public domain NASA data set. The performance of the predicted models was evaluated using Receiver Operating Characteristic (ROC) analysis. The results show that the area under the curve (measured from the ROC analysis) of models predicted using high severity faults is low as compared with the area under the curve of the model predicted with respect to medium and low severity faults. However, the number of faults in the classes correctly classified by predicted models with respect to high severity faults is not low. This study also shows that the performance of machine learning methods is better than logistic regression method with respect to all the severities of faults. Based on the results, it is reasonable to claim that models targeted at different severity levels of faults could help for planning and executing testing by focusing resources on fault-prone parts of the design and code that are likely to cause serious failures.},
  Affiliation              = {University School of Information Technology, GGS Indraprastha University Delhi 110403 India},
  ISSN                     = {0963-9314},
  Issue                    = {1},
  Keyword                  = {Computer Science},
  Publisher                = {Springer Netherlands},
  Url                      = {http://dx.doi.org/10.1007/s11219-009-9079-6}
}

@Article{SongJSYL2011,
  Title                    = {A General Software Defect-Proneness Prediction Framework},
  Author                   = {Q. Song and Z. Jia and M. Shepperd and S. Ying and J. Liu},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2011},

  Month                    = {May},
  Number                   = {3},
  Pages                    = {356--370},
  Volume                   = {37},

  Abstract                 = {BACKGROUND - Predicting defect-prone software components is an economically important activity and so has received a good deal of attention. However, making sense of the many, and sometimes seemingly inconsistent, results is difficult. OBJECTIVE - We propose and evaluate a general framework for software defect prediction that supports 1) unbiased and 2) comprehensive comparison between competing prediction systems. METHOD - The framework is comprised of 1) scheme evaluation and 2) defect prediction components. The scheme evaluation analyzes the prediction performance of competing learning schemes for given historical data sets. The defect predictor builds models according to the evaluated learning scheme and predicts software defects with new data according to the constructed model. In order to demonstrate the performance of the proposed framework, we use both simulation and publicly available software defect data sets. RESULTS - The results show that we should choose different learning schemes for different data sets (i.e., no scheme dominates), that small details in conducting how evaluations are conducted can completely reverse findings, and last, that our proposed framework is more effective and less prone to bias than previous approaches. CONCLUSIONS - Failure to properly or fully evaluate a learning scheme can be misleading; however, these problems may be overcome by our proposed framework.},
  Doi                      = {10.1109/TSE.2010.90},
  ISSN                     = {0098-5589},
  Keywords                 = {learning (artificial intelligence);software fault tolerance;software performance evaluation;competing learning schemes;defect predictor;scheme evaluation;software defect proneness prediction framework;Buildings;Data models;Prediction algorithms;Predictive models;Software;Training;Training data;Software defect prediction;machine learning;scheme evaluation.;software defect-proneness prediction}
}

@Article{SongTSE96,
  Title                    = {Software defect association mining and defect correction effort prediction},
  Author                   = {Q. Song and M. Shepperd and M. Cartwright and C. Mair},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2006},

  Month                    = {Feb.},
  Number                   = {2},
  Pages                    = {69--82},
  Volume                   = {32},

  Doi                      = {10.1109/TSE.2006.1599417},
  ISSN                     = {0098-5589},
  Keywords                 = {Accuracy;Association rules;Data mining;Inspection;Job shop scheduling;Project management;Resource management;Software development management;Software quality;Software systems; data mining; program testing; software process improvement; software quality; SEL defect data; association rule mining; defect correction effort prediction; software defect association prediction; software quality; Software defect prediction; defect association; defect correction effort.; defect isolation effort;}
}

@Article{tantithamthavorn2017,
  Title                    = {An Empirical Comparison of Model Validation Techniques for Defect Prediction Models},
  Author                   = {Tantithamthavorn, Chakkrit and McIntosh, Shane and Hassan, Ahmed E. and Matsumoto, Kenichi},
  Year                     = {2017},
  Number                   = {1},

  Booktitle                = {IEEE Transactions on Software Engineering},
  Page                     = {1--18},
  Volumn                   = {43}
}

@Article{TurhanESE12,
  Title                    = {On the dataset shift problem in software engineering prediction models},
  Author                   = {Turhan, Burak},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2012},
  Note                     = {10.1007/s10664-011-9182-8},
  Pages                    = {62--74},
  Volume                   = {17},

  Affiliation              = {Department of Information Processing Science, University of Oulu, POB.3000, 90014 Oulu, Finland},
  ISSN                     = {1382-3256},
  Issue                    = {1},
  Keyword                  = {Computer Science},
  Publisher                = {Springer Netherlands}
}

@InProceedings{VanHulseEtAl:2007,
  Title                    = {Experimental perspectives on learning from imbalanced data},
  Author                   = {J. {Van Hulse} and T.~M. Khoshgoftaar and A. Napolitano},
  Booktitle                = {Proceedings of the 24th International Conference on Machine Learning (ICML07)},
  Year                     = {2007},

  Address                  = {New York, USA},
  Publisher                = {ACM}
}

@Article{Vandecruys2008,
  Title                    = {Mining software repositories for comprehensible software fault prediction models},
  Author                   = {Olivier Vandecruys and David Martens and Bart Baesens and Christophe Mues and Manu {De Backer} and Raf Haesen},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2008},
  Number                   = {5},
  Pages                    = {823--839},
  Volume                   = {81},

  Abstract                 = {Software managers are routinely confronted with software projects that contain errors or inconsistencies and exceed budget and time limits. By mining software repositories with comprehensible data mining techniques, predictive models can be induced that offer software managers the insights they need to tackle these quality and budgeting problems in an efficient way. This paper deals with the role that the Ant Colony Optimization (ACO)-based classification technique AntMiner+ can play as a comprehensible data mining technique to predict erroneous software modules. In an empirical comparison on three real-world public datasets, the rule-based models produced by AntMiner+ are shown to achieve a predictive accuracy that is competitive to that of the models induced by several other included classification techniques, such as C4.5, logistic regression and support vector machines. In addition, we will argue that the intuitiveness and comprehensibility of the AntMiner+ models can be considered superior to the latter models.},
  Doi                      = {DOI: 10.1016/j.jss.2007.07.034},
  ISSN                     = {0164-1212},
  Keywords                 = {Classification},
  Url                      = {http://www.sciencedirect.com/science/article/B6V0N-4PHSC3R-2/2/129ce670cc51b090011f548066bb5711}
}

@InProceedings{WangKWN12,
  Title                    = {A Comparative Study on the Stability of Software Metric Selection Techniques},
  Author                   = {Huanjing Wang and Taghi M. Khoshgoftaar and Randall Wald and Amri Napolitano},
  Booktitle                = {11th International Conference on Machine Learning and Applications, ICMLA},
  Year                     = {2012},
  Pages                    = {301--307},

  Ee                       = {http://dx.doi.org/10.1109/ICMLA.2012.142}
}

@Book{WFH11,
  Title                    = {Data Mining: Practical machine learning tools and techniques},
  Author                   = {I.H. Witten and E. Frank and M.A. Hall},
  Publisher                = {Morgan Kaufmann},
  Year                     = {2011},

  Address                  = {San Francisco},
  Edition                  = {3rd Edition}
}

@Book{Wohlin2000ESE,
  Title                    = {Experimentation in software engineering: an introduction},
  Author                   = {Wohlin, Claes and Runeson, Per and H\"{o}st, Martin and Ohlsson, Magnus C. and Regnell, Bj\"{o}orn and Wessl{\'e}n, Anders},
  Publisher                = {Kluwer Academic Publishers},
  Year                     = {2000},

  Address                  = {Norwell, MA, USA},

  ISBN                     = {0-7923-8682-5}
}

@InBook{Wrobel01,
  Title                    = {Relational Data Mining},
  Author                   = {Stefan Wrobel},
  Chapter                  = {An algorithm for multi-relational discovery of subgroups},
  Editor                   = {Saso Dzeroski and Nada Lavra\u{c}},
  Pages                    = {74--101},
  Publisher                = {Springer},
  Year                     = {2001}
}

@InProceedings{Wrobel97,
  Title                    = {An algorithm for multi-relational discovery of subgroups},
  Author                   = {Stefan Wrobel},
  Booktitle                = {Proceedings of the 1st European Symposium on Principles of Data Mining},
  Year                     = {1997},
  Pages                    = {78--87}
}

@InProceedings{Zhang2009,
  Title                    = {An investigation of the relationships between lines of code and defects},
  Author                   = {H. Zhang},
  Booktitle                = {IEEE International Conference on Software Maintenance},
  Year                     = {2009},
  Month                    = {Sept},
  Pages                    = {274--283},

  Abstract                 = {It is always desirable to understand the quality of a software system based on static code metrics. In this paper, we analyze the relationships between lines of code (LOC) and defects (including both pre-release and post-release defects). We confirm the ranking ability of LOC discovered by Fenton and Ohlsson. Furthermore, we find that the ranking ability of LOC can be formally described using Weibull functions. We can use defect density values calculated from a small percentage of largest modules to predict the number of total defects accurately. We also find that, given LOC we can predict the number of defective components reasonably well using typical classification techniques. We perform an extensive experiment using the public Eclipse dataset, and replicate the study using the NASA dataset. Our results confirm that simple static code attributes such as LOC can be useful predictors of software quality.},
  Doi                      = {10.1109/ICSM.2009.5306304},
  ISSN                     = {1063-6773},
  Keywords                 = {program diagnostics;software metrics;software quality;Eclipse dataset;lines of code;post-release defects;pre-release defects;software system quality;static code metrics;Density measurement;Lab-on-a-chip;Laboratories;NASA;Packaging machines;Predictive models;Software metrics;Software quality;Software systems;System testing}
}

@Article{Zhang07,
  Title                    = {Comments on "Data Mining Static Code Attributes to Learn Defect Predictors"},
  Author                   = {Hongyu Zhang and Xiuzhen Zhang},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2007},
  Number                   = {9},
  Pages                    = {635--637},
  Volume                   = {33},

  Address                  = {Los Alamitos, CA, USA},
  Doi                      = {http://doi.ieeecomputersociety.org/10.1109/TSE.2007.70706},
  ISSN                     = {0098-5589},
  Publisher                = {IEEE Computer Society}
}

@Article{ZhouTGG2016,
  Title                    = {Combining text mining and data mining for bug report classification},
  Author                   = {Zhou, Yu and Tong, Yanxiang and Gu, Ruihang and Gall, Harald},
  Journal                  = {Journal of Software: Evolution and Process},
  Year                     = {2016},
  Note                     = {JSME-15-0091.R2},
  Number                   = {3},
  Pages                    = {150--176},
  Volume                   = {28},

  Abstract                 = {Bug reports represent an important information source for software construction. Misclassification of these reports inevitably introduces bias. Manual examinations can help reduce the noise, but bring a heavy burden for developers instead. In this paper, we propose a multi-stage approach by combining both text mining and data mining techniques to automate the prediction process. The first stage leverages text mining techniques to analyze the summary parts of bug reports and classifies them into three levels of probability. The extracted features and some other structured features of bug reports are then fed into the machine learner in the second stage. Data grafting techniques are employed to bridge the two stages. Comparative experiments with previous studies on the same data ---three large-scale open-source projects--- consistently achieve a reasonable enhancement (from 77.4\% to 81.7\%, 76.1\% to 81.6\%, and 87.4\% to 93.7\%, respectively) over their best results in terms of overall performance. Additional comparative empirical experiments on other seven popular open-source systems confirm the findings. Moreover, based on the data obtained, we also empirically studied the impact relation between the underlying classifiers and various other properties of the combined model. A prototypical recommender system has been developed to demonstrate the applicability of our approach.},
  Doi                      = {10.1002/smr.1770},
  ISSN                     = {2047-7481},
  Keywords                 = {software evolution, bug report classification, data mining, text mining},
  Url                      = {http://dx.doi.org/10.1002/smr.1770}
}

@InProceedings{Zimmermann2009,
  Title                    = {Cross-project Defect Prediction: A Large Scale Experiment on Data vs. Domain vs. Process},
  Author                   = {Zimmermann, Thomas and Nagappan, Nachiappan and Gall, Harald and Giger, Emanuel and Murphy, Brendan},
  Booktitle                = {Proceedings of the the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
  Year                     = {2009},

  Address                  = {New York, NY, USA},
  Pages                    = {91--100},
  Publisher                = {ACM},
  Series                   = {ESEC/FSE'09},

  Acmid                    = {1595713},
  Doi                      = {10.1145/1595696.1595713},
  ISBN                     = {978-1-60558-001-2},
  Keywords                 = {churn, cross-project, decision trees, defect prediction, logistic regression, prediction quality},
  Location                 = {Amsterdam, The Netherlands},
  Numpages                 = {10},
  Url                      = {http://doi.acm.org/10.1145/1595696.1595713}
}

@InProceedings{ZimmermannPZ07,
  Title                    = {Predicting Defects for Eclipse},
  Author                   = {Zimmermann, T. and Premraj, R. and Zeller, A.},
  Booktitle                = {International Workshop on Predictor Models in Software Engineering (PROMISE'07)},
  Year                     = {2007},
  Month                    = {may},
  Pages                    = {9},

  Abstract                 = {We have mapped defects from the bug database of eclipse (one of the largest open-source projects) to source code locations. The resulting data set lists the number of pre- and post-release defects for every package and file in the eclipse releases 2.0, 2.1, and 3.0. We additionally annotated the data with common complexity metrics. All data is publicly available and can serve as a benchmark for defect prediction models.},
  Doi                      = {10.1109/PROMISE.2007.10},
  Keywords                 = {Eclipse;bug database;common complexity metrics;defect prediction models;open-source projects;source code locations;program debugging;public domain software;}
}

